{"id": "2602.23407", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23407", "abs": "https://arxiv.org/abs/2602.23407", "authors": ["Jiazheng Quan", "Xiaodong Li", "Bin Wang", "Guo An", "Like Liu", "Degen Huang", "Lin Liu", "Chengbin Hou"], "title": "Learning to Generate Secure Code via Token-Level Rewards", "comment": "18 pages, 3 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in code generation, yet they remain prone to producing security vulnerabilities. Existing approaches commonly suffer from two key limitations: the scarcity of high-quality security data and coarse-grained reinforcement learning reward signals. To address these challenges, we propose Vul2Safe, a new secure code generation framework that leverages LLM self-reflection to construct high-confidence repair pairs from real-world vulnerabilities, and further generates diverse implicit prompts to build the PrimeVul+ dataset. Meanwhile, we introduce SRCode, a novel training framework that pioneers the use of token-level rewards in reinforcement learning for code security, which enables the model to continuously attend to and reinforce critical fine-grained security patterns during training. Compared with traditional instance-level reward schemes, our approach allows for more precise optimization of local security implementations. Extensive experiments show that PrimeVul+ and SRCode substantially reduce security vulnerabilities in generated code while improving overall code quality across multiple benchmarks.", "AI": {"tldr": "Vul2Safe\u548cSRCode\u901a\u8fc7\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u9ad8\u8d28\u91cf\u5b89\u5168\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u7c97\u7c92\u5ea6\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4fe1\u53f7\u3002", "method": "\u63d0\u51faVul2Safe\uff0c\u4e00\u4e2a\u5229\u7528LLM\u81ea\u6211\u53cd\u601d\u4ece\u73b0\u5b9e\u4e16\u754c\u6f0f\u6d1e\u4e2d\u6784\u5efa\u9ad8\u7f6e\u4fe1\u5ea6\u4fee\u590d\u5bf9\u7684\u65b0\u5b89\u5168\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5e76\u8fdb\u4e00\u6b65\u751f\u6210\u591a\u6837\u5316\u7684\u9690\u5f0f\u63d0\u793a\u6765\u6784\u5efaPrimeVul+\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u5f15\u5165SRCode\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u57f9\u8bad\u6846\u67b6\uff0c\u5b83\u9996\u6b21\u5728\u4ee3\u7801\u5b89\u5168\u6027\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528token\u7ea7\u522b\u7684\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u5173\u6ce8\u5e76\u5f3a\u5316\u5173\u952e\u7ec6\u7c92\u5ea6\u5b89\u5168\u6a21\u5f0f\u3002", "result": "PrimeVul+\u548cSRCode\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6574\u4f53\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "Vul2Safe\u548cSRCode\u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3002"}}
{"id": "2602.23513", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.23513", "abs": "https://arxiv.org/abs/2602.23513", "authors": ["Alex Carbajal", "Asma Jodeiri Akbarfam"], "title": "A Software-Defined Testbed for Quantifying Deauthentication Resilience in Modern Wi-Fi Networks", "comment": "6 pages, 5 figures, 1 table. Accepted for publication in IEEE SoutheastCon 2026", "summary": "Wi-Fi deauthentication attacks remain a practical denial-of-service (DoS) threat by exploiting unprotected management frames to disrupt client connectivity. In this work, we introduce a software-defined testbed to measure Wi-Fi resilience to deauthentication attacks. We experimentally evaluate five wireless security configurations: open networks, WPA1, WPA2 without Protected Management Frames (PMF), WPA2 with PMF, and WPA3. Using controlled experiments, we measure client disconnection rates, packet injection volume, and time-to-disruption under each configuration. Packet-level behavior is analyzed using standard wireless auditing tools. Open networks, WPA1, and WPA2 without PMF proved entirely vulnerable to deauthentication, while no successful attacks were observed for WPA2 with PMF or WPA3 under tested conditions. These findings confirm the effectiveness of management-frame protection and highlight the continued risk posed by legacy or misconfigured wireless deployments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23438", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23438", "abs": "https://arxiv.org/abs/2602.23438", "authors": ["Varun Gopal", "Rishabh Jain", "Aradhya Mathur", "Nikitha SR", "Sohan Patnaik", "Sudhir Yarram", "Mayur Hemani", "Balaji Krishnamurthy", "Mausoom Sarkar"], "title": "DesignSense: A Human Preference Dataset and Reward Modeling Framework for Graphic Layout Generation", "comment": "14 pages, 3 figures", "summary": "Graphic layouts serve as an important and engaging medium for visual communication across different channels. While recent layout generation models have demonstrated impressive capabilities, they frequently fail to align with nuanced human aesthetic judgment. Existing preference datasets and reward models trained on text-to-image generation do not generalize to layout evaluation, where the spatial arrangement of identical elements determines quality. To address this critical gap, we introduce DesignSense-10k, a large-scale dataset of 10,235 human-annotated preference pairs for graphic layout evaluation. We propose a five-stage curation pipeline that generates visually coherent layout transformations across diverse aspect ratios, using semantic grouping, layout prediction, filtering, clustering, and VLM-based refinement to produce high-quality comparison pairs. Human preferences are annotated using a 4-class scheme (left, right, both good, both bad) to capture subjective ambiguity. Leveraging this dataset, we train DesignSense, a vision-language model-based classifier that substantially outperforms existing open-source and proprietary models across comprehensive evaluation metrics (54.6% improvement in Macro F1 over the strongest proprietary baseline). Our analysis shows that frontier VLMs remain unreliable overall and fail catastrophically on the full four-class task, underscoring the need for specialized, preference-aware models. Beyond the dataset, our reward model DesignSense yields tangible downstream gains in layout generation. Using our judge during RL based training improves generator win rate by about 3%, while inference-time scaling, which involves generating multiple candidates and selecting the best one, provides a 3.6% improvement. These results highlight the practical impact of specialized, layout-aware preference modeling on real-world layout generation quality.", "AI": {"tldr": "DesignSense\u6a21\u578b\u5728\u56fe\u5f62\u5e03\u5c40\u8bc4\u4f30\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u56fe\u5f62\u5e03\u5c40\u4f5c\u4e3a\u89c6\u89c9\u901a\u4fe1\u7684\u91cd\u8981\u548c\u5438\u5f15\u4eba\u7684\u5a92\u4ecb\uff0c\u73b0\u6709\u7684\u5e03\u5c40\u751f\u6210\u6a21\u578b\u5728\u7ec6\u5fae\u7684\u4eba\u7c7b\u5ba1\u7f8e\u5224\u65ad\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u521b\u5efaDesignSense-10k\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e94\u9636\u6bb5\u7f16\u8f91\u6d41\u7a0b\uff0c\u5e76\u8bad\u7ec3DesignSense\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u7c7b\u5668\u3002", "result": "DesignSense\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u5e03\u5c40\u751f\u6210\u65b9\u9762\u5e26\u6765\u5b9e\u9645\u6548\u76ca\u3002", "conclusion": "\u8bbe\u8ba1Sense\u6a21\u578b\u5728\u56fe\u5f62\u5e03\u5c40\u8bc4\u4f30\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u524d\u6cbf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2602.23518", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23518", "abs": "https://arxiv.org/abs/2602.23518", "authors": ["Arkaprabha Ganguli", "Anirban Samaddar", "Florian K\u00e9ruzor\u00e9", "Nesar Ramachandra", "Julie Bessac", "Sandeep Madireddy", "Emil Constantinescu"], "title": "Uncovering Physical Drivers of Dark Matter Halo Structures with Auxiliary-Variable-Guided Generative Models", "comment": null, "summary": "Deep generative models (DGMs) compress high-dimensional data but often entangle distinct physical factors in their latent spaces. We present an auxiliary-variable-guided framework for disentangling representations of thermal Sunyaev-Zel'dovich (tSZ) maps of dark matter halos. We introduce halo mass and concentration as auxiliary variables and apply a lightweight alignment penalty to encourage latent dimensions to reflect these physical quantities. To generate sharp and realistic samples, we extend latent conditional flow matching (LCFM), a state-of-the-art generative model, to enforce disentanglement in the latent space. Our Disentangled Latent-CFM (DL-CFM) model recovers the established mass-concentration scaling relation and identifies latent space outliers that may correspond to unusual halo formation histories. By linking latent coordinates to interpretable astrophysical properties, our method transforms the latent space into a diagnostic tool for cosmological structure. This work demonstrates that auxiliary guidance preserves generative flexibility while yielding physically meaningful, disentangled embeddings, providing a generalizable pathway for uncovering independent factors in complex astronomical datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u8026\u6697\u7269\u8d28\u6655\u7684tSZ\u56fe\u8868\u793a\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u5b87\u5b99\u7ed3\u6784\u8bca\u65ad\u5de5\u5177\u3002", "motivation": "Deep generative models\u538b\u7f29\u9ad8\u7ef4\u6570\u636e\uff0c\u4f46\u901a\u5e38\u4f1a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7ea0\u7f20\u4e0d\u540c\u7684\u7269\u7406\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u53d8\u91cf\u5f15\u5bfc\u6846\u67b6\uff0c\u5c06\u6697\u7269\u8d28\u6655\u7684\u70edSunyaev-Zel'dovich\uff08tSZ\uff09\u56fe\u8868\u793a\u8fdb\u884c\u89e3\u8026\u3002\u5f15\u5165\u4e86\u6655\u8d28\u91cf\u548c\u6d53\u5ea6\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\uff0c\u5e76\u5e94\u7528\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u60e9\u7f5a\u6765\u9f13\u52b1\u6f5c\u5728\u7ef4\u5ea6\u53cd\u6620\u8fd9\u4e9b\u7269\u7406\u91cf\u3002\u4e3a\u4e86\u751f\u6210\u6e05\u6670\u548c\u903c\u771f\u7684\u6837\u672c\uff0c\u6269\u5c55\u4e86\u6f5c\u5728\u6761\u4ef6\u6d41\u5339\u914d\uff08LCFM\uff09\uff0c\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f3a\u5236\u5b9e\u73b0\u89e3\u8026\u3002", "result": "Disentangled Latent-CFM\uff08DL-CFM\uff09\u6a21\u578b\u6062\u590d\u4e86\u5df2\u5efa\u7acb\u7684\u8d28\u5fc3-\u6d53\u5ea6\u5c3a\u5ea6\u5173\u7cfb\uff0c\u5e76\u8bc6\u522b\u51fa\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u8fd9\u4e9b\u5f02\u5e38\u503c\u53ef\u80fd\u4e0e\u5f02\u5e38\u7684\u6655\u5f62\u6210\u5386\u53f2\u76f8\u5bf9\u5e94\u3002\u901a\u8fc7\u5c06\u6f5c\u5728\u5750\u6807\u4e0e\u53ef\u89e3\u91ca\u7684\u5929\u4f53\u7269\u7406\u5c5e\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6f5c\u5728\u7a7a\u95f4\u8f6c\u5316\u4e3a\u5b87\u5b99\u7ed3\u6784\u8bca\u65ad\u5de5\u5177\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u8f85\u52a9\u5f15\u5bfc\u5728\u4fdd\u6301\u751f\u6210\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u89e3\u8026\u5d4c\u5165\uff0c\u4e3a\u63ed\u793a\u590d\u6742\u5929\u4f53\u6570\u636e\u96c6\u4e2d\u7684\u72ec\u7acb\u56e0\u7d20\u63d0\u4f9b\u4e86\u4e00\u6761\u901a\u7528\u7684\u9014\u5f84\u3002"}}
{"id": "2602.23367", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23367", "abs": "https://arxiv.org/abs/2602.23367", "authors": ["Shubh Laddha", "Lucas Changbencharoen", "Win Kuptivej", "Surya Shringla", "Archana Vaidheeswaran", "Yash Bhaskar"], "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance", "comment": "4 pages, 2 figures, 3 tables", "summary": "Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MCP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30MCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u548c\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30MCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u548c\u751f\u6001\u7cfb\u7edf\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u771f\u5b9e\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7528\u6237\u67e5\u8be2\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684MCP\u6570\u636e\u96c6\uff0c\u5305\u542b\u9488\u5bf9308\u4e2aMCP\u670d\u52a1\u5668\u4e0a\u76842800\u4e2a\u5de5\u5177\u751f\u6210\u7684\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u67e5\u8be2\u3002", "result": "\u6bcf\u4e2a\u5de5\u5177\u90fd\u4e0e\u591a\u4e2a\u72ec\u7279\u7684\u7528\u6237\u89d2\u8272\u914d\u5bf9\uff0c\u4ee5\u6355\u6349\u4e0d\u540c\u5c42\u6b21\u7684\u7528\u6237\u610f\u56fe\uff0c\u4ece\u7cbe\u786e\u7684\u4efb\u52a1\u8bf7\u6c42\u5230\u6a21\u7cca\u7684\u63a2\u7d22\u6027\u547d\u4ee4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aMCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u548c\u751f\u6001\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2602.23391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23391", "abs": "https://arxiv.org/abs/2602.23391", "authors": ["Nazanin Mohammadi Sepahvand", "Eleni Triantafillou", "Hugo Larochelle", "Doina Precup", "Daniel M. Roy", "Gintare Karolina Dziugaite"], "title": "Detoxifying LLMs via Representation Erasure-Based Preference Optimization", "comment": null, "summary": "Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful \"directions\" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.", "AI": {"tldr": "\u63d0\u51faREPO\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u5bb3\u8f93\u51fa\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u5f15\u53d1\u5b89\u5168\u6027\u62c5\u5fe7\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8868\u793a\u64e6\u9664\u7684\u504f\u597d\u4f18\u5316\uff08REPO\uff09\u65b9\u6cd5\uff0c\u5c06\u8131\u6bd2\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6807\u8bb0\u7ea7\u522b\u7684\u504f\u597d\u95ee\u9898\u3002", "result": "REPO\u5728\u5bf9\u6297\u6027\u63d0\u793a\u548c\u518d\u5b66\u4e60\u653b\u51fb\u7b49\u590d\u6742\u5a01\u80c1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "REPO\u65b9\u6cd5\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.23516", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23516", "abs": "https://arxiv.org/abs/2602.23516", "authors": ["Meisam Mohammady", "Qin Yang", "Nicholas Stout", "Ayesha Samreen", "Han Wang", "Christopher J Quinn", "Yuan Hong"], "title": "Lap2: Revisiting Laplace DP-SGD for High Dimensions via Majorization Theory", "comment": "16 pages including appendix. arXiv admin note: text overlap with arXiv:2509.06264", "summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) is a cornerstone technique for ensuring privacy in deep learning, widely used in both training from scratch and fine-tuning large-scale language models. While DP-SGD predominantly relies on the Gaussian mechanism, the Laplace mechanism remains underutilized due to its reliance on L1 norm clipping. This constraint severely limits its practicality in high-dimensional models because the L1 norm of an n-dimensional gradient can be up to sqrt(n) times larger than its L2 norm. As a result, the required noise scale grows significantly with model size, leading to poor utility or untrainable models.\n  In this work, we introduce Lap2, a new solution that enables L2 clipping for Laplace DP-SGD while preserving strong privacy guarantees. We overcome the dimensionality-driven clipping barrier by computing coordinate-wise moment bounds and applying majorization theory to construct a tight, data-independent upper bound over the full model. By exploiting the Schur-convexity of the moment accountant function, we aggregate these bounds using a carefully designed majorization set that respects the L2 clipping constraint. This yields a multivariate privacy accountant that scales gracefully with model dimension and enables the use of thousands of moments. Empirical evaluations demonstrate that our approach significantly improves the performance of Laplace DP-SGD, achieving results comparable to or better than Gaussian DP-SGD under strong privacy constraints. For instance, fine-tuning RoBERTa-base (125M parameters) on SST-2 achieves 87.88% accuracy at epsilon=0.54, outperforming Gaussian (87.16%) and standard Laplace (48.97%) under the same budget.", "AI": {"tldr": "Lap2\u663e\u8457\u63d0\u9ad8\u4e86Laplace DP-SGD\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "DP-SGD\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u786e\u4fdd\u9690\u79c1\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46Laplace\u673a\u5236\u7531\u4e8eL1\u8303\u6570\u526a\u88c1\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faLap2\uff0c\u4e00\u79cd\u5141\u8bb8L2\u526a\u88c1\u7684Laplace DP-SGD\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u8ba1\u7b97\u5750\u6807-wise \u77ac\u65f6\u754c\u9650\u548c\u8fd0\u7528\u4e3b\u5316\u7406\u8bba\u6784\u9020\u6574\u4e2a\u6a21\u578b\u7684\u4e0a\u754c\uff0c\u5229\u7528Schur-\u51f8\u6027\u805a\u5408\u8fd9\u4e9b\u754c\u9650\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86Laplace DP-SGD\u7684\u6027\u80fd\uff0c\u5728\u5f3a\u9690\u79c1\u7ea6\u675f\u4e0b\uff0c\u4e0e\u9ad8\u65afDP-SGD\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u4f8b\u5982\uff0c\u5728SST-2\u4e0a\u5fae\u8c03RoBERTa-base\uff08125M\u53c2\u6570\uff09\u5728epsilon=0.54\u65f6\u8fbe\u523087.88%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u9ad8\u65af\uff0887.16%\uff09\u548c\u6807\u51c6Laplace\uff0848.97%\uff09\u3002", "conclusion": "Lap2\u662f\u4e00\u79cd\u6709\u6548\u7684Laplace DP-SGD\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u5e76\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2602.23514", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23514", "abs": "https://arxiv.org/abs/2602.23514", "authors": ["Mike Middleton", "Teymoor Ali", "Hakan Kayan", "Basabdatta Sen Bhattacharya", "Charith Perera", "Oliver Rhodes", "Elena Gheorghiu", "Mark Vousden", "Martin A. Trefzer"], "title": "Modelling and Simulation of Neuromorphic Datasets for Anomaly Detection in Computer Vision", "comment": "draft paper", "summary": "Limitations on the availability of Dynamic Vision Sensors (DVS) present a fundamental challenge to researchers of neuromorphic computer vision applications. In response, datasets have been created by the research community, but often contain a limited number of samples or scenarios. To address the lack of a comprehensive simulator of neuromorphic vision datasets, we introduce the Anomalous Neuromorphic Tool for Shapes (ANTShapes), a novel dataset simulation framework. Built in the Unity engine, ANTShapes simulates abstract, configurable 3D scenes populated by objects displaying randomly-generated behaviours describing attributes such as motion and rotation. The sampling of object behaviours, and the labelling of anomalously-acting objects, is a statistical process following central limit theorem principles. Datasets containing an arbitrary number of samples can be created and exported from ANTShapes, along with accompanying label and frame data, through the adjustment of a limited number of parameters within the software. ANTShapes addresses the limitations of data availability to researchers of event-based computer vision by allowing for the simulation of bespoke datasets to suit purposes including object recognition and localisation alongside anomaly detection.", "AI": {"tldr": "ANTShapes\u662f\u4e00\u79cd\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u6570\u636e\u96c6\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u9650\u5236\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff08DVS\uff09\u7684\u53ef\u7528\u6027\u5bf9\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4eba\u5458\u6784\u6210\u6311\u6218\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u5f02\u5e38\u795e\u7ecf\u5f62\u6001\u5de5\u5177\u5f62\u72b6\uff08ANTShapes\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u6a21\u62df\u6846\u67b6\u3002\u5728Unity\u5f15\u64ce\u4e2d\u6784\u5efa\uff0cANTShapes\u6a21\u62df\u4e86\u7531\u5177\u6709\u968f\u673a\u751f\u6210\u884c\u4e3a\uff08\u63cf\u8ff0\u8fd0\u52a8\u548c\u65cb\u8f6c\u7b49\u5c5e\u6027\uff09\u7684\u5bf9\u8c61\u7ec4\u6210\u7684\u62bd\u8c61\u3001\u53ef\u914d\u7f6e\u76843D\u573a\u666f\u3002", "result": "ANTShapes\u53ef\u4ee5\u521b\u5efa\u548c\u5bfc\u51fa\u5305\u542b\u4efb\u610f\u6570\u91cf\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4f34\u968f\u7684\u6807\u7b7e\u548c\u5e27\u6570\u636e\uff0c\u901a\u8fc7\u8c03\u6574\u8f6f\u4ef6\u4e2d\u7684\u5c11\u91cf\u53c2\u6570\u3002", "conclusion": "ANTShapes\u901a\u8fc7\u5141\u8bb8\u6a21\u62df\u5b9a\u5236\u6570\u636e\u96c6\u6765\u89e3\u51b3\u57fa\u4e8e\u4e8b\u4ef6\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4eba\u5458\u7684\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\uff0c\u4ee5\u9002\u5e94\u5305\u62ec\u5bf9\u8c61\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u5f02\u5e38\u68c0\u6d4b\u5728\u5185\u7684\u7528\u9014\u3002"}}
{"id": "2602.23535", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23535", "abs": "https://arxiv.org/abs/2602.23535", "authors": ["Adam Block", "Abhishek Shetty"], "title": "Partition Function Estimation under Bounded f-Divergence", "comment": null, "summary": "We study the statistical complexity of estimating partition functions given sample access to a proposal distribution and an unnormalized density ratio for a target distribution. While partition function estimation is a classical problem, existing guarantees typically rely on structural assumptions about the domain or model geometry. We instead provide a general, information-theoretic characterization that depends only on the relationship between the proposal and target distributions. Our analysis introduces the integrated coverage profile, a functional that quantifies how much target mass lies in regions where the density ratio is large. We show that integrated coverage tightly characterizes the sample complexity of multiplicative partition function estimation and provide matching lower bounds. We further express these bounds in terms of $f$-divergences, yielding sharp phase transitions depending on the growth rate of f and recovering classical results as a special case while extending to heavy-tailed regimes. Matching lower bounds establish tightness in all regimes. As applications, we derive improved finite-sample guarantees for importance sampling and self-normalized importance sampling, and we show a strict separation between the complexity of approximate sampling and counting under the same divergence constraints. Our results unify and generalize prior analyses of importance sampling, rejection sampling, and heavy-tailed mean estimation, providing a minimal-assumption theory of partition function estimation. Along the way we introduce new technical tools including new connections between coverage and $f$-divergences as well as a generalization of the classical Paley-Zygmund inequality.", "AI": {"tldr": "\u7814\u7a76\u4f30\u8ba1\u914d\u5206\u51fd\u6570\u7684\u7edf\u8ba1\u590d\u6742\u6027\uff0c\u5f15\u5165\u65b0\u5de5\u5177\uff0c\u63a8\u5e7f\u5148\u524d\u5206\u6790\uff0c\u63d0\u4f9b\u6700\u5c0f\u5047\u8bbe\u7406\u8bba\u3002", "motivation": "\u7814\u7a76\u5728\u7ed9\u5b9a\u76ee\u6807\u5206\u5e03\u7684\u672a\u89c4\u8303\u5316\u5bc6\u5ea6\u6bd4\u548c\u63d0\u8bae\u5206\u5e03\u7684\u6837\u672c\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u4f30\u8ba1\u914d\u5206\u51fd\u6570\u7684\u7edf\u8ba1\u590d\u6742\u6027\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u4e00\u822c\u6027\u63cf\u8ff0\uff0c\u8be5\u63cf\u8ff0\u4ec5\u4f9d\u8d56\u4e8e\u63d0\u8bae\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5f15\u5165\u4e86\u7efc\u5408\u8986\u76d6\u914d\u7f6e\u6587\u4ef6\uff0c\u91cf\u5316\u4e86\u76ee\u6807\u8d28\u91cf\u5728\u5bc6\u5ea6\u6bd4\u5927\u7684\u533a\u57df\u4e2d\u7684\u7a0b\u5ea6\u3002", "conclusion": "\u7edf\u4e00\u5e76\u63a8\u5e7f\u4e86\u5148\u524d\u5173\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u3001\u62d2\u7edd\u91c7\u6837\u548c\u91cd\u5c3e\u5747\u503c\u4f30\u8ba1\u7684\u5206\u6790\uff0c\u4e3a\u914d\u5206\u51fd\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6700\u5c0f\u5047\u8bbe\u7406\u8bba\u3002"}}
{"id": "2602.23388", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.23388", "abs": "https://arxiv.org/abs/2602.23388", "authors": ["Swati Sharma", "Divya V. Sharma", "Anubha Gupta"], "title": "Task-Lens: Cross-Task Utility Based Speech Dataset Profiling for Low-Resource Indian Languages", "comment": "Accepted at LREC 2026", "summary": "The rising demand for inclusive speech technologies amplifies the need for multilingual datasets for Natural Language Processing (NLP) research. However, limited awareness of existing task-specific resources in low-resource languages hinders research. This challenge is especially acute in linguistically diverse countries, such as India. Cross-task profiling of existing Indian speech datasets can alleviate the data scarcity challenge. This involves investigating the utility of datasets across multiple downstream tasks rather than focusing on a single task. Prior surveys typically catalogue datasets for a single task, leaving comprehensive cross-task profiling as an open opportunity. Therefore, we propose Task-Lens, a cross-task survey that assesses the readiness of 50 Indian speech datasets spanning 26 languages for nine downstream speech tasks. First, we analyze which datasets contain metadata and properties suitable for specific tasks. Next, we propose task-aligned enhancements to unlock datasets to their full downstream potential. Finally, we identify tasks and Indian languages that are critically underserved by current resources. Our findings reveal that many Indian speech datasets contain untapped metadata that can support multiple downstream tasks. By uncovering cross-task linkages and gaps, Task-Lens enables researchers to explore the broader applicability of existing datasets and to prioritize dataset creation for underserved tasks and languages.", "AI": {"tldr": "Task-Lens enhances the use of Indian speech datasets for NLP, addressing data scarcity and promoting inclusivity.", "motivation": "The rising demand for inclusive speech technologies and the need for multilingual datasets for NLP research.", "method": "Cross-task profiling of existing Indian speech datasets, proposing Task-Lens, a cross-task survey.", "result": "Identified untapped metadata in many Indian speech datasets, enabling broader applicability of existing datasets and prioritizing dataset creation for underserved tasks and languages.", "conclusion": "Task-Lens helps alleviate data scarcity in low-resource languages and promotes inclusive speech technologies."}}
{"id": "2602.23373", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23373", "abs": "https://arxiv.org/abs/2602.23373", "authors": ["Pavel Chernakov", "Sasan Jafarnejad", "Rapha\u00ebl Frank"], "title": "An Agentic LLM Framework for Adverse Media Screening in AML Compliance", "comment": null, "summary": "Adverse media screening is a critical component of anti-money laundering (AML) and know-your-customer (KYC) compliance processes in financial institutions. Traditional approaches rely on keyword-based searches that generate high false-positive rates or require extensive manual review. We present an agentic system that leverages Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automate adverse media screening. Our system implements a multi-step approach where an LLM agent searches the web, retrieves and processes relevant documents, and computes an Adverse Media Index (AMI) score for each subject. We evaluate our approach using multiple LLM backends on a dataset comprising Politically Exposed Persons (PEPs), persons from regulatory watchlists, and sanctioned persons from OpenSanctions and clean names from academic sources, demonstrating the system's ability to distinguish between high-risk and low-risk individuals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u548cRAG\u7684\u4e0d\u826f\u5a92\u4f53\u7b5b\u67e5\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u7b5b\u67e5\u6548\u7387\u548c\u51c6\u786e\u6027", "motivation": "\u63d0\u9ad8\u53cd\u6d17\u94b1\uff08AML\uff09\u548c\u4e86\u89e3\u4f60\u7684\u5ba2\u6237\uff08KYC\uff09\u5408\u89c4\u6d41\u7a0b\u4e2d\u4e0d\u826f\u5a92\u4f53\u7b5b\u67e5\u7684\u6548\u7387", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f", "result": "\u81ea\u52a8\u5316\u4e0d\u826f\u5a92\u4f53\u7b5b\u67e5\uff0c\u63d0\u9ad8\u7b5b\u67e5\u51c6\u786e\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u533a\u5206\u9ad8\u98ce\u9669\u548c\u4f4e\u98ce\u9669\u4e2a\u4f53"}}
{"id": "2602.23400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23400", "abs": "https://arxiv.org/abs/2602.23400", "authors": ["Zezheng Wu", "Rui Wang", "Xinghe Cheng", "Yang Shao", "Qing Yang", "Jiapu Wang", "Jingwei Zhang"], "title": "U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency.", "AI": {"tldr": "U-CAN\uff1a\u4e00\u79cd\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u9690\u79c1\u95ee\u9898\u7684\u7cbe\u786e\u53cd\u5b66\u4e60\u6846\u67b6", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u751f\u6210\u5f0f\u63a8\u8350\uff08GenRec\uff09\u4e2d\uff0c\u7531\u4e8e\u5fae\u8c03\u7528\u6237\u65e5\u5fd7\u800c\u65e0\u610f\u4e2d\u5c06\u654f\u611f\u5c5e\u6027\u7f16\u7801\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u4ece\u800c\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aU-CAN\u7684\u7cbe\u786e\u53cd\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4f4e\u79e9\u9002\u914d\u5668\u4e0a\u8fd0\u884c\u3002U-CAN\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u6d3b\u6765\u91cf\u5316\u98ce\u9669\uff0c\u91cd\u70b9\u5173\u6ce8\u5bf9\u9057\u5fd8\u96c6\u9ad8\u5ea6\u654f\u611f\u4f46\u5728\u4fdd\u7559\u96c6\u4e0a\u88ab\u6291\u5236\u7684\u5177\u6709\u4e0d\u5bf9\u79f0\u54cd\u5e94\u7684\u795e\u7ecf\u5143\u3002\u4e3a\u4e86\u4fdd\u62a4\u6027\u80fd\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u6548\u7528\u611f\u77e5\u6821\u51c6\u673a\u5236\uff0c\u8be5\u673a\u5236\u7ed3\u5408\u4e86\u6743\u91cd\u5e45\u5ea6\u548c\u4fdd\u7559\u96c6\u6fc0\u6d3b\u8303\u6570\uff0c\u5c06\u66f4\u9ad8\u7684\u6548\u7528\u5206\u6570\u5206\u914d\u7ed9\u5bf9\u4fdd\u7559\u6027\u80fd\u8d21\u732e\u8f83\u5927\u7684\u7ef4\u5ea6\u3002\u4e0e\u4e8c\u8fdb\u5236\u526a\u679d\u4e0d\u540c\uff0cU-CAN\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8f6f\u8870\u51cf\uff0c\u5177\u6709\u53ef\u5fae\u5206\u7684\u8870\u51cf\u51fd\u6570\uff0c\u4ee5\u9009\u62e9\u6027\u5730\u964d\u4f4eLoRA\u9002\u914d\u5668\u4e0a\u7684\u9ad8\u98ce\u9669\u53c2\u6570\uff0c\u6291\u5236\u654f\u611f\u7684\u68c0\u7d22\u8def\u5f84\uff0c\u5e76\u4fdd\u7559\u63a8\u7406\u7535\u8def\u7684\u62d3\u6251\u8fde\u901a\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u4e03\u4e2a\u6307\u6807\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cU-CAN\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u9057\u5fd8\u3001\u6548\u7528\u4fdd\u7559\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "U-CAN\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2602.23560", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23560", "abs": "https://arxiv.org/abs/2602.23560", "authors": ["Nicolas Constantinides"], "title": "I've Seen This IP: A Practical Intersection Attack Against Tor Introduction Circuits and Hidden Services", "comment": "11 pages, 3 figures", "summary": "Tor onion services rely on long-lived introduction circuits to support anonymous rendezvous between clients and services. Although Tor includes some defenses against traffic analysis, the introduction protocol retains deterministic routing structure that can be leveraged by an adversary. We describe a practical intersection attack on Tor introduction circuits that can, over repeated interactions, identify each hop from the introduction point toward the onion service while requiring observation at only one relay per stage. The attack issues repeated probes and intersects destination IP address sets observed within narrowly defined \\texttt{INTRODUCE1}--\\texttt{RENDEZVOUS2} time windows, without assuming global visibility or access to packet payloads. We evaluate feasibility with live-network experiments using a self-operated onion service and relays, and we follow data-minimization and ethical safeguards throughout. The results show reliable convergence in practice, with the rate affected by consensus weight, and time-varying background traffic. We also assess practicality under a partial-global adversary model and discuss implications in light of the geographic concentration of Tor relay weight across cooperating jurisdictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Tor\u6d0b\u8471\u670d\u52a1\u7684\u4ea4\u70b9\u653b\u51fb\uff0c\u5b9e\u9a8c\u8868\u660e\u653b\u51fb\u53ef\u884c\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5b9e\u7528\u6027\u548c\u5f71\u54cd\u3002", "motivation": "\u533f\u540d\u901a\u4fe1\u4e2d\uff0cTor\u6d0b\u8471\u670d\u52a1\u4f9d\u8d56\u957f\u751f\u5b58\u671f\u5f15\u5165\u7535\u8def\u652f\u6301\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u7684\u533f\u540d\u4f1a\u8bdd\u3002\u7136\u800c\uff0c\u5f15\u5165\u534f\u8bae\u4fdd\u7559\u4e86\u53ef\u88ab\u653b\u51fb\u8005\u5229\u7528\u7684\u786e\u5b9a\u6027\u8def\u7531\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Tor\u5f15\u5165\u7535\u8def\u7684\u4ea4\u70b9\u653b\u51fb\uff0c\u901a\u8fc7\u91cd\u590d\u4ea4\u4e92\uff0c\u5728\u53ea\u9700\u89c2\u5bdf\u6bcf\u4e2a\u9636\u6bb5\u7684\u5355\u4e2a\u4e2d\u7ee7\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u4ece\u5f15\u5165\u70b9\u5230\u6d0b\u8471\u670d\u52a1\u7684\u6bcf\u4e2a\u8df3\u3002\u653b\u51fb\u901a\u8fc7\u5728\u72ed\u7a84\u5b9a\u4e49\u7684INTRODUCE1--RENDEZVOUS2\u65f6\u95f4\u7a97\u53e3\u5185\u91cd\u590d\u63a2\u6d4b\u548c\u4ea4\u70b9\u76ee\u7684\u5730IP\u5730\u5740\u96c6\u6765\u8bc6\u522b\u6bcf\u4e2a\u8df3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u653b\u51fb\u5728\u5b9e\u8df5\u4e2d\u5177\u6709\u53ef\u9760\u7684\u6536\u655b\u6027\uff0c\u53d7\u5171\u8bc6\u6743\u91cd\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u80cc\u666f\u6d41\u91cf\u5f71\u54cd\u3002\u5728\u90e8\u5206\u5168\u5c40\u5bf9\u624b\u6a21\u578b\u4e0b\u8bc4\u4f30\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u4e2dTor\u4e2d\u7ee7\u6743\u91cd\u7684\u5730\u7406\u96c6\u4e2d\u5e26\u6765\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u653b\u51fb\u5bf9Tor\u6d0b\u8471\u670d\u52a1\u7684\u5b89\u5168\u6027\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u6539\u8fdb\u6765\u589e\u5f3a\u533f\u540d\u901a\u4fe1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.23523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23523", "abs": "https://arxiv.org/abs/2602.23523", "authors": ["Junjiang Wu", "Liejun Wang", "Zhiqing Guo"], "title": "All in One: Unifying Deepfake Detection, Tampering Localization, and Source Tracing with a Robust Landmark-Identity Watermark", "comment": "Accepted by CVPR 2026", "summary": "With the rapid advancement of deepfake technology, malicious face manipulations pose a significant threat to personal privacy and social security. However, existing proactive forensics methods typically treat deepfake detection, tampering localization, and source tracing as independent tasks, lacking a unified framework to address them jointly. To bridge this gap, we propose a unified proactive forensics framework that jointly addresses these three core tasks. Our core framework adopts an innovative 152-dimensional landmark-identity watermark termed LIDMark, which structurally interweaves facial landmarks with a unique source identifier. To robustly extract the LIDMark, we design a novel Factorized-Head Decoder (FHD). Its architecture factorizes the shared backbone features into two specialized heads (i.e., regression and classification), robustly reconstructing the embedded landmarks and identifier, respectively, even when subjected to severe distortion or tampering. This design realizes an \"all-in-one\" trifunctional forensic solution: the regression head underlies an \"intrinsic-extrinsic\" consistency check for detection and localization, while the classification head robustly decodes the source identifier for tracing. Extensive experiments show that the proposed LIDMark framework provides a unified, robust, and imperceptible solution for the detection, localization, and tracing of deepfake content. The code is available at https://github.com/vpsg-research/LIDMark.", "code_url": "https://github.com/vpsg-research/LIDMark", "code_stars": 3, "code_last_update": "2026-03-02", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4e3b\u52a8\u53d6\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u8ffd\u8e2a\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u5177\u6709\u7edf\u4e00\u3001\u9c81\u68d2\u548c\u4e0d\u6613\u5bdf\u89c9\u7684\u7279\u70b9\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6076\u610f\u7684\u4eba\u8138\u64cd\u7eb5\u5bf9\u4e2a\u4eba\u9690\u79c1\u548c\u793e\u4f1a\u5b89\u5168\u6784\u6210\u4e86\u91cd\u5927\u5a01\u80c1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4e3b\u52a8\u53d6\u8bc1\u65b9\u6cd5\u901a\u5e38\u5c06\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3001\u7be1\u6539\u5b9a\u4f4d\u548c\u6eaf\u6e90\u89c6\u4e3a\u72ec\u7acb\u7684\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5171\u540c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4e3b\u52a8\u53d6\u8bc1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u4e00\u79cd\u521b\u65b0\u7684152\u7ef4\u5730\u6807-\u8eab\u4efd\u6c34\u5370\uff0c\u79f0\u4e3aLIDMark\uff0c\u5b83\u5c06\u9762\u90e8\u5730\u6807\u4e0e\u72ec\u7279\u7684\u6e90\u6807\u8bc6\u7b26\u7ed3\u6784\u6027\u5730\u4ea4\u7ec7\u5728\u4e00\u8d77\u3002\u4e3a\u4e86\u9c81\u68d2\u5730\u63d0\u53d6LIDMark\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u5b50\u5934\u89e3\u7801\u5668\uff08FHD\uff09\u3002\u5176\u67b6\u6784\u5c06\u5171\u4eab\u9aa8\u5e72\u7279\u5f81\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e13\u7528\u5934\uff08\u5373\u56de\u5f52\u548c\u5206\u7c7b\uff09\uff0c\u5206\u522b\u9c81\u68d2\u5730\u91cd\u5efa\u5d4c\u5165\u7684\u5730\u6807\u548c\u6807\u8bc6\u7b26\uff0c\u5373\u4f7f\u5728\u4e25\u91cd\u626d\u66f2\u6216\u7be1\u6539\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4e00\u4e2a\u201c\u4e00\u7ad9\u5f0f\u201d\u7684\u4e09\u529f\u80fd\u53d6\u8bc1\u89e3\u51b3\u65b9\u6848\uff1a\u56de\u5f52\u5934\u4e3a\u68c0\u6d4b\u548c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u201c\u5185\u5728-\u5916\u5728\u201d\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u800c\u5206\u7c7b\u5934\u9c81\u68d2\u5730\u89e3\u7801\u6e90\u6807\u8bc6\u7b26\u4ee5\u8fdb\u884c\u8ffd\u8e2a\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684LIDMark\u6846\u67b6\u4e3a\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u9c81\u68d2\u4e14\u4e0d\u6613\u5bdf\u89c9\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LIDMark\u6846\u67b6\u4e3a\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u6eaf\u6e90\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7edf\u4e00\u3001\u9c81\u68d2\u548c\u4e0d\u6613\u5bdf\u89c9\u7684\u7279\u70b9\u3002"}}
{"id": "2602.23602", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23602", "abs": "https://arxiv.org/abs/2602.23602", "authors": ["Yoichi Chikahara"], "title": "Moment Matters: Mean and Variance Causal Graph Discovery from Heteroscedastic Observational Data", "comment": "17 pages, 6 figures", "summary": "Heteroscedasticity -- where the variance of a variable changes with other variables -- is pervasive in real data, and elucidating why it arises from the perspective of statistical moments is crucial in scientific knowledge discovery and decision-making. However, standard causal discovery does not reveal which causes act on the mean versus the variance, as it returns a single moment-agnostic graph, limiting interpretability and downstream intervention design. We propose a Bayesian, moment-driven causal discovery framework that infers separate \\textit{mean} and \\textit{variance} causal graphs from observational heteroscedastic data. We first derive the identification results by establishing sufficient conditions under which these two graphs are separately identifiable. Building on this theory, we develop a variational inference method that learns a posterior distribution over both graphs, enabling principled uncertainty quantification of structural features (e.g., edges, paths, and subgraphs). To address the challenges of parameter optimization in heteroscedastic models with two graph structures, we take a curvature-aware optimization approach and develop a prior incorporation technique that leverages domain knowledge on node orderings, improving sample efficiency. Experiments on synthetic, semi-synthetic, and real data show that our approach accurately recovers mean and variance structures and outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u77e9\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5f02\u65b9\u5dee\u6570\u636e\u4e2d\u8bc6\u522b\u5747\u503c\u548c\u65b9\u5dee\u7ed3\u6784\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u9610\u660e\u5f02\u65b9\u5dee\u6027\u4ea7\u751f\u7684\u539f\u56e0\u5bf9\u4e8e\u79d1\u5b66\u77e5\u8bc6\u53d1\u73b0\u548c\u51b3\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\uff0c\u57fa\u4e8e\u7edf\u8ba1\u77e9\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u4ece\u89c2\u5bdf\u5230\u7684\u5f02\u65b9\u5dee\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u5206\u522b\u7684\u5747\u503c\u548c\u65b9\u5dee\u56e0\u679c\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u51c6\u786e\u6062\u590d\u5747\u503c\u548c\u65b9\u5dee\u7ed3\u6784\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f02\u65b9\u5dee\u6a21\u578b\u4e2d\u5177\u6709\u8f83\u597d\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5747\u503c\u548c\u65b9\u5dee\u7ed3\u6784\uff0c\u4e3a\u79d1\u5b66\u77e5\u8bc6\u53d1\u73b0\u548c\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2602.23440", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23440", "abs": "https://arxiv.org/abs/2602.23440", "authors": ["Chris Samarinas", "Haw-Shiuan Chang", "Hamed Zamani"], "title": "Truncated Step-Level Sampling with Process Rewards for Retrieval-Augmented Reasoning", "comment": null, "summary": "Training large language models to reason with search engines via reinforcement learning is hindered by a fundamental credit assignment problem: existing methods such as Search-R1 provide only a sparse outcome reward after an entire multi-step trajectory, making it infeasible to attribute success or failure to individual reasoning and retrieval decisions. Process-reward methods like StepSearch alleviate this by introducing step-level supervision, but rely on heuristic rewards such as TF-IDF overlap with gold documents, and still sample k complete trajectories per example, retaining high gradient variance. We propose SLATE, a framework built on two complementary ideas: (1) truncated step-level sampling, which generates k trajectories that share a common prefix and differ only at the next step, and (2) dense LLM-as-judge rewards, which replace heuristic scoring with a capable LLM evaluator that assesses the quality of each reasoning step, search query, and answer, providing richer and more reliable supervision. We theoretically prove that under the same dense reward structure, truncated sampling reduces the variance of advantage estimates by up to a factor of T compared to full-trajectory sampling for T-step trajectories, yielding lower-variance, better-targeted policy gradients. Experiments on seven QA benchmarks confirm that SLATE consistently outperforms both sparse-reward and process-reward baselines, with the largest gains on harder multi-hop tasks and smaller models.", "AI": {"tldr": "SLATE improves reinforcement learning for language models by reducing variance and outperforming baselines.", "motivation": "Training large language models to reason with search engines via reinforcement learning faces a credit assignment problem due to sparse outcome rewards.", "method": "Proposing SLATE, a framework with truncated step-level sampling and dense LLM-as-judge rewards.", "result": "SLATE reduces the variance of advantage estimates and outperforms baselines on QA benchmarks.", "conclusion": "SLATE is an effective framework for training language models to reason with search engines."}}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.", "AI": {"tldr": "\u63d0\u51faCTFIDU+\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u7b2c\u4e09\u5c42\u53cd\u4e8b\u5b9e\u8bc6\u522b\u7684\u7406\u8bba\u6781\u9650\uff0c\u5e76\u8bc1\u660e\u4e86\u53cd\u4e8b\u5b9e\u6570\u636e\u5728\u5206\u6790\u975e\u53ef\u8bc6\u522b\u91cf\u65f6\u7684\u4f5c\u7528\u3002", "motivation": "\u5206\u6790\u53cd\u4e8b\u5b9e\u8bc6\u522b\u5728\u4e09\u5c42\u56e0\u679c\u5c42\u6b21\u7ed3\u6784\u4e2d\u7b2c\u4e09\u5c42\u6570\u636e\u7684\u65b0\u8bbf\u95ee\u65b9\u5f0f\uff0c\u4ee5\u53ca\u5b83\u5bf9\u53ef\u8bc6\u522b\u53cd\u4e8b\u5b9e\u6570\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1CTFIDU+\u7b97\u6cd5\u4ee5\u4ece\u4efb\u610f\u7b2c\u4e09\u5c42\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4efb\u52a1\u4e0a\u7684\u5b8c\u5907\u6027\u3002\u5efa\u7acb\u7406\u8bba\u6781\u9650\uff0c\u4ee5\u786e\u5b9a\u54ea\u4e9b\u53cd\u4e8b\u5b9e\u53ef\u4ee5\u4ece\u7269\u7406\u53ef\u5b9e\u73b0\u5206\u5e03\u4e2d\u8bc6\u522b\u51fa\u6765\u3002\u4f7f\u7528\u53ef\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u6570\u636e\u63a8\u5bfc\u51fa\u65b0\u578b\u5206\u6790\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u8bc1\u5b9e\u53cd\u4e8b\u5b9e\u6570\u636e\u6709\u52a9\u4e8e\u5728\u5b9e\u8df5\u4e2d\u6536\u7d27\u975e\u53ef\u8bc6\u522b\u91cf\u7684\u754c\u9650\u3002", "result": "\u8bc1\u660e\u4e86CTFIDU+\u7b97\u6cd5\u5728\u8bc6\u522b\u7b2c\u4e09\u5c42\u53cd\u4e8b\u5b9e\u67e5\u8be2\u65b9\u9762\u7684\u5b8c\u5907\u6027\uff0c\u786e\u5b9a\u4e86\u53ef\u8bc6\u522b\u53cd\u4e8b\u5b9e\u7684\u7406\u8bba\u6781\u9650\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e86\u65b0\u578b\u5206\u6790\u754c\u9650\uff0c\u8bc1\u5b9e\u4e86\u53cd\u4e8b\u5b9e\u6570\u636e\u5728\u6536\u7d27\u975e\u53ef\u8bc6\u522b\u91cf\u754c\u9650\u65b9\u9762\u7684\u4f5c\u7528\u3002", "conclusion": "CTFIDU+\u7b97\u6cd5\u4e3a\u4ece\u7b2c\u4e09\u5c42\u6570\u636e\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7cbe\u786e\u56e0\u679c\u63a8\u65ad\u7684\u6839\u672c\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528\u53ef\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u6570\u636e\u8fdb\u884c\u5206\u6790\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian W\u00f6lckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u89d2\u5ea6\u7f16\u7801\u5b9e\u73b0\u51fd\u6570\u903c\u8fd1\uff0c\u4e09\u8fdb\u5236\u7f51\u683c\u521d\u59cb\u5316\u514b\u670d\u9891\u7387\u53ef\u8fbe\u6027\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63d0\u51fa\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f7f\u7528\u89d2\u5ea6\u7f16\u7801\u4ee5\u81ea\u7136\u5730\u8868\u793a\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u901a\u7528\u7684\u51fd\u6570\u903c\u8fd1\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53ef\u8bad\u7ec3\u9891\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u9891\u7387\u524d\u56e0\u5b50\u53ef\u8bad\u7ec3\u6027\u6709\u9650\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7f51\u683c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u4e09\u8fdb\u5236\u7f16\u7801\u4ee5\u514b\u670d\u9891\u7387\u53ef\u8fbe\u6027\u9650\u5236\u3002", "result": "\u4e09\u8fdb\u5236\u7f51\u683c\u521d\u59cb\u5316\u5728\u5408\u6210\u76ee\u6807\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u53ef\u8bad\u7ec3\u9891\u7387\u65b9\u6cd5\uff0c\u5206\u522b\u8fbe\u52300.9969\u548c0.9671\u7684R^2\u5206\u6570\uff0c\u5206\u522b\u63d0\u9ad8\u4e8622.8%\u548c18.9%\u3002", "conclusion": "\u4e09\u8fdb\u5236\u7f51\u683c\u521d\u59cb\u5316\u662f\u514b\u670d\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9891\u7387\u53ef\u8fbe\u6027\u9650\u5236\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.23569", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23569", "abs": "https://arxiv.org/abs/2602.23569", "authors": ["Vincent Langford", "Shihan Zhao", "Hongyu Zhang", "Ben Dong", "Qian Wang", "Anees Rehman", "Yuntao Liu"], "title": "CLOAQ: Combined Logic and Angle Obfuscation for Quantum Circuits", "comment": "To appear at ISCAS 2026", "summary": "In the realm of quantum computing, quantum circuits serve as essential depictions of quantum algorithms, which are then compiled into executable operations for quantum computations. Quantum compilers are responsible for converting these algorithmic quantum circuits into versions compatible with specific quantum hardware, thus connecting quantum software with hardware. Nevertheless, untrusted quantum compilers present notable threats. They have the potential to result in the theft of quantum circuit designs and jeopardize sensitive intellectual property (IP). In this work, we propose CLOAQ, a quantum circuit obfuscation (QCO) approach that hides the logic and the phase angles of selected gates within the obfuscated quantum circuit. To evaluate the effectiveness of CLOAQ, we sample the input state uniformly from the Hilbert space of all qubits, which is more accurate than prior work that use all-|0> inputs. Our results show that CLOAQ benefits from the synergy between logic and phase protections. Compared with prior QCO approaches using only one perspective, the combined method is more resilient to attacks and causes greater functional disruption when the unlocking key is incorrect.", "AI": {"tldr": "CLOAQ\uff1a\u4e00\u79cd\u6709\u6548\u7684\u91cf\u5b50\u7535\u8def\u6df7\u6dc6\u65b9\u6cd5\uff0c\u63d0\u9ad8\u91cf\u5b50\u7f16\u8bd1\u5668\u5b89\u5168\u6027\u3002", "motivation": "\u91cf\u5b50\u7f16\u8bd1\u5668\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u88ab\u76d7\u548c\u77e5\u8bc6\u4ea7\u6743\u53d7\u5230\u5a01\u80c1\u3002", "method": "\u63d0\u51faCLOAQ\u91cf\u5b50\u7535\u8def\u6df7\u6dc6\u65b9\u6cd5\uff0c\u9690\u85cf\u9009\u5b9a\u7684\u95e8\u7684\u903b\u8f91\u548c\u76f8\u4f4d\u89d2\u5ea6\u3002", "result": "CLOAQ\u901a\u8fc7\u903b\u8f91\u548c\u76f8\u4f4d\u4fdd\u62a4\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u5bf9\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\uff0c\u5e76\u5728\u89e3\u9501\u5bc6\u94a5\u9519\u8bef\u65f6\u9020\u6210\u66f4\u5927\u7684\u529f\u80fd\u7834\u574f\u3002", "conclusion": "CLOAQ\u662f\u4e00\u79cd\u6709\u6548\u7684\u91cf\u5b50\u7535\u8def\u6df7\u6dc6\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5b50\u7f16\u8bd1\u5668\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.23543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23543", "abs": "https://arxiv.org/abs/2602.23543", "authors": ["Ziqi Gao", "Jieyu Zhang", "Wisdom Oluchi Ikezogwo", "Jae Sung Park", "Tario G. You", "Daniel Ogbu", "Chenhao Zheng", "Weikai Huang", "Yinuo Yang", "Winson Han", "Quan Kong", "Rajat Saini", "Ranjay Krishna"], "title": "Synthetic Visual Genome 2: Extracting Large-scale Spatio-Temporal Scene Graphs from Videos", "comment": null, "summary": "We introduce Synthetic Visual Genome 2 (SVG2), a large-scale panoptic video scene graph dataset. SVG2 contains over 636K videos with 6.6M objects, 52.0M attributes, and 6.7M relations, providing an order-of-magnitude increase in scale and diversity over prior spatio-temporal scene graph datasets. To create SVG2, we design a fully automated pipeline that combines multi-scale panoptic segmentation, online-offline trajectory tracking with automatic new-object discovery, per-trajectory semantic parsing, and GPT-5-based spatio-temporal relation inference. Building on this resource, we train TRaSER, a video scene graph generation model. TRaSER augments VLMs with a trajectory-aligned token arrangement mechanism and new modules: an object-trajectory resampler and a temporal-window resampler to convert raw videos and panoptic trajectories into compact spatio-temporal scene graphs in a single forward pass. The temporal-window resampler binds visual tokens to short trajectory segments to preserve local motion and temporal semantics, while the object-trajectory resampler aggregates entire trajectories to maintain global context for objects. On the PVSG, VIPSeg, VidOR and SVG2 test datasets, TRaSER improves relation detection by +15 to 20%, object prediction by +30 to 40% over the strongest open-source baselines and by +13% over GPT-5, and attribute prediction by +15%. When TRaSER's generated scene graphs are sent to a VLM for video question answering, it delivers a +1.5 to 4.6% absolute accuracy gain over using video only or video augmented with Qwen2.5-VL's generated scene graphs, demonstrating the utility of explicit spatio-temporal scene graphs as an intermediate representation.", "AI": {"tldr": "\u6211\u4eec\u5f00\u53d1\u4e86SVG2\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u8de8\u65f6\u7a7a\u573a\u666f\u56fe\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86TRaSER\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u66f4\u5927\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Synthetic Visual Genome 2\uff08SVG2\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u8de8\u65f6\u7a7a\u573a\u666f\u56fe\u6570\u636e\u96c6\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u6d41\u7a0b\u6765\u521b\u5efaSVG2\uff0c\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u5168\u666f\u5206\u5272\u3001\u5728\u7ebf-\u79bb\u7ebf\u8f68\u8ff9\u8ddf\u8e2a\u3001\u81ea\u52a8\u65b0\u5bf9\u8c61\u53d1\u73b0\u3001\u8f68\u8ff9\u8bed\u4e49\u89e3\u6790\u548c\u57fa\u4e8eGPT-5\u7684\u65f6\u7a7a\u5173\u7cfb\u63a8\u7406\u3002", "result": "\u57fa\u4e8eSVG2\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86TRaSER\uff0c\u4e00\u4e2a\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\u3002TRaSER\u5728PVSG\u3001VIPSeg\u3001VidOR\u548cSVG2\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u5173\u7cfb\u68c0\u6d4b\u3001\u5bf9\u8c61\u9884\u6d4b\u548c\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "TRaSER\u751f\u6210\u7684\u573a\u666f\u56fe\u5728\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u65f6\u7a7a\u573a\u666f\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u7684\u6548\u7528\u3002"}}
{"id": "2602.23611", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23611", "abs": "https://arxiv.org/abs/2602.23611", "authors": ["Yoichi Chikahara"], "title": "Fairness under Graph Uncertainty: Achieving Interventional Fairness with Partially Known Causal Graphs over Clusters of Variables", "comment": "26 pages, 9 figures", "summary": "Algorithmic decisions about individuals require predictions that are not only accurate but also fair with respect to sensitive attributes such as gender and race. Causal notions of fairness align with legal requirements, yet many methods assume access to detailed knowledge of the underlying causal graph, which is a demanding assumption in practice. We propose a learning framework that achieves interventional fairness by leveraging a causal graph over \\textit{clusters of variables}, which is substantially easier to estimate than a variable-level graph. With possible \\textit{adjustment cluster sets} identified from such a cluster causal graph, our framework trains a prediction model by reducing the worst-case discrepancy between interventional distributions across these sets. To this end, we develop a computationally efficient barycenter kernel maximum mean discrepancy (MMD) that scales favorably with the number of sensitive attribute values. Extensive experiments show that our framework strikes a better balance between fairness and accuracy than existing approaches, highlighting its effectiveness under limited causal graph knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u91cf\u7c07\u56e0\u679c\u56fe\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5e72\u9884\u5206\u5e03\u95f4\u7684\u6700\u5927\u504f\u5dee\u6765\u5b9e\u73b0\u5e72\u9884\u516c\u5e73\uff0c\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u7b97\u6cd5\u51b3\u7b56\u5bf9\u4e2a\u4eba\u9700\u8981\u51c6\u786e\u4e14\u5bf9\u654f\u611f\u5c5e\u6027\u5982\u6027\u522b\u548c\u79cd\u65cf\u516c\u5e73\u7684\u9884\u6d4b\u3002\u56e0\u679c\u516c\u5e73\u6982\u5ff5\u4e0e\u6cd5\u5f8b\u8981\u6c42\u4e00\u81f4\uff0c\u4f46\u8bb8\u591a\u65b9\u6cd5\u5047\u8bbe\u5bf9\u5e95\u5c42\u56e0\u679c\u56fe\u7684\u8be6\u7ec6\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u53d8\u91cf\u7c07\u4e0a\u7684\u56e0\u679c\u56fe\u6765\u5b9e\u73b0\u5e72\u9884\u516c\u5e73\uff0c\u4ece\u800c\u964d\u4f4e\u5bf9\u5e95\u5c42\u56e0\u679c\u56fe\u7684\u8be6\u7ec6\u77e5\u8bc6\u7684\u9700\u6c42\u3002", "result": "\u901a\u8fc7\u8bc6\u522b\u6765\u81ea\u7c07\u56e0\u679c\u56fe\u7684\u53ef\u80fd\u7684\u8c03\u6574\u7c07\u96c6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u8fd9\u4e9b\u96c6\u4e4b\u95f4\u5e72\u9884\u5206\u5e03\u4e4b\u95f4\u7684\u6700\u5927\u504f\u5dee\u6765\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u3002\u5f00\u53d1\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u8d28\u5fc3\u6838\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6709\u9650\u56e0\u679c\u56fe\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2602.23452", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.23452", "abs": "https://arxiv.org/abs/2602.23452", "authors": ["Zhengqing Yuan", "Kaiwen Shi", "Zheyuan Zhang", "Lichao Sun", "Nitesh V. Chawla", "Yanfang Ye"], "title": "CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era", "comment": null, "summary": "Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u79d1\u5b66\u5199\u4f5c\u4e2d\u5e7b\u89c9\u5f15\u7528\u7684\u7efc\u5408\u57fa\u51c6\u548c\u68c0\u6d4b\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u79d1\u5b66\u5f15\u7528\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u4e2d\u51c6\u786e\u5f15\u7528\u5bf9\u4e8e\u5f52\u5c5e\u548c\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5f15\u5165\u4e86\u65b0\u7684\u98ce\u9669\uff1a\u865a\u6784\u7684\u53c2\u8003\u6587\u732e\u770b\u4f3c\u53ef\u4fe1\uff0c\u4f46\u5b9e\u9645\u4e0a\u5e76\u4e0d\u5bf9\u5e94\u4efb\u4f55\u771f\u5b9e\u51fa\u7248\u7269\u3002\u8fd9\u79cd\u5e7b\u89c9\u5f15\u7528\u5df2\u5728\u4e3b\u8981\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u7684\u63d0\u4ea4\u548c\u5df2\u63a5\u53d7\u7684\u8bba\u6587\u4e2d\u89c2\u5bdf\u5230\uff0c\u66b4\u9732\u4e86\u540c\u884c\u8bc4\u5ba1\u7684\u6f0f\u6d1e\u3002\u540c\u65f6\uff0c\u5feb\u901f\u589e\u957f\u7684\u53c2\u8003\u6587\u732e\u5217\u8868\u4f7f\u624b\u52a8\u9a8c\u8bc1\u4e0d\u5207\u5b9e\u9645\uff0c\u73b0\u6709\u7684\u81ea\u52a8\u5316\u5de5\u5177\u5bf9\u5608\u6742\u548c\u5f02\u6784\u7684\u5f15\u7528\u683c\u5f0f\u4ecd\u7136\u8106\u5f31\uff0c\u5e76\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u79d1\u5b66\u5199\u4f5c\u4e2d\u5e7b\u89c9\u5f15\u7528\u7684\u7efc\u5408\u57fa\u51c6\u548c\u68c0\u6d4b\u6846\u67b6\u3002\u6211\u4eec\u7684\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u7ba1\u9053\u5c06\u5f15\u7528\u68c0\u67e5\u5206\u89e3\u4e3a\u7d22\u8d54\u63d0\u53d6\u3001\u8bc1\u636e\u68c0\u7d22\u3001\u6bb5\u843d\u5339\u914d\u3001\u63a8\u7406\u548c\u6821\u51c6\u5224\u65ad\uff0c\u4ee5\u8bc4\u4f30\u6240\u5f15\u7528\u7684\u6765\u6e90\u662f\u5426\u771f\u6b63\u652f\u6301\u5176\u4e3b\u5f20\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u5927\u578b\u4eba\u7c7b\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5e76\u5b9a\u4e49\u4e86\u7edf\u4e00\u7684\u6807\u51c6\u6765\u8861\u91cf\u5f15\u7528\u7684\u5fe0\u5b9e\u5ea6\u548c\u8bc1\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3aLLM\u65f6\u4ee3\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5ba1\u8ba1\u5f15\u7528\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u4e3a\u63d0\u9ad8\u79d1\u5b66\u5f15\u7528\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLM\u65f6\u4ee3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f15\u7528\u5ba1\u8ba1\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u6709\u52a9\u4e8e\u63d0\u9ad8\u79d1\u5b66\u5f15\u7528\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2602.23545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23545", "abs": "https://arxiv.org/abs/2602.23545", "authors": ["Matteo Ceriscioli", "Karthika Mohan"], "title": "Planning under Distribution Shifts with Causal POMDPs", "comment": "To appear at the 36th International Conference on Automated Planning and Scheduling (ICAPS-26)", "summary": "In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $\u03b1$-vector-based POMDP methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u77e5\u8bc6\u7684POMDP\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5e94\u5bf9\u5206\u5e03\u53d8\u5316\uff0c\u5e76\u4fdd\u6301\u89c4\u5212\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u7531\u4e8e\u5206\u5e03\u53d8\u5316\uff0c\u89c4\u5212\u7ecf\u5e38\u53d7\u5230\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5728\u4e00\u79cd\u6761\u4ef6\u4e0b\u83b7\u5f97\u7684\u5173\u4e8e\u73af\u5883\u7684\u6a21\u578b\u53ef\u80fd\u4e0d\u518d\u6709\u6548\uff0c\u56e0\u4e3a\u72b6\u6001\u5206\u5e03\u6216\u73af\u5883\u52a8\u6001\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u53cd\u8fc7\u6765\u53c8\u5bfc\u81f4\u4e4b\u524d\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5931\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u56e0\u679c\u77e5\u8bc6\u516c\u7406\u5316\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4e0b\u8fdb\u884c\u89c4\u5212\u3002\u901a\u8fc7\u5c06\u73af\u5883\u7684\u53d8\u5316\u8868\u793a\u4e3a\u5bf9\u56e0\u679cPOMDP\u7684\u5e72\u9884\uff0c\u8be5\u6846\u67b6\u4f7f\u8bc4\u4f30\u5047\u8bbe\u53d8\u5316\u4e0b\u7684\u8ba1\u5212\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4e3b\u52a8\u8bc6\u522b\u54ea\u4e9b\u73af\u5883\u7ec4\u4ef6\u5df2\u66f4\u6539\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u589e\u5f3a\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\uff0c\u503c\u51fd\u6570\u4fdd\u6301\u5206\u6bb5\u7ebf\u6027\u4e14\u51f8\uff08PWLC\uff09\u3002\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u4fdd\u6301PWLC\u7684\u4f18\u70b9\u662f\uff0c\u901a\u8fc7\u57fa\u4e8e$\u03b1$\u5411\u91cf\u7684POMDP\u65b9\u6cd5\u4fdd\u6301\u89c4\u5212\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u5206\u5e03\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u73af\u5883\u53d8\u5316\u5e76\u4fdd\u6301\u89c4\u5212\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.23587", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.23587", "abs": "https://arxiv.org/abs/2602.23587", "authors": ["Ning Lyu", "Yuntao Liu", "Yonghong Bai", "Zhiyuan Yan"], "title": "PDF: PUF-based DNN Fingerprinting for Knowledge Distillation Traceability", "comment": null, "summary": "Knowledge distillation transfers large teacher models to compact student models, enabling deployment on resource-limited platforms while suffering minimal performance degradation. However, this paradigm could lead to various security risks, especially model theft. Existing defenses against model theft, such as watermarking and secure enclaves, focus primarily on identity authentication and incur significant resource costs. Aiming to provide post-theft accountability and traceability, we propose a novel fingerprinting framework that superimposes device-specific Physical Unclonable Function (PUF) signatures onto teacher logits during distillation. Compared with watermarking or secure enclaves, our approach is lightweight, requires no architectural changes, and enables traceability of any leaked or cloned model. Since the signatures are based on PUFs, this framework is robust against reverse engineering and tampering attacks. In this framework, the signature recovery process consists of two stages: first a neural network-based decoder and then a Hamming distance decoder. Furthermore, we also propose a bit compression scheme to support a large number of devices. Experiment results demonstrate that our framework achieves high key recovery rate and negligible accuracy loss while allowing a tunable trade-off between these two key metrics. These results show that the proposed framework is a practical and robust solution for protecting distilled models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePUF\u7b7e\u540d\u7684\u6307\u7eb9\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4\u84b8\u998f\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u5bc6\u94a5\u6062\u590d\u7387\u548c\u53ef\u5ffd\u7565\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u6a21\u578b\u88ab\u76d7\u540e\u7684\u8d23\u4efb\u5f52\u5c5e\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u7eb9\u6846\u67b6\u3002", "method": "\u5728\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u7279\u5b9a\u4e8e\u8bbe\u5907\u7684\u7269\u7406\u4e0d\u53ef\u514b\u9686\u529f\u80fd\uff08PUF\uff09\u7b7e\u540d\u53e0\u52a0\u5230\u6559\u5e08\u6a21\u578b\u7684logits\u4e0a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u5bc6\u94a5\u6062\u590d\u7387\u548c\u53ef\u5ffd\u7565\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u540c\u65f6\u5141\u8bb8\u5728\u4e24\u4e2a\u5173\u952e\u6307\u6807\u4e4b\u95f4\u8fdb\u884c\u53ef\u8c03\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u5065\u58ee\u7684\u4fdd\u62a4\u84b8\u998f\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23553", "abs": "https://arxiv.org/abs/2602.23553", "authors": ["Shawn Liang", "Sahil Shah", "Chengwei Zhou", "SP Sharan", "Harsh Goel", "Arnab Sanyal", "Sandeep Chinchali", "Gourav Datta"], "title": "LE-NeuS: Latency-Efficient Neuro-Symbolic Video Understanding via Adaptive Temporal Verification", "comment": "Under review", "summary": "Neuro-symbolic approaches to long-form video question answering (LVQA) have demonstrated significant accuracy improvements by grounding temporal reasoning in formal verification. However, existing methods incur prohibitive latency overheads, up to 90x slower than base VLM prompting, rendering them impractical for latency-sensitive edge deployments. We present LE-NeuS, a latency-efficient neuro-symbolic framework that preserves the accuracy benefits of temporal logic-guided video understanding while drastically reducing inference latency. Our key insight is that the dominant computational bottleneck arises from sequential and dense proposition detection across video frames during automaton construction. We address this through two principled optimizations: (1) CLIP guided two-stage adaptive sampling that exploits visual redundancy to skip semantically similar frames while preserving temporal boundaries, and (2) batched proposition detection that parallelizes VLM inference across temporal windows. Theoretically, we derive latency bounds as a function of video length, proposition complexity, and sampling density, establishing conditions under which latency efficiency is achievable. Empirically, on LongVideoBench and Video-MME benchmarks deployed on NVIDIA H100 GPUs, LE-NeuS reduces the latency gap from 90x to approximately 10x while maintaining >10% accuracy gains on temporally complex queries.", "AI": {"tldr": "LE-NeuS improves LVQA efficiency with reduced latency and maintained accuracy.", "motivation": "Neuro-symbolic approaches to LVQA have shown significant accuracy improvements, but existing methods have high latency, making them impractical for edge deployments.", "method": "LE-NeuS, a latency-efficient neuro-symbolic framework that optimizes automaton construction through two-stage adaptive sampling and batched proposition detection.", "result": "LE-NeuS reduces latency from 90x to approximately 10x while maintaining >10% accuracy gains on temporally complex queries.", "conclusion": "LE-NeuS is a practical solution for LVQA with reduced latency and maintained accuracy."}}
{"id": "2602.23479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23479", "abs": "https://arxiv.org/abs/2602.23479", "authors": ["Michael Frew", "Nishit Bheda", "Bryan Tripp"], "title": "FHIRPath-QA: Executable Question Answering over FHIR Electronic Health Records", "comment": "Submitted to LREC 2026 CL4Health Workshop", "summary": "Though patients are increasingly granted digital access to their electronic health records (EHRs), existing interfaces may not support precise, trustworthy answers to patient-specific questions. Large language models (LLM) show promise in clinical question answering (QA), but retrieval-based approaches are computationally inefficient, prone to hallucination, and difficult to deploy over real-life EHRs. In this work, we introduce FHIRPath-QA, the first open dataset and benchmark for patient-specific QA that includes open-standard FHIRPath queries over real-world clinical data. We propose a text-to-FHIRPath QA paradigm that shifts reasoning from free-text generation to FHIRPath query synthesis, significantly reducing LLM usage. Built on MIMIC-IV on FHIR Demo, the dataset pairs over 14k natural language questions in patient and clinician phrasing with validated FHIRPath queries and answers. Further, we demonstrate that state-of-the-art LLMs struggle to deal with ambiguity in patient language and perform poorly in FHIRPath query synthesis. However, they benefit strongly from supervised fine-tuning. Our results highlight that text-to-FHIRPath synthesis has the potential to serve as a practical foundation for safe, efficient, and interoperable consumer health applications, and our dataset and benchmark serve as a starting point for future research on the topic. The full dataset and generation code is available at: https://github.com/mooshifrew/fhirpath-qa.", "code_url": "https://github.com/mooshifrew/fhirpath-qa", "code_stars": 0, "code_last_update": "2026-02-25", "AI": {"tldr": "\u63d0\u51fa\u4e86FHIRPath-QA\uff0c\u4e00\u79cd\u57fa\u4e8eFHIRPath\u7684\u95ee\u7b54\u8303\u5f0f\uff0c\u4ee5\u63d0\u9ad8EHR\u4e2d\u9488\u5bf9\u60a3\u8005\u7279\u5b9a\u95ee\u9898\u7684\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u63a5\u53e3\u53ef\u80fd\u65e0\u6cd5\u652f\u6301\u7cbe\u786e\u3001\u53ef\u9760\u7684\u9488\u5bf9\u60a3\u8005\u7279\u5b9a\u95ee\u9898\u7684\u7b54\u6848\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e34\u5e8a\u95ee\u7b54\uff08QA\uff09\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\u3001\u96be\u4ee5\u5728\u5b9e\u9645EHR\u4e2d\u90e8\u7f72\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFHIRPath\u7684\u95ee\u7b54\u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u4ece\u81ea\u7531\u6587\u672c\u751f\u6210\u8f6c\u79fb\u5230FHIRPath\u67e5\u8be2\u5408\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86LLM\u7684\u4f7f\u7528\u3002\u5728MIMIC-IV on FHIR Demo\u4e0a\u6784\u5efa\u4e86\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc714k\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u548c\u7ecf\u8fc7\u9a8c\u8bc1\u7684FHIRPath\u67e5\u8be2\u548c\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLM\u5728\u5904\u7406\u60a3\u8005\u8bed\u8a00\u7684\u6b67\u4e49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5728FHIRPath\u67e5\u8be2\u5408\u6210\u4e2d\u7684\u8868\u73b0\u4e5f\u8f83\u5dee\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4ece\u76d1\u7763\u5fae\u8c03\u4e2d\u53d7\u76ca\u5f88\u5927\u3002", "conclusion": "\u6587\u672c\u5230FHIRPath\u5408\u6210\u6709\u6f5c\u529b\u6210\u4e3a\u5b89\u5168\u3001\u9ad8\u6548\u3001\u4e92\u64cd\u4f5c\u7684\u5065\u5eb7\u5e94\u7528\u7684\u5b9e\u9645\u57fa\u7840\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e3a\u8be5\u4e3b\u9898\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d77\u70b9\u3002"}}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodr\u00edguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.", "AI": {"tldr": "RL-CMSA\u5728\u591a\u65c5\u884c\u5546\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272", "motivation": "\u89e3\u51b3\u5bf9\u79f0\u5355\u4ed3\u5e93\u6700\u5c0f-\u6700\u5927\u591a\u65c5\u884c\u5546\u95ee\u9898", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5373\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u6784\u5efa\u3001\u5408\u5e76\u3001\u6c42\u89e3\u548c\u9002\u5e94\uff08RL-CMSA\uff09", "result": "\u5728\u968f\u673a\u548cTSPLIB\u5b9e\u4f8b\u4e0a\u7684\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0cRL-CMSA\u59cb\u7ec8\u627e\u5230\uff08\u8fd1\uff09\u6700\u4f73\u89e3\uff0c\u5e76\u5728\u53ef\u6bd4\u65f6\u95f4\u9650\u5236\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u9057\u4f20\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b9e\u4f8b\u89c4\u6a21\u548c\u9500\u552e\u4eba\u5458\u6570\u91cf\u589e\u52a0\u65f6", "conclusion": "RL-CMSA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u79f0\u5355\u4ed3\u5e93\u6700\u5c0f-\u6700\u5927\u591a\u65c5\u884c\u5546\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23413", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.23413", "abs": "https://arxiv.org/abs/2602.23413", "authors": ["Shu Liu", "Shubham Agarwal", "Monishwaran Maheswaran", "Mert Cemri", "Zhifei Li", "Qiuyang Mang", "Ashwin Naren", "Ethan Boneh", "Audrey Cheng", "Melissa Z. Pan", "Alexander Du", "Kurt Keutzer", "Alexandros G. Dimakis", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "title": "EvoX: Meta-Evolution for Automated Discovery", "comment": null, "summary": "Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.", "AI": {"tldr": "EvoX\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8fdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8fdb\u5316\u8fc7\u7a0b\u63d0\u9ad8\u4e86LLM\u9a71\u52a8\u7684\u4f18\u5316\u548c\u8fdb\u5316\u641c\u7d22\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u7a0b\u5e8f\u3001\u63d0\u793a\u548c\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u4f18\u5316\u548c\u8fdb\u5316\u641c\u7d22\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u641c\u7d22\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9759\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvoX\u7684\u9002\u5e94\u6027\u8fdb\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u4f18\u5316\u5176\u81ea\u8eab\u7684\u8fdb\u5316\u8fc7\u7a0b\u3002EvoX\u540c\u65f6\u8fdb\u5316\u5019\u9009\u89e3\u51b3\u65b9\u6848\u548c\u7528\u4e8e\u751f\u6210\u5b83\u4eec\u7684\u641c\u7d22\u7b56\u7565\uff0c\u5e76\u6839\u636e\u8fdb\u5c55\u60c5\u51b5\u6301\u7eed\u66f4\u65b0\u5982\u4f55\u9009\u62e9\u548c\u53d8\u5f02\u5148\u524d\u89e3\u51b3\u65b9\u6848\u7684\u65b9\u5f0f\u3002", "result": "\u5728\u8fd1200\u4e2a\u771f\u5b9e\u4e16\u754c\u4f18\u5316\u4efb\u52a1\u4e2d\uff0cEvoX\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684AI\u9a71\u52a8\u8fdb\u5316\u65b9\u6cd5\uff0c\u5305\u62ecAlphaEvolve\u3001OpenEvolve\u3001GEPA\u548cShinkaEvolve\u3002", "conclusion": "EvoX\u662f\u4e00\u79cd\u6709\u6548\u7684\u9002\u5e94\u6027\u8fdb\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u7a0b\u5e8f\u3001\u63d0\u793a\u548c\u7b97\u6cd5\u7684\u6548\u7387\u3002"}}
{"id": "2602.23659", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23659", "abs": "https://arxiv.org/abs/2602.23659", "authors": ["Jeff Nijsse", "Andrea Pinto"], "title": "Central Bank Digital Currencies: Where is the Privacy, Technology, and Anonymity?", "comment": "21 pages, 7 figures", "summary": "In an age of financial system digitisation and the increasing adoption of digital currencies, Central Bank Digital Currencies (CBDCs) have emerged as a focal point for technological innovation. Privacy compliance has become a key factor in the successful design of CBDCs, extending beyond technical requirements to influence legal requirements, user trust, and security considerations. Implementing Privacy-Enhancing Technologies (PETs) in CBDCs requires an interdisciplinary approach, however, the lack of a common understanding of privacy and the essential technological characteristics restricts progress. This work investigates: (1) How privacy can be defined within the framework of CBDCs and what implications does this definition have for CBDCs design? and (2) Which PETs can be employed to enhance privacy in CBDC design? We propose a comprehensive definition for privacy that is mapped to the cryptographic landscape for feature implementation. The research is validated against case studies from 20 current CBDCs. The study shows that comprehensive privacy can be designed in the proposal stage, but that privacy does not reach the launched version of the CBDC.", "AI": {"tldr": "CBDC\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1\u7814\u7a76", "motivation": "\u91d1\u878d\u7cfb\u7edf\u6570\u5b57\u5316\u548c\u6570\u5b57\u8d27\u5e01\u7684\u666e\u53ca\u63a8\u52a8\u4e86\u4e2d\u592e\u94f6\u884c\u6570\u5b57\u8d27\u5e01\uff08CBDC\uff09\u7684\u53d1\u5c55\uff0c\u9690\u79c1\u5408\u89c4\u6210\u4e3aCBDC\u8bbe\u8ba1\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5bf920\u4e2a\u73b0\u884cCBDC\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u63d0\u51fa\u7efc\u5408\u9690\u79c1\u5b9a\u4e49\u5e76\u6620\u5c04\u5230\u5bc6\u7801\u5b66\u9886\u57df\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u5728\u63d0\u6848\u9636\u6bb5\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u7efc\u5408\u9690\u79c1\uff0c\u4f46CBDC\u7684\u53d1\u5e03\u7248\u672c\u5e76\u672a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "CBDC\u7684\u9690\u79c1\u4fdd\u62a4\u9700\u8981\u5728\u8bbe\u8ba1\u9636\u6bb5\u8fdb\u884c\uff0c\u4f46\u5b9e\u73b0\u96be\u5ea6\u8f83\u5927\u3002"}}
{"id": "2602.23559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23559", "abs": "https://arxiv.org/abs/2602.23559", "authors": ["Cho-Ying Wu", "Zixun Huang", "Xinyu Huang", "Liu Ren"], "title": "No Calibration, No Depth, No Problem: Cross-Sensor View Synthesis with 3D Consistency", "comment": "CVPR 2026 Main Conference. Project page: https://choyingw.github.io/3d-rgbx.github.io/", "summary": "We present the first study of cross-sensor view synthesis across different modalities. We examine a practical, fundamental, yet widely overlooked problem: getting aligned RGB-X data, where most RGB-X prior work assumes such pairs exist and focuses on modality fusion, but it empirically requires huge engineering effort in calibration. We propose a match-densify-consolidate method. First, we perform RGB-X image matching followed by guided point densification. Using the proposed confidence-aware densification and self-matching filtering, we attain better view synthesis and later consolidate them in 3D Gaussian Splatting (3DGS). Our method uses no 3D priors for X-sensor and only assumes nearly no-cost COLMAP for RGB. We aim to remove the cumbersome calibration for various RGB-X sensors and advance the popularity of cross-sensor learning by a scalable solution that breaks through the bottleneck in large-scale real-world RGB-X data collection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6821\u51c6\u7684RGB-X\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8de8\u4f20\u611f\u5668\u5b66\u4e60\u7684\u666e\u53ca\u7387\u3002", "motivation": "\u7814\u7a76\u8de8\u4f20\u611f\u5668\u89c6\u89d2\u5408\u6210\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u7b2c\u4e00\u4e2a\u7814\u7a76\uff0c\u89e3\u51b3RGB-X\u6570\u636e\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u5de5\u7a0b\u91cf\uff0c\u63d0\u9ad8\u8de8\u4f20\u611f\u5668\u5b66\u4e60\u7684\u666e\u53ca\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5339\u914d-\u7ec6\u5316-\u5de9\u56fa\u7684\u65b9\u6cd5\uff0c\u5305\u62ecRGB-X\u56fe\u50cf\u5339\u914d\u3001\u5f15\u5bfc\u70b9\u7ec6\u5316\u3001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7ec6\u5316\u3001\u81ea\u5339\u914d\u8fc7\u6ee4\u548c3D\u9ad8\u65af\u5206\u5c42\uff083DGS\uff09\u3002", "result": "\u5b9e\u73b0\u66f4\u597d\u7684\u89c6\u89d2\u5408\u6210\uff0c\u65e0\u97003D\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ec5\u5047\u8bbeRGB\u7684COLMAP\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aRGB-X\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u65e0\u7e41\u7410\u6821\u51c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cRGB-X\u6570\u636e\u6536\u96c6\u7684\u74f6\u9888\u3002"}}
{"id": "2602.23672", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23672", "abs": "https://arxiv.org/abs/2602.23672", "authors": ["Masahiro Kato"], "title": "General Bayesian Policy Learning", "comment": null, "summary": "This study proposes the General Bayes framework for policy learning. We consider decision problems in which a decision-maker chooses an action from an action set to maximize its expected welfare. Typical examples include treatment choice and portfolio selection. In such problems, the statistical target is a decision rule, and the prediction of each outcome $Y(a)$ is not necessarily of primary interest. We formulate this policy learning problem by loss-based Bayesian updating. Our main technical device is a squared-loss surrogate for welfare maximization. We show that maximizing empirical welfare over a policy class is equivalent to minimizing a scaled squared error in the outcome difference, up to a quadratic regularization controlled by a tuning parameter $\u03b6>0$. This rewriting yields a General Bayes posterior over decision rules that admits a Gaussian pseudo-likelihood interpretation. We clarify two Bayesian interpretations of the resulting generalized posterior, a working Gaussian view and a decision-theoretic loss-based view. As one implementation example, we introduce neural networks with tanh-squashed outputs. Finally, we provide theoretical guarantees in a PAC-Bayes style.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u901a\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u63d0\u51fa\u901a\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u4ee5\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u7684\u8d1d\u53f6\u65af\u66f4\u65b0\u3002", "result": "\u6700\u5927\u5316\u7ecf\u9a8c\u798f\u5229\u4e0e\u6700\u5c0f\u5316\u7ed3\u679c\u5dee\u5f02\u7684\u7f29\u653e\u5e73\u65b9\u8bef\u5dee\u7b49\u4ef7\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u7b56\u7565\u5b66\u4e60\u7684\u901a\u7528\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.23605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23605", "abs": "https://arxiv.org/abs/2602.23605", "authors": ["Zongzhe Xu", "Zitao Shuai", "Eideen Mozaffari", "Ravi S. Aysola", "Rajesh Kumar", "Yuzhe Yang"], "title": "SleepLM: Natural-Language Intelligence for Human Sleep", "comment": null, "summary": "We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.", "AI": {"tldr": "SleepLM\u662f\u4e00\u79cd\u65b0\u578b\u7761\u7720\u5206\u6790\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u7761\u7720\u751f\u7406\u5b66\u7684\u8bed\u8a00\u5316\u8868\u793a\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7761\u7720\u5206\u6790\u7cfb\u7edf\u5728\u63cf\u8ff0\u3001\u67e5\u8be2\u6216\u63a8\u5e7f\u5230\u65b0\u7684\u7761\u7720\u73b0\u8c61\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SleepLM\u6a21\u578b\u3002", "method": "SleepLM\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u548c\u591a\u6a21\u6001\u7761\u7720\u8111\u7535\u56fe\uff0c\u5f15\u5165\u4e86\u591a\u7ea7\u7761\u7720\u5b57\u5e55\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u3002", "result": "SleepLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u7761\u7720\u5b57\u5e55\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5c55\u73b0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684\u4e8b\u4ef6\u5b9a\u4f4d\u3001\u9488\u5bf9\u6027\u89c1\u89e3\u751f\u6210\u548c\u96f6\u6837\u672c\u6cdb\u5316\u7b49\u80fd\u529b\u3002", "conclusion": "SleepLM\u662f\u4e00\u79cd\u6709\u6548\u7684\u7761\u7720\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u5b9e\u73b0\u7761\u7720\u751f\u7406\u5b66\u7684\u8bed\u8a00\u5316\u8868\u793a\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.23446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23446", "abs": "https://arxiv.org/abs/2602.23446", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning", "comment": "Proceedings from IEEE CAI 2026, Conference on Artificial Intelligence, 8-10 May, Granada, Spain. 8 Pages, 3 Figures, 7 Tables", "summary": "Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23574", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23574", "abs": "https://arxiv.org/abs/2602.23574", "authors": ["Ruxiao Duan", "Alex Wong"], "title": "Evidential Neural Radiance Fields", "comment": null, "summary": "Understanding sources of uncertainty is fundamental to trustworthy three-dimensional scene modeling. While recent advances in neural radiance fields (NeRFs) achieve impressive accuracy in scene reconstruction and novel view synthesis, the lack of uncertainty estimation significantly limits their deployment in safety-critical settings. Existing uncertainty quantification methods for NeRFs fail to capture both aleatoric and epistemic uncertainty. Among those that do quantify one or the other, many of them either compromise rendering quality or incur significant computational overhead to obtain uncertainty estimates. To address these issues, we introduce Evidential Neural Radiance Fields, a probabilistic approach that seamlessly integrates with the NeRF rendering process and enables direct quantification of both aleatoric and epistemic uncertainty from a single forward pass. We compare multiple uncertainty quantification methods on three standardized benchmarks, where our approach demonstrates state-of-the-art scene reconstruction fidelity and uncertainty estimation quality.", "AI": {"tldr": "Evidential Neural Radiance Fields provides a novel approach to uncertainty quantification in NeRFs, improving scene reconstruction fidelity and uncertainty estimation quality.", "motivation": "Understanding sources of uncertainty is fundamental to trustworthy three-dimensional scene modeling.", "method": "Introducing Evidential Neural Radiance Fields, a probabilistic approach that seamlessly integrates with the NeRF rendering process and enables direct quantification of both aleatoric and epistemic uncertainty from a single forward pass.", "result": "State-of-the-art scene reconstruction fidelity and uncertainty estimation quality demonstrated on three standardized benchmarks.", "conclusion": "Evidential Neural Radiance Fields effectively addresses uncertainty quantification issues in NeRFs."}}
{"id": "2602.24230", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24230", "abs": "https://arxiv.org/abs/2602.24230", "authors": ["Eug\u00e8ne Berta", "Sacha Braun", "David Holzm\u00fcller", "Francis Bach", "Michael I. Jordan"], "title": "A Variational Estimator for $L_p$ Calibration Errors", "comment": null, "summary": "Calibration$\\unicode{x2014}$the problem of ensuring that predicted probabilities align with observed class frequencies$\\unicode{x2014}$is a basic desideratum for reliable prediction with machine learning systems. Calibration error is traditionally assessed via a divergence function, using the expected divergence between predictions and empirical frequencies. Accurately estimating this quantity is challenging, especially in the multiclass setting. Here, we show how to extend a recent variational framework for estimating calibration errors beyond divergences induced induced by proper losses, to cover a broad class of calibration errors induced by $L_p$ divergences. Our method can separate over- and under-confidence and, unlike non-variational approaches, avoids overestimation. We provide extensive experiments and integrate our code in the open-source package probmetrics (https://github.com/dholzmueller/probmetrics) for evaluating calibration errors.", "code_url": "https://github.com/dholzmueller/probmetrics", "code_stars": 52, "code_last_update": "2026-01-09", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u6821\u51c6\u8bef\u5dee\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u9884\u6d4b\u6982\u7387\u4e0e\u89c2\u5bdf\u5230\u7684\u7c7b\u522b\u9891\u7387\u7684\u4e00\u81f4\u6027\u3002", "motivation": "Calibration\u662f\u786e\u4fdd\u9884\u6d4b\u6982\u7387\u4e0e\u89c2\u5bdf\u5230\u7684\u7c7b\u522b\u9891\u7387\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u9760\u9884\u6d4b\u662f\u4e00\u4e2a\u57fa\u672c\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u6700\u8fd1\u53d8\u5206\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u6821\u51c6\u8bef\u5dee\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e$L_p$\u6563\u5ea6\u8bf1\u5bfc\u7684\u5e7f\u6cdb\u7c7b\u522b\u7684\u6821\u51c6\u8bef\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u533a\u5206\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4e0d\u8db3\u81ea\u4fe1\uff0c\u5e76\u4e14\u4e0e\u53d8\u5206\u65b9\u6cd5\u4e0d\u540c\uff0c\u907f\u514d\u4e86\u9ad8\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u6821\u51c6\u8bef\u5dee\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.23546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23546", "abs": "https://arxiv.org/abs/2602.23546", "authors": ["Gaurav Kamath", "Sreenath Madathil", "Sebastian Schuster", "Marie-Catherine de Marneffe", "Siva Reddy"], "title": "Humans and LLMs Diverge on Probabilistic Inferences", "comment": null, "summary": "Human reasoning often involves working over limited information to arrive at probabilistic conclusions. In its simplest form, this involves making an inference that is not strictly entailed by a premise, but rather only likely given the premise. While reasoning LLMs have demonstrated strong performance on logical and mathematical tasks, their behavior on such open-ended, non-deterministic inferences remains largely unexplored. We introduce ProbCOPA, a dataset of 210 handcrafted probabilistic inferences in English, each annotated for inference likelihood by 25--30 human participants. We find that human responses are graded and varied, revealing probabilistic judgments of the inferences in our dataset. Comparing these judgments with responses from eight state-of-the-art reasoning LLMs, we show that models consistently fail to produce human-like distributions. Finally, analyzing LLM reasoning chains, we find evidence of a common reasoning pattern used to evaluate such inferences. Our findings reveal persistent differences between humans and LLMs, and underscore the need to evaluate reasoning beyond deterministic settings.", "AI": {"tldr": "\u7814\u7a76\u4eba\u7c7b\u63a8\u7406\u548cLLMs\u5728\u6982\u7387\u63a8\u7406\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u65e0\u6cd5\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5206\u5e03\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u63a8\u7406\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u63a8\u7406\u5728\u6709\u9650\u4fe1\u606f\u4e0b\u7684\u6982\u7387\u7ed3\u8bba\uff0c\u4ee5\u53ca\u63a8\u7406LLMs\u5728\u6b64\u7c7b\u5f00\u653e\u6027\u3001\u975e\u786e\u5b9a\u6027\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b210\u4e2a\u624b\u5de5\u5236\u4f5c\u7684\u6982\u7387\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u8ba925-30\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u5bf9\u63a8\u7406\u7684\u53ef\u80fd\u6027\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u4eba\u7c7b\u56de\u7b54\u5177\u6709\u5206\u7ea7\u548c\u591a\u6837\u6027\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u4e2d\u63a8\u7406\u7684\u6982\u7387\u5224\u65ad\u3002\u4e0e\u516b\u79cd\u6700\u5148\u8fdb\u7684\u63a8\u7406LLMs\u7684\u54cd\u5e94\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u6a21\u578b\u65e0\u6cd5\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5206\u5e03\u3002", "conclusion": "\u4eba\u7c7b\u548cLLMs\u5728\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u786e\u5b9a\u6027\u8bbe\u7f6e\u4e4b\u5916\u8bc4\u4f30\u63a8\u7406\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.23632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23632", "abs": "https://arxiv.org/abs/2602.23632", "authors": ["Lun Zhan", "Feng Xiong", "Huanyong Liu", "Feng Zhang", "Yuhui Yin"], "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs", "comment": null, "summary": "Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS", "code_url": "https://github.com/360AILAB-NLP/MMKG-RDS", "code_stars": 1, "code_last_update": "2026-02-27", "AI": {"tldr": "\u63d0\u51faMMKG-RDS\u6846\u67b6\uff0c\u63d0\u9ad8\u63a8\u7406\u6570\u636e\u5408\u6210\u8d28\u91cf\u548c\u6548\u7387", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u77e5\u8bc6\u8986\u76d6\u3001\u6709\u6548\u6027\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u5728\u529f\u80fd\u3001\u7c92\u5ea6\u3001\u53ef\u5b9a\u5236\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMKG-RDS\u7684\u7075\u6d3b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u63a8\u7406\u6570\u636e\u5408\u6210\u3002\u5b83\u652f\u6301\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u3001\u53ef\u5b9a\u5236\u7684\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u5ea6\u6570\u636e\u8d28\u91cf\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5c11\u91cf\u5408\u6210\u7684\u6837\u672c\u4e0a\u5fae\u8c03Qwen3\u6a21\u578b\uff080.6B/8B/32B\uff09\u53ef\u4ee5\u63d0\u9ad8\u63a8\u7406\u7cbe\u5ea69.2%\u3002\u6846\u67b6\u8fd8\u751f\u6210\u4e86\u72ec\u7279\u7684\u6570\u636e\uff0c\u5728\u6d89\u53ca\u8868\u683c\u548c\u516c\u5f0f\u7684\u4efb\u52a1\u4e2d\u6311\u6218\u4e86\u73b0\u6709\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u590d\u6742\u57fa\u51c6\u7684\u6784\u5efa\u3002", "conclusion": "MMKG-RDS\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u63a8\u7406\u6570\u636e\u5408\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u590d\u6742\u57fa\u51c6\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.23459", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23459", "abs": "https://arxiv.org/abs/2602.23459", "authors": ["Eric V. Strobl"], "title": "Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires", "comment": null, "summary": "Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.", "AI": {"tldr": "REFINE\u65b9\u6cd5\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u80fd\u529b\u9650\u5236\u5728\u9884\u5904\u7406\u6a21\u5757\u4e2d\uff0c\u63d0\u9ad8\u4e86\u7cbe\u795e\u75c5\u5b66\u95ee\u5377\u6570\u636e\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u7cbe\u795e\u75c5\u5b66\u95ee\u5377\u6570\u636e\u4e2d\u9884\u6d4b\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u95ee\u5377\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u654f\u611f\u6027\uff0c\u5e76\u4e14\u901a\u5e38\u53ea\u80fd\u5f31\u9884\u6d4b\u540e\u7eed\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5REFINE\uff08\u5197\u4f59\u5229\u7528\u540e\u7eed\u4fe1\u606f\u544a\u77e5\u975e\u7ebf\u6027\u589e\u5f3a\uff09\uff0c\u5c06\u975e\u7ebf\u6027\u80fd\u529b\u9650\u5236\u5728\u57fa\u7ebf\u9884\u5904\u7406\u6a21\u5757\u4e2d\uff0c\u8be5\u6a21\u5757\u4f30\u8ba1\u7a33\u5b9a\u7684\u9879\u503c\uff0c\u7136\u540e\u4ece\u8fd9\u4e9b\u7a33\u5b9a\u7684\u57fa\u7ebf\u9879\u5230\u672a\u6765\u7684\u4e25\u91cd\u7a0b\u5ea6\u5b66\u4e60\u7ebf\u6027\u6620\u5c04\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cREFINE\u4f18\u4e8e\u5176\u4ed6\u53ef\u89e3\u91ca\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8de8\u7cbe\u795e\u75c5\u5b66\u548c\u975e\u7cbe\u795e\u75c5\u5b66\u7eb5\u5411\u9884\u6d4b\u4efb\u52a1\u4e2d\u9884\u540e\u56e0\u7d20\u7684\u660e\u786e\u5168\u5c40\u5f52\u56e0\u3002", "conclusion": "REFINE\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u540e\u5173\u7cfb\u900f\u660e\u7ebf\u6027\u5e76\u53ef\u5168\u5c40\u89e3\u91ca\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2602.23760", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.23760", "abs": "https://arxiv.org/abs/2602.23760", "authors": ["Jie Li", "Jing Li", "Lu Lv", "Zhanyu Ju", "Fengkui Gong"], "title": "PLA for Drone RID Frames via Motion Estimation and Consistency Verification", "comment": null, "summary": "Drone Remote Identification (RID) plays a critical role in low-altitude airspace supervision, yet its broadcast nature and lack of cryptographic protection make it vulnerable to spoofing and replay attacks. In this paper, we propose a consistency verification-based physical-layer authentication (PLA) algorithm for drone RID frames. A RID-aware sensing and decoding module is first developed to extract communication-derived sensing parameters, including angle-of-arrival, Doppler shift, average channel gain, and the number of transmit antennas, together with the identity and motion-related information decoded from previously authenticated RID frames. Rather than fusing all heterogeneous information into a single representation, different types of information are selectively utilized according to their physical relevance and reliability. Specifically, real-time wireless sensing parameter constraints and previously authenticated motion states are incorporated in a yaw-augmented constant-acceleration extended Kalman filter (CA-EKF) to estimate the three-dimensional position and motion states of the drone. To further enhance authentication reliability under highly maneuverable and non-stationary flight scenarios, a data-driven long short-term memory-based motion estimator is employed, and its predictions are adaptively combined with the CA-EKF via an error-aware fusion strategy. Finally, RID frames are authenticated by verifying consistency in the number of transmit antennas, motion estimates, and no-fly-zone constraints. Simulation results demonstrate that the proposed algorithm significantly improves authentication reliability and robustness under realistic wireless impairments and complex drone maneuvers, outperforming existing RF feature-based and motion model-based PLA schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u65e0\u4eba\u673aRID\u5e27\u7684\u5b89\u5168\u6027\u3002", "motivation": "Drone RID frames\u6613\u53d7\u6b3a\u9a97\u548c\u91cd\u653e\u653b\u51fb\uff0c\u9700\u8981\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u7b97\u6cd5\uff0c\u5229\u7528\u4f20\u611f\u5668\u53c2\u6570\u548c\u8eab\u4efd\u4fe1\u606f\u8fdb\u884c\u8ba4\u8bc1\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u73b0\u5b9e\u65e0\u7ebf\u5e72\u6270\u548c\u590d\u6742\u65e0\u4eba\u673a\u673a\u52a8\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u8bc1\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u65e0\u4eba\u673aRID\u5e27\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.24263", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24263", "abs": "https://arxiv.org/abs/2602.24263", "authors": ["James Cheshire", "Stephan Cl\u00e9men\u00e7on"], "title": "Active Bipartite Ranking with Smooth Posterior Distributions", "comment": null, "summary": "In this article, bipartite ranking, a statistical learning problem involved in many applications and widely studied in the passive context, is approached in a much more general \\textit{active setting} than the discrete one previously considered in the literature. While the latter assumes that the conditional distribution is piece wise constant, the framework we develop permits in contrast to deal with continuous conditional distributions, provided that they fulfill a H\u00f6lder smoothness constraint. We first show that a naive approach based on discretisation at a uniform level, fixed \\textit{a priori} and consisting in applying next the active strategy designed for the discrete setting generally fails. Instead, we propose a novel algorithm, referred to as smooth-rank and designed for the continuous setting, which aims to minimise the distance between the ROC curve of the estimated ranking rule and the optimal one w.r.t. the $\\sup$ norm. We show that, for a fixed confidence level $\u03b5>0$ and probability $\u03b4\\in (0,1)$, smooth-rank is PAC$(\u03b5,\u03b4)$. In addition, we provide a problem dependent upper bound on the expected sampling time of smooth-rank and establish a problem dependent lower bound on the expected sampling time of any PAC$(\u03b5,\u03b4)$ algorithm. Beyond the theoretical analysis carried out, numerical results are presented, providing solid empirical evidence of the performance of the algorithm proposed, which compares favorably with alternative approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u66f4\u901a\u7528\u7684\u4e3b\u52a8\u8bbe\u7f6e\u4e2d\u89e3\u51b3\u53cc\u5206\u56fe\u6392\u540d\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u53cc\u5206\u56fe\u6392\u540d\u662f\u4e00\u4e2a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u95ee\u9898\uff0c\u5728\u88ab\u52a8\u73af\u5883\u4e2d\u88ab\u5e7f\u6cdb\u7814\u7a76\u3002\u672c\u6587\u5728\u6bd4\u6587\u732e\u4e2d\u5148\u524d\u8003\u8651\u7684\u79bb\u6563\u8bbe\u7f6e\u4e2d\u66f4\u901a\u7528\u7684\u4e3b\u52a8\u8bbe\u7f6e\u4e2d\u7814\u7a76\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u79f0\u4e3a\u5e73\u6ed1\u6392\u540d\uff0c\u7528\u4e8e\u8fde\u7eed\u8bbe\u7f6e\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u4f30\u8ba1\u6392\u540d\u89c4\u5219\u4e0e\u6700\u4f18\u6392\u540d\u4e4b\u95f4\u7684ROC\u66f2\u7ebf\u8ddd\u79bb\u3002", "result": "\u5e73\u6ed1\u6392\u540d\u5728\u56fa\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u03b5>0\u548c\u6982\u7387\u03b4\u2208(0,1)\u4e0b\u662fPAC(\u03b5,\u03b4)\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5e73\u6ed1\u6392\u540d\u7684\u671f\u671b\u91c7\u6837\u65f6\u95f4\u7684\u4e0a\u754c\u548c\u4efb\u4f55PAC(\u03b5,\u03b4)\u7b97\u6cd5\u7684\u671f\u671b\u91c7\u6837\u65f6\u95f4\u7684\u4e0b\u754c\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u66f4\u901a\u7528\u7684\u4e3b\u52a8\u8bbe\u7f6e\u4e2d\u89e3\u51b3\u53cc\u5206\u56fe\u6392\u540d\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.23643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23643", "abs": "https://arxiv.org/abs/2602.23643", "authors": ["Judah Goldfeder", "Philippe Wyder", "Yann LeCun", "Ravid Shwartz Ziv"], "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence", "comment": null, "summary": "Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAI\u6982\u5ff5\uff0c\u65e8\u5728\u66ff\u4ee3AGI\uff0c\u4ee5\u63a8\u52a8AI\u6280\u672f\u5411\u66f4\u7cbe\u786e\u3001\u66f4\u6709\u6548\u7684\u65b9\u5411\u53d1\u5c55\u3002", "motivation": "\u63a2\u8ba8\u5bf9AGI\u7684\u7406\u89e3\u548c\u5b9a\u4e49\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51faSAI\u6982\u5ff5\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5206\u6790\u73b0\u6709AGI\u5b9a\u4e49\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faSAI\u7684\u5b9a\u4e49\u548c\u4f18\u52bf\u3002", "result": "\u63d0\u51faSAI\u6982\u5ff5\uff0c\u5e76\u9610\u8ff0\u5176\u5728AI\u53d1\u5c55\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u548c\u5f71\u54cd\u3002", "conclusion": "SAI\u662f\u4e00\u4e2a\u66f4\u7cbe\u786e\u3001\u66f4\u6709\u7528\u7684AI\u6982\u5ff5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u6280\u672f\u7684\u5065\u5eb7\u53d1\u5c55\u3002"}}
{"id": "2602.23495", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23495", "abs": "https://arxiv.org/abs/2602.23495", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Uncertainty-aware Language Guidance for Concept Bottleneck Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5CBM\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLMs\u6807\u6ce8\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u5ff5\u4e0d\u786e\u5b9a\u6027\u5728CBM\u4e2d\u7684\u5e94\u7528\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86CBMs\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u5728\u8f93\u5165\u6837\u672c\u6620\u5c04\u5230\u9ad8\u7ea7\u8bed\u4e49\u6982\u5ff5\u540e\u8fdb\u884c\u7ec4\u5408\u4ee5\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u8fd9\u4e9b\u6982\u5ff5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\u548c\u52b3\u52a8\u529b\uff0c\u9650\u5236\u4e86CBMs\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4e00\u4e9b\u5de5\u4f5c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u6765\u6784\u5efa\u6982\u5ff5\u74f6\u9888\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u9996\u5148\uff0c\u5b83\u4eec\u5ffd\u7565\u4e86LLMs\u6807\u6ce8\u7684\u6982\u5ff5\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u6709\u6548\u7684\u673a\u5236\u6765\u91cf\u5316\u6807\u6ce8\u6982\u5ff5\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u589e\u52a0\u4e86\u7531\u4e8eLLMs\u7684\u5e7b\u89c9\u800c\u9020\u6210\u9519\u8bef\u7684\u98ce\u9669\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u672a\u80fd\u5c06\u6807\u6ce8\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5CBM\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e25\u683c\u91cf\u5316LLMs\u6807\u6ce8\u7684\u6982\u5ff5\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5177\u6709\u6709\u6548\u548c\u5206\u5e03\u65e0\u5173\u7684\u4fdd\u8bc1\uff0c\u800c\u4e14\u8fd8\u628a\u91cf\u5316\u540e\u7684\u6982\u5ff5\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165CBM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u8003\u8651\u5230LLMs\u6807\u6ce8\u7684\u6982\u5ff5\u7684\u53ef\u9760\u6027\u6c34\u5e73\u7684\u4e0d\u540c\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u671f\u671b\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5CBM\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLMs\u6807\u6ce8\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u5ff5\u4e0d\u786e\u5b9a\u6027\u5728CBM\u4e2d\u7684\u5e94\u7528\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86CBMs\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.23588", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23588", "abs": "https://arxiv.org/abs/2602.23588", "authors": ["Abhishek Dalvi", "Vasant Honavar"], "title": "Hyperdimensional Cross-Modal Alignment of Frozen Language and Image Models for Efficient Image Captioning", "comment": null, "summary": "Large unimodal foundation models for vision and language encode rich semantic structures, yet aligning them typically requires computationally intensive multimodal fine-tuning. Such approaches depend on large-scale parameter updates, are resource intensive, and can perturb pretrained representations. Emerging evidence suggests, however, that independently trained foundation models may already exhibit latent semantic compatibility, reflecting shared structures in the data they model. This raises a fundamental question: can cross-modal alignment be achieved without modifying the models themselves? Here we introduce HDFLIM (HyperDimensional computing with Frozen Language and Image Models), a framework that establishes cross-modal mappings while keeping pretrained vision and language models fully frozen. HDFLIM projects unimodal embeddings into a shared hyperdimensional space and leverages lightweight symbolic operations -- binding, bundling, and similarity-based retrieval to construct associative cross-modal representations in a single pass over the data. Caption generation emerges from high-dimensional memory retrieval rather than iterative gradient-based optimization. We show that HDFLIM achieves performance comparable to end-to-end vision-language training methods and produces captions that are more semantically grounded than zero-shot baselines. By decoupling alignment from parameter tuning, our results suggest that semantic mapping across foundation models can be realized through symbolic operations on hyperdimensional encodings of the respective embeddings. More broadly, this work points toward an alternative paradigm for foundation model alignment in which frozen models are integrated through structured representational mappings rather than through large-scale retraining. The codebase for our implementation can be found at https://github.com/Abhishek-Dalvi410/HDFLIM.", "code_url": "https://github.com/Abhishek-Dalvi410/HDFLIM", "code_stars": 0, "code_last_update": "2026-03-02", "AI": {"tldr": "HDFLIM\u662f\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u672c\u8eab\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u64cd\u4f5c\u5728\u8d85\u7ef4\u7a7a\u95f4\u4e2d\u6784\u5efa\u5173\u8054\u7684\u8de8\u6a21\u6001\u8868\u793a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u7684\u8bed\u4e49\u7ed3\u6784\u7f16\u7801\u95ee\u9898\uff0c\u4ee5\u53ca\u901a\u5e38\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u578b\u591a\u6a21\u6001\u5fae\u8c03\u6765\u5bf9\u9f50\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDFLIM\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u5b8c\u5168\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\u5efa\u7acb\u8de8\u6a21\u6001\u6620\u5c04\u3002HDFLIM\u5c06\u5355\u6a21\u6001\u5d4c\u5165\u6295\u5f71\u5230\u5171\u4eab\u7684\u8d85\u7ef4\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u7b26\u53f7\u64cd\u4f5c\uff08\u7ed1\u5b9a\u3001\u6346\u7ed1\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\uff09\u5728\u5355\u6b21\u6570\u636e\u904d\u5386\u4e2d\u6784\u5efa\u5173\u8054\u7684\u8de8\u6a21\u6001\u8868\u793a\u3002", "result": "HDFLIM\u5728\u6027\u80fd\u4e0a\u4e0e\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u751f\u6210\u7684\u5b57\u5e55\u5728\u8bed\u4e49\u4e0a\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u66f4\u624e\u5b9e\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bf9\u9f50\u4e0e\u53c2\u6570\u8c03\u6574\u5206\u79bb\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5404\u81ea\u5d4c\u5165\u7684\u8d85\u7ef4\u7f16\u7801\u4e0a\u7684\u7b26\u53f7\u64cd\u4f5c\u53ef\u4ee5\u5b9e\u73b0\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u7684\u8bed\u4e49\u6620\u5c04\u3002\u66f4\u5e7f\u6cdb\u5730\u8bf4\uff0c\u8fd9\u9879\u5de5\u4f5c\u6307\u5411\u4e86\u4e00\u79cd\u66ff\u4ee3\u8303\u5f0f\uff0c\u5728\u8fd9\u79cd\u8303\u5f0f\u4e2d\uff0c\u51bb\u7ed3\u7684\u6a21\u578b\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u6620\u5c04\u800c\u4e0d\u662f\u901a\u8fc7\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u6765\u96c6\u6210\u3002"}}
{"id": "2602.23577", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23577", "abs": "https://arxiv.org/abs/2602.23577", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Shijie Zhang", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Multi-Agent Causal Reasoning for Suicide Ideation Detection Through Online Conversations", "comment": null, "summary": "Suicide remains a pressing global public health concern. While social media platforms offer opportunities for early risk detection through online conversation trees, existing approaches face two major limitations: (1) They rely on predefined rules (e.g., quotes or relies) to log conversations that capture only a narrow spectrum of user interactions, and (2) They overlook hidden influences such as user conformity and suicide copycat behavior, which can significantly affect suicidal expression and propagation in online communities. To address these limitations, we propose a Multi-Agent Causal Reasoning (MACR) framework that collaboratively employs a Reasoning Agent to scale user interactions and a Bias-aware Decision-Making Agent to mitigate harmful biases arising from hidden influences. The Reasoning Agent integrates cognitive appraisal theory to generate counterfactual user reactions to posts, thereby scaling user interactions. It analyses these reactions through structured dimensions, i.e., cognitive, emotional, and behavioral patterns, with a dedicated sub-agent responsible for each dimension. The Bias-aware Decision-Making Agent mitigates hidden biases through a front-door adjustment strategy, leveraging the counterfactual user reactions produced by the Reasoning Agent. Through the collaboration of reasoning and bias-aware decision making, the proposed MACR framework not only alleviates hidden biases, but also enriches contextual information of user interactions with counterfactual knowledge. Extensive experiments on real-world conversational datasets demonstrate the effectiveness and robustness of MACR in identifying suicide risk.", "AI": {"tldr": "MACR is an effective framework for identifying suicide risk in social media.", "motivation": "Existing approaches for early risk detection on social media platforms have limitations.", "method": "Proposing a Multi-Agent Causal Reasoning (MACR) framework.", "result": "MACR effectively identifies suicide risk.", "conclusion": "MACR alleviates hidden biases and enriches contextual information of user interactions."}}
{"id": "2602.23504", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23504", "abs": "https://arxiv.org/abs/2602.23504", "authors": ["Anik Pramanik", "Murat Kantarcioglu", "Vincent Oria", "Shantanu Sharma"], "title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "comment": "This paper has been accepted in ICLR 2026", "summary": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23834", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23834", "abs": "https://arxiv.org/abs/2602.23834", "authors": ["Xuhui Dou", "Hayretdin Bahsi", "Alejandro Guerra-Manzanares"], "title": "Enhancing Continual Learning for Software Vulnerability Prediction: Addressing Catastrophic Forgetting via Hybrid-Confidence-Aware Selective Replay for Temporal LLM Fine-Tuning", "comment": "Accepted for publication in the Proceedings of the 2026 International Conference on Information Systems Security and Privacy (ICISSP)", "summary": "Recent work applies Large Language Models (LLMs) to source-code vulnerability detection, but most evaluations still rely on random train-test splits that ignore time and overestimate real-world performance. In practice, detectors are deployed on evolving code bases and must recognise future vulnerabilities under temporal distribution shift. This paper investigates continual fine-tuning of a decoder-style language model (microsoft/phi-2 with LoRA) on a CVE-linked dataset spanning 2018-2024, organised into bi-monthly windows. We evaluate eight continual learning strategies, including window-only and cumulative training, replay-based baselines and regularisation-based variants. We propose Hybrid Class-Aware Selective Replay (Hybrid-CASR), a confidence-aware replay method for binary vulnerability classification that prioritises uncertain samples while maintaining a balanced ratio of VULNERABLE and FIXED functions in the replay buffer. On bi-monthly forward evaluation Hybrid-CASR achieves a Macro-F1 of 0.667, improving on the window-only baseline (0.651) by 0.016 with statistically significant gains ($p = 0.026$) and stronger backward retention (IBR@1 of 0.741). Hybrid-CASR also reduces training time per window by about 17 percent compared to the baseline, whereas cumulative training delivers only a minor F1 increase (0.661) at a 15.9-fold computational cost. Overall, the results show that selective replay with class balancing offers a practical accuracy-efficiency trade-off for LLM-based temporal vulnerability detection under continuous temporal drift.", "AI": {"tldr": "This paper investigates continual fine-tuning of a decoder-style language model for source-code vulnerability detection. The proposed Hybrid-CASR method achieves higher accuracy and efficiency compared to traditional methods.", "motivation": "Recent work applies Large Language Models (LLMs) to source-code vulnerability detection, but most evaluations still rely on random train-test splits that ignore time and overestimate real-world performance. In practice, detectors are deployed on evolving code bases and must recognise future vulnerabilities under temporal distribution shift.", "method": "Continual fine-tuning of a decoder-style language model (microsoft/phi-2 with LoRA) on a CVE-linked dataset spanning 2018-2024, organised into bi-monthly windows. We evaluate eight continual learning strategies, including window-only and cumulative training, replay-based baselines and regularisation-based variants. We propose Hybrid Class-Aware Selective Replay (Hybrid-CASR), a confidence-aware replay method for binary vulnerability classification that prioritises uncertain samples while maintaining a balanced ratio of VULNERABLE and FIXED functions in the replay buffer.", "result": "On bi-monthly forward evaluation Hybrid-CASR achieves a Macro-F1 of 0.667, improving on the window-only baseline (0.651) by 0.016 with statistically significant gains ($p = 0.026$) and stronger backward retention (IBR@1 of 0.741). Hybrid-CASR also reduces training time per window by about 17 percent compared to the baseline, whereas cumulative training delivers only a minor F1 increase (0.661) at a 15.9-fold computational cost.", "conclusion": "Selective replay with class balancing offers a practical accuracy-efficiency trade-off for LLM-based temporal vulnerability detection under continuous temporal drift."}}
{"id": "2602.23589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23589", "abs": "https://arxiv.org/abs/2602.23589", "authors": ["Hiroshi Sasaki"], "title": "Pseudo Contrastive Learning for Diagram Comprehension in Multimodal Models", "comment": "9 pages, 3 figures", "summary": "Recent multimodal models such as Contrastive Language-Image Pre-training (CLIP) have shown remarkable ability to align visual and linguistic representations. However, domains where small visual differences carry large semantic significance, such as diagram understanding, remain challenging due to the models' limited sensitivity to fine-grained structural variations.\n  We propose a new training paradigm designed to enhance diagram comprehension in vision-language models. Our approach introduces pseudo contrastive samples generated by a diagram renderer that creates synthetic diagrams using randomly picked text elements. These samples highlight structural differences in diagrammatic imagery without requiring any modification or editing of the original data. By incorporating these pseudo contrastive samples into the training objective, the model learns to capture more precise and semantically consistent diagram structures.\n  Empirical evaluations on a benchmark dataset of flowcharts demonstrate substantial improvements over standard CLIP and hard-negative CLIP training in both image-text matching and visual question answering tasks. The results underscore the value of domain-specific training strategies and contribute to advancing diagrammatic understanding within the broader context of vision-language learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u5bf9\u6bd4\u6837\u672c\u6765\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u89e3\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u7ec6\u5fae\u7ed3\u6784\u53d8\u5316\u7684\u654f\u611f\u6027\u6709\u9650\uff0c\u56e0\u6b64\u5728\u56fe\u89e3\u7406\u89e3\u7b49\u9886\u57df\u7684\u5e94\u7528\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u7531\u56fe\u89e3\u6e32\u67d3\u5668\u751f\u6210\u7684\u4f2a\u5bf9\u6bd4\u6837\u672c\u6765\u589e\u5f3a\u56fe\u89e3\u7406\u89e3\u3002\u8fd9\u4e9b\u6837\u672c\u901a\u8fc7\u968f\u673a\u9009\u62e9\u6587\u672c\u5143\u7d20\u521b\u5efa\u5408\u6210\u56fe\u89e3\uff0c\u7a81\u51fa\u4e86\u56fe\u89e3\u56fe\u50cf\u4e2d\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u800c\u65e0\u9700\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u6216\u7f16\u8f91\u3002", "result": "\u5728\u6d41\u7a0b\u56fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6807\u51c6CLIP\u548c\u786c\u8d1fCLIP\u8bad\u7ec3\u76f8\u6bd4\uff0c\u5728\u56fe\u50cf-\u6587\u672c\u5339\u914d\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5f3a\u8c03\u4e86\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u7b56\u7565\u7684\u4ef7\u503c\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u89c6\u89c9-\u8bed\u8a00\u5b66\u4e60\u7684\u66f4\u5e7f\u6cdb\u80cc\u666f\u4e0b\u63a8\u8fdb\u56fe\u89e3\u7406\u89e3\u3002"}}
{"id": "2602.23528", "categories": ["cs.LG", "cs.CE", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23528", "abs": "https://arxiv.org/abs/2602.23528", "authors": ["Yicen Li", "Jose Antonio Lara Benitez", "Ruiyang Hong", "Anastasis Kratsios", "Paul David McNicholas", "Maarten Valentijn de Hoop"], "title": "Neural Operators Can Discover Functional Clusters", "comment": null, "summary": "Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.\n  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory.", "AI": {"tldr": "\u795e\u7ecf\u7b97\u5b50\uff08NOs\uff09\u5728\u65e0\u9650\u7ef4\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7c7b\u522b\uff0c\u5e76\u5e94\u7528\u4e8e\u805a\u7c7b\u548c\u529f\u80fd\u6570\u636e\u5206\u6790\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "motivation": "\u901a\u8fc7\u5728\u65e0\u9650\u4e2a\u95ee\u9898\u5bb6\u65cf\u4e2d\u644a\u9500\u63a8\u7406\uff0c\u7b97\u5b50\u5b66\u4e60\u6b63\u5728\u6539\u53d8\u79d1\u5b66\u8ba1\u7b97\u3002\u867d\u7136\u795e\u7ecf\u7b97\u5b50\uff08NOs\uff09\u5728\u56de\u5f52\u65b9\u9762\u8d8a\u6765\u8d8a\u88ab\u4eba\u4eec\u6240\u7406\u89e3\uff0c\u4f46\u5728\u5206\u7c7b\u53ca\u5176\u65e0\u76d1\u7763\u7c7b\u4f3c\u7269\uff1a\u805a\u7c7b\u65b9\u9762\uff0c\u4e86\u89e3\u751a\u5c11\u3002", "method": "\u6211\u4eec\u8bc1\u660e\u4e86\u57fa\u4e8e\u6837\u672c\u7684\u795e\u7ecf\u7b97\u5b50\u53ef\u4ee5\u5728\u65e0\u9650\u7ef4\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5b66\u4e60\u4efb\u4f55\u6709\u9650\u7c7b\u522b\u7684\u96c6\u5408\uff0c\u5373\u4f7f\u5728\u7c7b\u522b\u65e2\u4e0d\u51f8\u4e5f\u4e0d\u8fde\u901a\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u8981\u6ee1\u8db3\u8f7b\u5fae\u7684\u6838\u91c7\u6837\u5047\u8bbe\u3002\u6211\u4eec\u7684\u4e00\u822c\u805a\u7c7b\u5b9a\u7406\u8868\u660e\uff0c\u4efb\u4f55K\u4e2a\u95ed\u7c7b\u90fd\u53ef\u4ee5\u5728\u4e0a Kuratowski\u62d3\u6251\u7684\u95ed\u96c6\u4e0a\u7531NO\u53c2\u6570\u5316\u7c7b\u4efb\u610f\u7cbe\u786e\u5730\u8fd1\u4f3c\uff0c\u8fd9\u53ef\u4ee5\u89e3\u91ca\u4e3a\u4e0d\u5141\u8bb8\u9519\u8bef\u7684\u8bef\u5206\u7c7b\u3002", "result": "\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eNO\u7684\u805a\u7c7b\u6d41\u7a0b\uff0c\u7528\u4e8e\u529f\u80fd\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u672a\u6807\u8bb0\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u8f68\u8ff9\u5bb6\u65cf\u3002\u79bb\u6563\u8f68\u8ff9\u901a\u8fc7\u4e00\u4e2a\u56fa\u5b9a\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u5347\u5230\u4e00\u4e2a\u8fde\u7eed\u7684\u7279\u5f81\u56fe\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u53ef\u8bad\u7ec3\u5934\u6620\u5c04\u5230\u8f6f\u5206\u914d\u3002\u5728\u591a\u79cd\u5408\u6210ODE\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u5f97\u5230\u7684\u5b9e\u9645SNO\u5728\u7ecf\u5178\u65b9\u6cd5\u5931\u8d25\u7684\u533a\u57df\u4e2d\u6062\u590d\u4e86\u6f5c\u5728\u7684\u52a8\u6001\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u4e0e\u6211\u4eec\u7684\u4e00\u822c\u805a\u7c7b\u7406\u8bba\u4e00\u81f4\u7684\u8bc1\u636e\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\uff08NOs\uff09\u5728\u5206\u7c7b\u548c\u805a\u7c7b\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u975e\u7ebf\u6027\u95ee\u9898\u3002"}}
{"id": "2602.23580", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23580", "abs": "https://arxiv.org/abs/2602.23580", "authors": ["Yun Wang", "Xuansheng Wu", "Jingyuan Huang", "Lei Liu", "Xiaoming Zhai", "Ninghao Liu"], "title": "BRIDGE the Gap: Mitigating Bias Amplification in Automated Scoring of English Language Learners via Inter-group Data Augmentation", "comment": "15 pages, 1 figure", "summary": "In the field of educational assessment, automated scoring systems increasingly rely on deep learning and large language models (LLMs). However, these systems face significant risks of bias amplification, where model prediction gaps between student groups become larger than those observed in training data. This issue is especially severe for underrepresented groups such as English Language Learners (ELLs), as models may inherit and further magnify existing disparities in the data. We identify that this issue is closely tied to representation bias: the scarcity of minority (high-scoring ELL) samples makes models trained with empirical risk minimization favor majority (non-ELL) linguistic patterns. Consequently, models tend to under-predict ELL students who even demonstrate comparable domain knowledge but use different linguistic patterns, thereby undermining the fairness of automated scoring outcomes. To mitigate this, we propose BRIDGE, a Bias-Reducing Inter-group Data GEneration framework designed for low-resource assessment settings. Instead of relying on the limited minority samples, BRIDGE synthesizes high-scoring ELL samples by \"pasting\" construct-relevant (i.e., rubric-aligned knowledge and evidence) content from abundant high-scoring non-ELL samples into authentic ELL linguistic patterns. We further introduce a discriminator model to ensure the quality of synthetic samples. Experiments on California Science Test (CAST) datasets demonstrate that BRIDGE effectively reduces prediction bias for high-scoring ELL students while maintaining overall scoring performance. Notably, our method achieves fairness gains comparable to using additional real human data, offering a cost-effective solution for ensuring equitable scoring in large-scale assessments.", "AI": {"tldr": "\u63d0\u51faBRIDGE\u6846\u67b6\uff0c\u4ee5\u51cf\u8f7b\u6559\u80b2\u8bc4\u4f30\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\u653e\u5927\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u6837\u672c\u6765\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u4e86\u51cf\u8f7b\u6559\u80b2\u8bc4\u4f30\u4e2d\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u7684\u504f\u5dee\u653e\u5927\u98ce\u9669\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u82f1\u8bed\u5b66\u4e60\u8005\uff08ELLs\uff09\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7fa4\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBRIDGE\u7684\u504f\u5dee\u964d\u4f4e\u8de8\u7ec4\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u5927\u91cf\u975e\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u9ad8\u8d28\u91cf\u5185\u5bb9\u201c\u7c98\u8d34\u201d\u5230\u771f\u5b9e\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u8bed\u8a00\u6a21\u5f0f\u4e2d\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBRIDGE\u5728\u51cf\u5c11\u5bf9\u9ad8\u5206\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u9884\u6d4b\u504f\u5dee\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6574\u4f53\u8bc4\u5206\u6027\u80fd\u3002", "conclusion": "BRIDGE\u662f\u4e00\u79cd\u6709\u6548\u51cf\u8f7b\u6559\u80b2\u8bc4\u4f30\u4e2d\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u504f\u5dee\u653e\u5927\u7684\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u7684\u516c\u5e73\u8bc4\u5206\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23507", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23507", "abs": "https://arxiv.org/abs/2602.23507", "authors": ["Diana Shamsutdinova", "Felix Zimmer", "Oyebayo Ridwan Olaniran", "Sarah Markham", "Daniel Stahl", "Gordon Forbes", "Ewan Carr"], "title": "Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package", "comment": "26 pages, 4 figures, 1 table, preprint", "summary": "Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u62df\u65b9\u6cd5\u6765\u4f30\u8ba1\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6837\u672c\u91cf\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u79cd\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u786e\u5b9a\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u5f00\u53d1\u7684\u6700\u5c0f\u6837\u672c\u91cf\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u6837\u672c\u91cf\u4e0d\u8db3\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u9884\u6d4b\u504f\u5dee\u3002", "method": "\u56de\u987e\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u6837\u672c\u91cf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b66\u4e60\u66f2\u7ebf\u3001\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u548c\u4fdd\u8bc1\u539f\u5219\u6765\u8bc6\u522b\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u7684\u6837\u672c\u91cf\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u65b9\u6cd5\u3001\u6027\u80fd\u6307\u6807\u548c\u5efa\u6a21\u7b56\u7565\u7684\u6837\u672c\u91cf\u4f30\u8ba1\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002pmsims\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u6a21\u578b\u548c\u7528\u6237\u5b9a\u4e49\u7684\u6307\u6807\uff0c\u540c\u65f6\u660e\u786e\u8003\u8651\u6a21\u578b\u6027\u80fd\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u8f6f\u4ef6\u901a\u8fc7\u7ed3\u5408\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u63a8\u8fdb\u4e86\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u6837\u672c\u91cf\u65b9\u6cd5\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u6269\u5c55\u8fd9\u4e9b\u65b9\u6cd5\u5230\u5206\u5c42\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u7eb3\u5165\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u6307\u6807\uff0c\u5e76\u89e3\u51b3\u7f3a\u5931\u6570\u636e\u548c\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u7b49\u6311\u6218\u3002"}}
{"id": "2602.23846", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23846", "abs": "https://arxiv.org/abs/2602.23846", "authors": ["Wei Lian", "Alejandro Guerra-Manzanares"], "title": "MI$^2$DAS: A Multi-Layer Intrusion Detection Framework with Incremental Learning for Securing Industrial IoT Networks", "comment": "Accepted for publication in the Proceedings of the 2026 International Conference on Information Systems Security and Privacy (ICISSP)", "summary": "The rapid expansion of Industrial IoT (IIoT) systems has amplified security challenges, as heterogeneous devices and dynamic traffic patterns increase exposure to sophisticated and previously unseen cyberattacks. Traditional intrusion detection systems often struggle in such environments due to their reliance on extensive labeled data and limited ability to detect new threats. To address these challenges, we propose MI$^2$DAS, a multi-layer intrusion detection framework that integrates anomaly-based hierarchical traffic pooling, open-set recognition to distinguish between known and unknown attacks and incremental learning for adapting to novel attack types with minimal labeling. Experiments conducted on the Edge-IIoTset dataset demonstrate strong performance across all layers. In the first layer, GMM achieves superior normal-attack discrimination (accuracy = 0.953, TPR = 1.000). In open-set recognition, GMM attains a recall of 0.813 for known attacks, while LOF achieves 0.882 recall for unknown attacks. For fine-grained classification of known attacks, Random Forest achieves a macro-F1 of 0.941. Finally, the incremental learning module maintains robust performance when incorporation novel attack classes, achieving a macro-F1 of 0.8995. These results showcase MI$^2$DAS as an effective, scalable and adaptive framework for enhancing IIoT security against evolving threats.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23595", "abs": "https://arxiv.org/abs/2602.23595", "authors": ["Teng-Yok Lee"], "title": "Incremental dimension reduction for efficient and accurate visual anomaly detection", "comment": null, "summary": "While nowadays visual anomaly detection algorithms use deep neural networks to extract salient features from images, the high dimensionality of extracted features makes it difficult to apply those algorithms to large data with 1000s of images. To address this issue, we present an incremental dimension reduction algorithm to reduce the extracted features. While our algorithm essentially computes truncated singular value decomposition of these features, other than processing all vectors at once, our algorithm groups the vectors into batches. At each batch, our algorithm updates the truncated singular values and vectors that represent all visited vectors, and reduces each batch by its own singular values and vectors so they can be stored in the memory with low overhead. After processing all batches, we re-transform these batch-wise singular vectors to the space spanned by the singular vectors of all features. We show that our algorithm can accelerate the training of state-of-the-art anomaly detection algorithm with close accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u964d\u7ef4\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u6570\u636e\u96c6\u4e0a\u7684\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u8bad\u7ec3", "motivation": "\u51cf\u5c11\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u7684\u7ef4\u5ea6\uff0c\u4ee5\u5904\u7406\u5305\u542b\u6570\u5343\u5f20\u56fe\u50cf\u7684\u5927\u6570\u636e\u96c6", "method": "\u589e\u91cf\u964d\u7ef4\u7b97\u6cd5\uff0c\u5c06\u5411\u91cf\u5206\u7ec4\u5e76\u8ba1\u7b97\u622a\u65ad\u5947\u5f02\u503c\u5206\u89e3", "result": "\u52a0\u901f\u6700\u5148\u8fdb\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bad\u7ec3\uff0c\u7cbe\u5ea6\u63a5\u8fd1", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u7ef4\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5927\u6570\u636e\u96c6\u5904\u7406\u7684\u6548\u7387"}}
{"id": "2602.24083", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24083", "abs": "https://arxiv.org/abs/2602.24083", "authors": ["Xinlong Du", "Harsha Honnappa", "Vinayak Rao"], "title": "Neural Diffusion Intensity Models for Point Process Data", "comment": null, "summary": "Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6269\u6563\u7684Cox\u8fc7\u7a0b\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u6f02\u79fb\u6821\u6b63\u7684SDE\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u540e\u9a8c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5bf9Cox\u8fc7\u7a0b\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u5904\u7406\u8fc7\u5ea6\u5206\u6563\u7684\u70b9\u8fc7\u7a0b\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u7684\u975e\u53c2\u6570\u4f30\u8ba1\u548c\u540e\u9a8c\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5b9e\u73b0\uff0c\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684MCMC\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDEs\uff09\u7684Cox\u8fc7\u7a0b\u53d8\u5206\u6846\u67b6\uff0c\u79f0\u4e3a\u795e\u7ecf\u6269\u6563\u5f3a\u5ea6\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u6f02\u79fb\u6821\u6b63\u7684SDE\uff0c\u5c06\u53ef\u53d8\u957f\u5ea6\u7684\u4e8b\u4ef6\u5e8f\u5217\u6620\u5c04\u5230\u540e\u9a8c\u5f3a\u5ea6\u8def\u5f84\uff0c\u4ece\u800c\u907f\u514d\u4e86\u91cd\u590d\u7684MCMC\u8fd0\u884c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u6062\u590d\u6f5c\u5728\u7684\u5f3a\u5ea6\u52a8\u6001\u548c\u540e\u9a8c\u8def\u5f84\uff0c\u6bd4\u57fa\u4e8eMCMC\u7684\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u795e\u7ecf\u6269\u6563\u5f3a\u5ea6\u6a21\u578b\u4e3aCox\u8fc7\u7a0b\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.23603", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23603", "abs": "https://arxiv.org/abs/2602.23603", "authors": ["Rafid Ishrak Jahan", "Fahmid Shahriar Iqbal", "Sagnik Ray Choudhury"], "title": "LFQA-HP-1M: A Large-Scale Human Preference Dataset for Long-Form Question Answering", "comment": "LREC 2026 Accepted. https://huggingface.co/datasets/nlpatunt/LFQA-HP-1M", "summary": "Long-form question answering (LFQA) demands nuanced evaluation of multi-sentence explanatory responses, yet existing metrics often fail to reflect human judgment. We present LFQA-HP-1M, a large-scale dataset comprising 1.3M human pairwise preference annotations for LFQA. We propose nine rubrics for answer quality evaluation, and show that simple linear models based on these features perform comparably to state-of-the-art LLM evaluators. We further examine transitivity consistency, positional bias, and verbosity biases in LLM evaluators and demonstrate their vulnerability to adversarial perturbations. Overall, this work provides one of the largest public LFQA preference datasets and a rubric-driven framework for transparent and reliable evaluation.", "AI": {"tldr": "\u63d0\u51fa\u5927\u578bLFQA\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u957f\u6587\u672c\u95ee\u7b54\u3002", "motivation": "\u957f\u6587\u672c\u95ee\u7b54\uff08LFQA\uff09\u9700\u8981\u8bc4\u4f30\u591a\u53e5\u89e3\u91ca\u6027\u56de\u7b54\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u4f46\u73b0\u6709\u6307\u6807\u5f80\u5f80\u65e0\u6cd5\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\u3002", "method": "\u63d0\u51faLFQA-HP-1M\uff0c\u4e00\u4e2a\u5305\u542b130\u4e07\u4eba\u7c7b\u6210\u5bf9\u504f\u597d\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5236\u5b9a\u4e5d\u4e2a\u8bc4\u4ef7\u6807\u51c6\u3002", "result": "\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u5728\u8fd9\u4e9b\u7279\u5f81\u7684\u57fa\u7840\u4e0a\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684LLM\u8bc4\u4f30\u5668\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u9879\u5927\u578b\u516c\u5171LFQA\u504f\u597d\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u8bc4\u4ef7\u6807\u51c6\u7684\u900f\u660e\u53ef\u9760\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.23615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23615", "abs": "https://arxiv.org/abs/2602.23615", "authors": ["Jiacheng Yang", "Anqi Chen", "Yunkai Dang", "Qi Fan", "Cong Wang", "Wenbin Li", "Feng Miao", "Yang Gao"], "title": "Annotation-Free Visual Reasoning for High-Resolution Large Multimodal Models via Reinforcement Learning", "comment": null, "summary": "Current Large Multimodal Models (LMMs) struggle with high-resolution visual inputs during the reasoning process, as the number of image tokens increases quadratically with resolution, introducing substantial redundancy and irrelevant information. A common practice is to identify key image regions and refer to their high-resolution counterparts during reasoning, typically trained with external visual supervision. However, such visual supervision cues require costly grounding labels from human annotators. Meanwhile, it remains an open question how to enhance a model's grounding abilities to support reasoning without relying on additional annotations. In this paper, we propose High-resolution Annotation-free Reasoning Technique (HART), a closed-loop framework that enables LMMs to focus on and self-verify key regions of high-resolution visual inputs. HART incorporates a post-training paradigm in which we design Advantage Preference Group Relative Policy Optimization (AP-GRPO) to encourage accurate localization of key regions. Notably, HART provides explainable reasoning pathways and enables efficient optimization of localization. Extensive experiments demonstrate that HART improves performance across a wide range of high-resolution visual tasks, consistently outperforming strong baselines. When applied to post-train Qwen2.5-VL-7B, HART even surpasses larger-scale models such as Qwen2.5-VL-72B and LLaVA-OneVision-72B on high-resolution, vision-centric benchmarks.", "AI": {"tldr": "HART\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6ce8\u91ca\u5373\u53ef\u589e\u5f3aLMMs\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u63a8\u7406\u80fd\u529b\u7684\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u56fe\u50cf\u6807\u8bb0\u7684\u6570\u91cf\u968f\u7740\u5206\u8fa8\u7387\u7684\u589e\u52a0\u800c\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u5f15\u5165\u4e86\u5927\u91cf\u7684\u5197\u4f59\u548c\u4e0d\u76f8\u5173\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHART\u7684\u95ed\u5faa\u73af\u6846\u67b6\uff0c\u4f7fLMMs\u80fd\u591f\u5173\u6ce8\u5e76\u81ea\u6211\u9a8c\u8bc1\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u7684\u5173\u952e\u533a\u57df\u3002HART\u91c7\u7528\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aAP-GRPO\u7684\u4f18\u52bf\u504f\u597d\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u9f13\u52b1\u5bf9\u5173\u952e\u533a\u57df\u7684\u51c6\u786e\u5b9a\u4f4d\u3002", "result": "HART\u5728\u5404\u79cd\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002\u5728\u5e94\u7528\u4e8eQwen2.5-VL-7B\u540e\u8bad\u7ec3\u6a21\u578b\u65f6\uff0cHART\u5728\u89c6\u89c9\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751a\u81f3\u8d85\u8fc7\u4e86Qwen2.5-VL-72B\u548cLLaVA-OneVision-72B\u7b49\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "HART\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u589e\u5f3aLMMs\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u65f6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u7684\u6ce8\u91ca\u3002"}}
{"id": "2602.24207", "categories": ["cs.LG", "cs.CY", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24207", "abs": "https://arxiv.org/abs/2602.24207", "authors": ["Gabriele Farina", "Juan Carlos Perdomo"], "title": "The Stability of Online Algorithms in Performative Prediction", "comment": null, "summary": "The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u65e0\u76d1\u7ba1\u7b97\u6cd5\u5728\u8868\u73b0\u5f0f\u73af\u5883\u4e2d\u4f1a\u6536\u655b\u5230\u7a33\u5b9a\u5747\u8861\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u7a33\u5b9a\u6027\u7684\u539f\u56e0\u3002", "motivation": "\u7814\u7a76\u7b97\u6cd5\u9884\u6d4b\u5728\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u9785\u8bba\u636e\u548c\u5141\u8bb8\u968f\u673a\u5316\uff0c\u907f\u514d\u4e86\u5bf9\u6a21\u578b\u5f71\u54cd\u5206\u5e03\u7684\u4efb\u4f55\u5047\u8bbe\uff0c\u5e76\u7ed5\u8fc7\u4e86\u5bfb\u627e\u7a33\u5b9a\u6a21\u578b\u7684\u4e00\u4e9b\u6700\u65b0\u96be\u5ea6\u7ed3\u679c\u3002", "result": "\u53d1\u73b0\u4e86\u8868\u73b0\u5f0f\u7a33\u5b9a\u5747\u8861\u7684\u5b58\u5728\uff0c\u5e76\u63ed\u793a\u4e86\u4e3a\u4ec0\u4e48\u5e38\u89c1\u7684\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u4e0b\u964d\uff09\u5177\u6709\u81ea\u7136\u7a33\u5b9a\u6027\u548c\u9632\u6b62\u53cd\u9988\u5faa\u73af\u7684\u80fd\u529b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4efb\u4f55\u5728\u8868\u73b0\u5f0f\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u65e0\u6094\u7b97\u6cd5\u90fd\u4f1a\u6536\u655b\u5230\u4e00\u4e2a\uff08\u6df7\u5408\u7684\uff09\u8868\u73b0\u5f0f\u7a33\u5b9a\u5747\u8861\u3002\u8fd9\u79cd\u5747\u8861\u4e2d\uff0c\u6a21\u578b\u4f1a\u4ee5\u4f7f\u5176\u9884\u6d4b\u5728\u4e8b\u540e\u770b\u8d77\u6765\u6700\u4f18\u7684\u65b9\u5f0f\u4e3b\u52a8\u5851\u9020\u6570\u636e\u5206\u5e03\u3002"}}
{"id": "2602.23610", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23610", "abs": "https://arxiv.org/abs/2602.23610", "authors": ["Yu Zhu", "Kai Yang"], "title": "LLM-Driven Multi-Turn Task-Oriented Dialogue Synthesis for Realistic Reasoning", "comment": null, "summary": "The reasoning capability of large language models (LLMs), defined as their ability to analyze, infer, and make decisions based on input information, is essential for building intelligent task-oriented dialogue systems. However, existing benchmarks do not sufficiently reflect the complexity of real-world scenarios, which limits their effectiveness in evaluating and enhancing LLM reasoning in practical contexts. Many current reasoning datasets are overly simplistic and abstract, often disconnected from realistic task flows, domain constraints, and operational rules, making it difficult to effectively evaluate LLMs' logical reasoning ability. In addition, data contamination from pretraining corpora undermines the reliability of evaluation results, and traditional crowdsourcing methods for dataset construction are labor-intensive and difficult to scale. To address these challenges, we propose a LLM-driven framework for synthesizing multi-turn, task-oriented dialogues grounded in realistic reasoning scenarios, leveraging trilevel optimization to enhance dialogue quality. Our method generates dialogues grounded in authentic task scenarios, enriched with real-world information, and exhibiting strong contextual coherence. Corresponding reasoning tasks are carefully designed around these dialogues and iteratively refined to continuously improve the tasks' quality and challenge. The resulting dataset serves as a valuable benchmark for assessing and advancing the realistic logical reasoning capabilities of LLMs. Experimental results show that our synthetic data-based reasoning tasks introduce non-trivial reasoning challenges and provide meaningful support for improving the reasoning capabilities of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u6b21\u4f18\u5316\u589e\u5f3a\u5bf9\u8bdd\u8d28\u91cf\uff0c\u751f\u6210\u57fa\u4e8e\u771f\u5b9e\u63a8\u7406\u573a\u666f\u7684\u591a\u8f6e\u3001\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u63a8\u7406\u4efb\u52a1\u5f15\u5165\u4e86\u975e\u5e73\u51e1\u7684\u63a8\u7406\u6311\u6218\uff0c\u5e76\u4e3a\u63d0\u9ad8LLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u652f\u6301\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347LLMs\u7684\u73b0\u5b9e\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2602.23529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23529", "abs": "https://arxiv.org/abs/2602.23529", "authors": ["Martin \u010cern\u00fd", "David Sychrovsk\u00fd", "Filip \u00daradn\u00edk", "Jakub \u010cern\u00fd"], "title": "Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning", "comment": null, "summary": "Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u903c\u8fd1\u672a\u77e5\u5b50\u53ef\u52a0\u96c6\u5408\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u53d7\u9650\u4e8e\u8ba1\u7b97\u7ecf\u6d4e\u5b66\u3001\u7ec4\u5408\u4f18\u5316\u548c\u4eba\u5de5\u667a\u80fd\u5e94\u7528\uff08\u5982\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\uff09\u4e2d\u5b50\u53ef\u52a0\u96c6\u5408\u51fd\u6570\u7684\u4e0d\u53ef\u7ea6\u6027\uff0c\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u4f55\u7528\u52a0\u6027\u8bef\u5dee\u903c\u8fd1\u672a\u77e5\u5b50\u53ef\u52a0\u96c6\u5408\u51fd\u6570\u3002", "method": "\uff08i\uff09\u5f7b\u5e95\u63a2\u7d22\u4e0d\u540c\u7c7b\u522b\u7684\u7f3a\u5931\u503c\u96c6\u5408\u51fd\u6570\u7684\u6700\u5c0f\u548c\u6700\u5927\u8865\u5168\u4ee5\u53ca\u5b83\u4eec\u7684\u7ed3\u679c\u8ddd\u79bb\uff1b\uff08ii\uff09\u5f00\u53d1\u65b9\u6cd5\u6700\u5c0f\u5316\u5177\u6709\u5df2\u77e5\u5148\u9a8c\u7684\u96c6\u5408\u51fd\u6570\u7c7b\u522b\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u901a\u8fc7\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u65b9\u5f0f\u62ab\u9732\u989d\u5916\u7684\u5b50\u96c6\u503c\uff1b\uff08iii\uff09\u5b9e\u8bc1\u6f14\u793a\u7b97\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u4e3b\u8981\u8d21\u732e\uff0c\u5305\u62ec\u5bf9\u7f3a\u5931\u503c\u96c6\u5408\u51fd\u6570\u7684\u6700\u5c0f\u548c\u6700\u5927\u8865\u5168\u7684\u5f7b\u5e95\u63a2\u7d22\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6700\u5c0f\u5316\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u5bf9\u5b50\u53ef\u52a0\u96c6\u5408\u51fd\u6570\u7684\u903c\u8fd1\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u7f3a\u5931\u503c\uff0c\u5e76\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.23618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23618", "abs": "https://arxiv.org/abs/2602.23618", "authors": ["Peng Dai", "Yu Zhang", "Yiqiang Feng", "Zhen Fan", "Yang Zhang"], "title": "Egocentric Visibility-Aware Human Pose Estimation", "comment": "Conference on Computer Vision and Pattern Recognition 2026", "summary": "Egocentric human pose estimation (HPE) using a head-mounted device is crucial for various VR and AR applications, but it faces significant challenges due to keypoint invisibility. Nevertheless, none of the existing egocentric HPE datasets provide keypoint visibility annotations, and the existing methods often overlook the invisibility problem, treating visible and invisible keypoints indiscriminately during estimation. As a result, their capacity to accurately predict visible keypoints is compromised. In this paper, we first present Eva-3M, a large-scale egocentric visibility-aware HPE dataset comprising over 3.0M frames, with 435K of them annotated with keypoint visibility labels. Additionally, we augment the existing EMHI dataset with keypoint visibility annotations to further facilitate the research in this direction. Furthermore, we propose EvaPose, a novel egocentric visibility-aware HPE method that explicitly incorporates visibility information to enhance pose estimation accuracy. Extensive experiments validate the significant value of ground-truth visibility labels in egocentric HPE settings, and demonstrate that our EvaPose achieves state-of-the-art performance in both Eva-3M and EMHI datasets.", "AI": {"tldr": "EvaPose: A novel egocentric visibility-aware HPE method achieving state-of-the-art performance", "motivation": "Egocentric human pose estimation (HPE) using a head-mounted device is crucial for various VR and AR applications, but it faces significant challenges due to keypoint invisibility.", "method": "We first present Eva-3M, a large-scale egocentric visibility-aware HPE dataset comprising over 3.0M frames, with 435K of them annotated with keypoint visibility labels. Additionally, we augment the existing EMHI dataset with keypoint visibility annotations to further facilitate the research in this direction. Furthermore, we propose EvaPose, a novel egocentric visibility-aware HPE method that explicitly incorporates visibility information to enhance pose estimation accuracy.", "result": "Extensive experiments validate the significant value of ground-truth visibility labels in egocentric HPE settings, and demonstrate that our EvaPose achieves state-of-the-art performance in both Eva-3M and EMHI datasets.", "conclusion": "The proposed EvaPose method significantly improves the accuracy of egocentric HPE by incorporating visibility information, and achieves state-of-the-art performance on the Eva-3M and EMHI datasets."}}
{"id": "2602.23656", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23656", "abs": "https://arxiv.org/abs/2602.23656", "authors": ["Zitong Xu", "Yuqing Wu", "Yue Zhao"], "title": "TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining", "comment": null, "summary": "TRIZ-based contradiction mining is a fundamental task in patent analysis and systematic innovation, as it enables the identification of improving and worsening technical parameters that drive inventive problem solving. However, existing approaches largely rely on rule-based systems or traditional machine learning models, which struggle with semantic ambiguity, domain dependency, and limited generalization when processing complex patent language. Recently, large language models (LLMs) have shown strong semantic understanding capabilities, yet their direct application to TRIZ parameter extraction remains challenging due to hallucination and insufficient grounding in structured TRIZ knowledge. To address these limitations, this paper proposes TRIZ-RAGNER, a retrieval-augmented large language model framework for TRIZ-aware named entity recognition in patent-based contradiction mining. TRIZ-RAGNER reformulates contradiction mining as a semantic-level NER task and integrates dense retrieval over a TRIZ knowledge base, cross-encoder reranking for context refinement, and structured LLM prompting to extract improving and worsening parameters from patent sentences. By injecting domain-specific TRIZ knowledge into the LLM reasoning process, the proposed framework effectively reduces semantic noise and improves extraction consistency. Experiments on the PaTRIZ dataset demonstrate that TRIZ-RAGNER consistently outperforms traditional sequence labeling models and LLM-based baselines. The proposed framework achieves a precision of 85.6%, a recall of 82.9%, and an F1-score of 84.2% in TRIZ contradiction pair identification. Compared with the strongest baseline using prompt-enhanced GPT, TRIZ-RAGNER yields an absolute F1-score improvement of 7.3 percentage points, confirming the effectiveness of retrieval-augmented TRIZ knowledge grounding for robust and accurate patent-based contradiction mining.", "AI": {"tldr": "TRIZ-RAGNER\u5728\u4e13\u5229\u77db\u76fe\u6316\u6398\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e13\u5229\u5206\u6790\u548c\u7cfb\u7edf\u521b\u65b0\u4e2d\u7684TRIZ-based\u77db\u76fe\u6316\u6398\u662f\u4e00\u4e2a\u57fa\u672c\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u8bc6\u522b\u63a8\u52a8\u521b\u65b0\u95ee\u9898\u89e3\u51b3\u7684\u6280\u672f\u53c2\u6570\u7684\u6539\u8fdb\u548c\u6076\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u6216\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u4e13\u5229\u8bed\u8a00\u65f6\u96be\u4ee5\u5e94\u5bf9\u8bed\u4e49\u6b67\u4e49\u3001\u9886\u57df\u4f9d\u8d56\u548c\u6709\u9650\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u76f4\u63a5\u5e94\u7528\u4e8eTRIZ\u53c2\u6570\u63d0\u53d6\u65f6\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b58\u5728\u5e7b\u89c9\u548c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u5316TRIZ\u77e5\u8bc6\u7684\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51faTRIZ-RAGNER\uff0c\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u4e13\u5229\u7684TRIZ\u610f\u8bc6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002\u5c06\u77db\u76fe\u6316\u6398\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bed\u4e49\u7ea7\u522b\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u96c6\u6210\u5bc6\u96c6\u68c0\u7d22\u3001\u4ea4\u53c9\u7f16\u7801\u91cd\u6392\u5e8f\u548c\u7ed3\u6784\u5316LLM\u63d0\u793a\uff0c\u4ee5\u4ece\u4e13\u5229\u53e5\u5b50\u4e2d\u63d0\u53d6\u6539\u8fdb\u548c\u6076\u5316\u7684\u53c2\u6570\u3002\u901a\u8fc7\u5c06\u9886\u57df\u7279\u5b9a\u7684TRIZ\u77e5\u8bc6\u6ce8\u5165LLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u8be5\u6846\u67b6\u6709\u6548\u5730\u51cf\u5c11\u4e86\u8bed\u4e49\u566a\u58f0\u5e76\u63d0\u9ad8\u4e86\u63d0\u53d6\u4e00\u81f4\u6027\u3002", "result": "\u5728PaTRIZ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRIZ-RAGNER\u5728TRIZ\u77db\u76fe\u5bf9\u8bc6\u522b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7684\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u3002\u8be5\u6846\u67b6\u5728TRIZ\u77db\u76fe\u5bf9\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e8685.6%\u7684\u7cbe\u786e\u7387\u300182.9%\u7684\u53ec\u56de\u7387\u548c84.2%\u7684F1\u5206\u6570\u3002\u4e0e\u4f7f\u7528\u63d0\u793a\u589e\u5f3a\u7684GPT\u7684\u6700\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0cTRIZ-RAGNER\u7684F1\u5206\u6570\u63d0\u9ad8\u4e867.3\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u5b9e\u4e86\u68c0\u7d22\u589e\u5f3a\u7684TRIZ\u77e5\u8bc6\u5b9a\u4f4d\u5bf9\u4e8e\u7a33\u5065\u548c\u51c6\u786e\u7684\u57fa\u4e8e\u4e13\u5229\u7684\u77db\u76fe\u6316\u6398\u7684\u6709\u6548\u6027\u3002", "conclusion": "TRIZ-RAGNER\u5728\u4e13\u5229\u77db\u76fe\u6316\u6398\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.24166", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.24166", "abs": "https://arxiv.org/abs/2602.24166", "authors": ["Christian Ewert", "Tim Hardow", "Melf Fritsch", "Leon Dietrich", "Henrik Strunck", "Rainer Buchty", "Mladen Berekovic", "Saleh Mulhem"], "title": "SAILOR: A Scalable and Energy-Efficient Ultra-Lightweight RISC-V for IoT Security", "comment": null, "summary": "Recently, RISC-V has contributed to the development of IoT devices, requiring architectures that balance energy efficiency, compact area, and integrated security. However, most recent RISC-V cores for IoT prioritize either area footprint or energy efficiency, while adding cryptographic support further compromises compactness. As a result, truly integrated architectures that simultaneously optimize efficiency and security remain largely unexplored, leaving constrained IoT environments vulnerable to performance and security trade-offs. In this paper, we introduce SAILOR, an energy-efficient and scalable ultra-lightweight RISC-V core family for cryptographic applications in IoT. Our design is modular and spans 1-, 2-, 4-, 8-, 16-, and 32-bit serialized execution data-paths, prioritizing minimal area. This modular design and adaptable data-path minimizes the overhead of integrating RISC-V cryptography extensions, achieving low hardware cost while significantly improving energy efficiency. We validate our design approach through a comprehensive analysis of area, energy, and efficiency trade-offs. The results surpass state-of-the-art solutions in both performance and energy efficiency by up to 13x and reduce area by up to 59 %, demonstrating that lightweight cryptographic features can be added without prohibitive overhead, and that energy- or area-efficient designs need not compromise performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8282\u80fd\u7684RISC-V\u5185\u6838SAILOR\uff0c\u89e3\u51b3\u4e86\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u7684\u80fd\u6e90\u6548\u7387\u3001\u9762\u79ef\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524dRISC-V\u5185\u6838\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5728\u80fd\u6e90\u6548\u7387\u3001\u9762\u79ef\u548c\u96c6\u6210\u5b89\u5168\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86SAILOR\uff0c\u8fd9\u662f\u4e00\u4e2a\u8282\u80fd\u548c\u53ef\u6269\u5c55\u7684\u8d85\u8f7b\u91cf\u7ea7RISC-V\u5185\u6838\u7cfb\u5217\uff0c\u9002\u7528\u4e8e\u7269\u8054\u7f51\u4e2d\u7684\u52a0\u5bc6\u5e94\u7528\u3002\u8bbe\u8ba1\u662f\u6a21\u5757\u5316\u7684\uff0c\u5305\u62ec1-\u30012-\u30014-\u30018-\u300116-\u548c32\u4f4d\u5e8f\u5217\u5316\u6267\u884c\u6570\u636e\u8def\u5f84\u3002", "result": "SAILOR\u5728\u6027\u80fd\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u9ad8\u8fbe13\u500d\uff0c\u9762\u79ef\u51cf\u5c11\u4e86\u9ad8\u8fbe59%\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u7269\u8054\u7f51\u4e2d\uff0c\u8f7b\u91cf\u7ea7\u52a0\u5bc6\u529f\u80fd\u53ef\u4ee5\u6dfb\u52a0\u800c\u4e0d\u4ea7\u751f\u8fc7\u9ad8\u7684\u5f00\u9500\uff0c\u5e76\u4e14\u80fd\u6e90\u6216\u9762\u79ef\u9ad8\u6548\u7684\u8bbe\u8ba1\u4e0d\u5fc5\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2602.23622", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23622", "abs": "https://arxiv.org/abs/2602.23622", "authors": ["Shibo Hong", "Boxian Ai", "Jun Kuang", "Wei Wang", "FengJiao Chen", "Zhongyuan Peng", "Chenhao Huang", "Yixin Cao"], "title": "DLEBench: Evaluating Small-scale Object Editing Ability for Instruction-based Image Editing Model", "comment": null, "summary": "Significant progress has been made in the field of Instruction-based Image Editing Models (IIEMs). However, while these models demonstrate plausible adherence to instructions and strong reasoning ability on current benchmarks, their ability to edit small objects remains underexplored, despite its importance for precise local editing and refining details in both real and generated images. In this paper, we introduce DeepLookEditBench (DLEBench), the first benchmark dedicated to assessing the abilities of IIEMs in editing small-scale objects. Specifically, we construct a challenging testbed comprising 1889 samples across seven instruction types. In these samples, target objects occupy only 1%-10% of the image area, covering complex scenarios such as partial occlusion and multi-object editing. To ensure robust evaluation on this benchmark, we propose an evaluation protocol with refined score rubrics to minimize subjectivity and ambiguity in two criteria: Instruction Following and Visual Consistency. This protocol also introduces a dual-mode evaluation framework (Tool-driven and Oracle-guided Modes) addressing the misalignment between LMM-as-a-Judge and human judgements on DLEBench. Empirical results on 10 IIEMs reveal significant performance gaps in small-scale object editing, highlighting the need for specialized benchmarks to advance this ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86DLEBench\uff0c\u4ee5\u89e3\u51b3IIEMs\u5728\u5c0f\u89c4\u6a21\u5bf9\u8c61\u7f16\u8f91\u4e2d\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5728\u73b0\u6709\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e2d\uff0c\u5c0f\u89c4\u6a21\u5bf9\u8c61\u7f16\u8f91\u7684\u80fd\u529b\u4ecd\u88ab\u4f4e\u4f30\uff0c\u5c3d\u7ba1\u8fd9\u5728\u7cbe\u786e\u7684\u5c40\u90e8\u7f16\u8f91\u548c\u7ec6\u5316\u7ec6\u8282\u65b9\u9762\u975e\u5e38\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86DeepLookEditBench\uff08DLEBench\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30IIEMs\u5728\u5c0f\u89c4\u6a21\u5bf9\u8c61\u7f16\u8f91\u80fd\u529b\u7684\u57fa\u51c6\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1889\u4e2a\u6837\u672c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u76ee\u6807\u5bf9\u8c61\u4ec5\u5360\u56fe\u50cf\u9762\u79ef\u76841%-10%\uff0c\u6db5\u76d6\u4e86\u90e8\u5206\u906e\u6321\u548c\u591a\u5bf9\u8c61\u7f16\u8f91\u7b49\u590d\u6742\u573a\u666f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u7ec6\u5316\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u4ee5\u51cf\u5c11\u4e3b\u89c2\u6027\u548c\u6a21\u7cca\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u6a21\u5f0f\u8bc4\u4f30\u6846\u67b6\uff08\u5de5\u5177\u9a71\u52a8\u548cOracle\u5f15\u5bfc\u6a21\u5f0f\uff09\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u572810\u4e2aIIEMs\u4e2d\uff0c\u5c0f\u89c4\u6a21\u5bf9\u8c61\u7f16\u8f91\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a81\u51fa\u4e86\u53d1\u5c55\u8fd9\u4e00\u80fd\u529b\u6240\u9700\u7684\u4e13\u95e8\u57fa\u51c6\u3002", "conclusion": "DLEBench\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u548c\u63a8\u52a8\u5c0f\u89c4\u6a21\u5bf9\u8c61\u7f16\u8f91\u5728IIEMs\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23730", "abs": "https://arxiv.org/abs/2602.23730", "authors": ["Longyin Zhang", "Shuo Sun", "Yingxu He", "Won Cheng Yi Lewis", "Muhammad Huzaifah Bin Md Shahrin", "Hardik Bhupendra Sailor", "Heng Meng Jeremy Wong", "Tarun Kumar Vangani", "Yi Ma", "Qiongqiong Wang", "Minh Duc Pham", "Ridong Jiang", "Jingtao Li", "Jingyi Liao", "Zhuohan Liu", "Yanfeng Lu", "Manas Gupta", "Ai Ti Aw"], "title": "Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates \"System 1\" (Perception) and \"System 2\" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.\n  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.", "AI": {"tldr": "MERaLiON2-Omni\uff08Alpha\uff09\u6a21\u578b\u901a\u8fc7\u6b63\u4ea4\u6a21\u6001\u81ea\u9002\u5e94\u548cGenerate-Judge-Refine\u6d41\u7a0b\u5b9e\u73b0\u591a\u6a21\u6001\u611f\u77e5\u548c\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u6548\u7387-\u7a33\u5b9a\u6027\u6096\u8bba\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5c06\u7a33\u5065\u7684\u611f\u89c9\u57fa\u7840\u4e0e\u590d\u6742\u7684\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5730\u533a\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86MERaLiON2-Omni\uff08Alpha\uff09\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6b63\u4ea4\u6a21\u6001\u81ea\u9002\u5e94\u5c06\u7279\u5b9a\u4e8e\u5730\u533a\u7684\u97f3\u9891-\u89c6\u89c9\u7ebf\u7d22\uff08\u4f8b\u5982\uff0cSinglish\u4ee3\u7801\u8f6c\u6362\u3001\u5f53\u5730\u6587\u5316\u5730\u6807\uff09\u4e0e\u591a\u8bed\u8a00LLM\u5bf9\u9f50\uff0c\u5efa\u7acb\u7a33\u5065\u7684\u611f\u89c9\u9aa8\u5e72\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684Generate-Judge-Refine\u6d41\u7a0b\uff0c\u5229\u7528Super-LLM\u8fdb\u884c\u5e7b\u89c9\u8fc7\u6ee4\u548c\u51b2\u7a81\u89e3\u51b3\u3002", "result": "\u5728SEA-Omni\u57fa\u51c6\u5957\u4ef6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u6548\u7387-\u7a33\u5b9a\u6027\u6096\u8bba\u3002\u63a8\u7406\u5728\u62bd\u8c61\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u975e\u7ebf\u6027\u653e\u5927\u5668\uff0c\u4f46\u5728\u4f4e\u7ea7\u611f\u5b98\u5904\u7406\u4e2d\u5f15\u5165\u4e86\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u67b6\u6784\u3001\u6570\u636e\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u4ee5\u53ca\u7a33\u5065\u611f\u77e5\u548c\u7ed3\u6784\u5316\u63a8\u7406\u4e4b\u95f4\u7684\u6743\u8861\u7684\u8bca\u65ad\u5206\u6790\u3002"}}
{"id": "2602.23565", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23565", "abs": "https://arxiv.org/abs/2602.23565", "authors": ["Adhyyan Narang", "Sarah Dean", "Lillian J Ratliff", "Maryam Fazel"], "title": "Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing", "comment": null, "summary": "In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the \"local\" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to \"probe\" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fc7\u5ea6\u4e13\u4e1a\u5316\u9677\u9631\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8bb8\u591a\u7ecf\u6d4e\u76f8\u5173\u573a\u666f\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u88ab\u90e8\u7f72\u5728\u591a\u4e2a\u5e73\u53f0\u4e0a\uff0c\u6bcf\u4e2a\u5e73\u53f0\u90fd\u4ece\u540c\u4e00\u7528\u6237\u6c60\u4e2d\u83b7\u53d6\u6570\u636e\uff0c\u6bcf\u4e2a\u7528\u6237\u90fd\u9009\u62e9\u6700\u80fd\u6ee1\u8db3\u4ed6\u4eec\u9700\u6c42\u7684\u5e73\u53f0\u3002\u5148\u524d\u7684\u5de5\u4f5c\u4ec5\u5173\u6ce8\u5b66\u4e60\u8005\u5728\u4ed6\u4eec\u89c2\u5bdf\u5230\u7684\u6570\u636e\u5206\u5e03\u4e0a\u7684\u201c\u5c40\u90e8\u201d\u635f\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u5b66\u4e60\u8005\u201c\u63a2\u6d4b\u201d\u540c\u4f34\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u4e86\u89e3\u672a\u9009\u62e9\u4ed6\u4eec\u7684\u7528\u6237\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5f53\u63a2\u6d4b\u6e90\u8db3\u591f\u4fe1\u606f\u4e30\u5bcc\u65f6\uff08\u4f8b\u5982\uff0c\u5df2\u77e5\u7684\u884c\u4e1a\u9886\u5bfc\u8005\u6216\u5177\u6709\u826f\u597d\u5168\u5c40\u6027\u80fd\u7684\u5927\u591a\u6570\u540c\u4f34\uff09\uff0c\u8be5\u8fc7\u7a0b\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u4e00\u4e2a\u5177\u6709\u6709\u754c\u5168\u4eba\u53e3\u98ce\u9669\u7684\u5e73\u7a33\u70b9\u3002", "conclusion": "\u901a\u8fc7\u53cd\u9988\u8bf1\u5bfc\u7684\u673a\u5236\uff0c\u5373\u8fc7\u5ea6\u4e13\u4e1a\u5316\u9677\u9631\uff0c\u4f7f\u7528\u73b0\u6709\u7b97\u6cd5\u7684\u5b66\u4e60\u8005\u51e0\u4e4e\u80af\u5b9a\u6536\u655b\u5230\u5177\u6709\u4efb\u610f\u5dee\u5168\u5c40\u6027\u80fd\u7684\u6a21\u578b\u3002"}}
{"id": "2602.23645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23645", "abs": "https://arxiv.org/abs/2602.23645", "authors": ["Tongyan Hua", "Haoran Gong", "Yuan Liu", "Di Wang", "Ying-Cong Chen", "Wufan Zhao"], "title": "BuildAnyPoint: 3D Building Structured Abstraction from Diverse Point Clouds", "comment": "CVPR 2026", "summary": "We introduce BuildAnyPoint, a novel generative framework for structured 3D building reconstruction from point clouds with diverse distributions, such as those captured by airborne LiDAR and Structure-from-Motion. To recover artist-created building abstraction in this highly underconstrained setting, we capitalize on the role of explicit 3D generative priors in autoregressive mesh generation. Specifically, we design a Loosely Cascaded Diffusion Transformer (Loca-DiT) that initially recovers the underlying distribution from noisy or sparse points, followed by autoregressively encapsulating them into compact meshes. We first formulate distribution recovery as a conditional generation task by training latent diffusion models conditioned on input point clouds, and then tailor a decoder-only transformer for conditional autoregressive mesh generation based on the recovered point clouds. Our method delivers substantial qualitative and quantitative improvements over prior building abstraction methods. Furthermore, the effectiveness of our approach is evidenced by the strong performance of its recovered point clouds on building point cloud completion benchmarks, which exhibit improved surface accuracy and distribution uniformity.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23753", "abs": "https://arxiv.org/abs/2602.23753", "authors": ["Jiasen Zheng", "Zijun Zhou", "Huajun Zhang", "Junjiang Lin", "Jingyun Jia", "Qi Wang"], "title": "Structured Prompt Optimization for Few-Shot Text Classification via Semantic Alignment in Latent Space", "comment": null, "summary": "This study addresses the issues of semantic entanglement, unclear label structure, and insufficient feature representation in few-shot text classification, and proposes an optimization framework based on structured prompts to enhance semantic understanding and task adaptation under low-resource conditions. The framework first uses a pretrained language model to encode the input text and obtain basic semantic representations. It then introduces structured prompts composed of multi-dimensional semantic factors and integrates them with text features through a learnable combination mechanism, which forms task-related representations with clear boundaries in the latent space. To further strengthen the consistency between text representations and label semantics, the method constructs a structured label embedding matrix and employs a cross-space alignment mechanism to ensure stable matching between textual features and label attributes. In addition, the model applies prompt orthogonality constraints and a joint optimization objective to maintain independence across different semantic factors in the prompts, allowing the structured prompts to provide transparent and controllable guidance for classification decisions. Three types of sensitivity experiments, including learning rate sensitivity, prompt length sensitivity, and data scale sensitivity, are designed to evaluate the stability and robustness of the framework under different conditions. Experimental results show that the proposed structured prompt optimization framework effectively alleviates semantic conflicts and label ambiguity in few-shot text classification. It significantly improves performance on accuracy, precision, recall, and AUC, and demonstrates strong cross-task applicability.", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u4f18\u5316\u6846\u67b6\u63d0\u9ad8\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u6027\u80fd", "motivation": "\u89e3\u51b3\u8bed\u4e49\u7ea0\u7f20\u3001\u6807\u7b7e\u7ed3\u6784\u4e0d\u660e\u786e\u548c\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u7b49\u95ee\u9898", "method": "\u57fa\u4e8e\u7ed3\u6784\u5316\u63d0\u793a\u7684\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7f16\u7801\u548c\u8bed\u4e49\u8868\u793a\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63d0\u793a\u548c\u6587\u672c\u7279\u5f81\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u6807\u7b7e\u5d4c\u5165\u77e9\u9635\u548c\u8de8\u7a7a\u95f4\u5bf9\u9f50\u673a\u5236\uff0c\u5e94\u7528\u63d0\u793a\u6b63\u4ea4\u7ea6\u675f\u548c\u8054\u5408\u4f18\u5316\u76ee\u6807", "result": "\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cAUC\uff0c\u5177\u6709\u826f\u597d\u7684\u8de8\u4efb\u52a1\u9002\u7528\u6027", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u4e2d\u6709\u6548\u7f13\u89e3\u4e86\u8bed\u4e49\u51b2\u7a81\u548c\u6807\u7b7e\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.23777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23777", "abs": "https://arxiv.org/abs/2602.23777", "authors": ["Zhipeng Xu", "Zilong Wang", "Xinyang Jiang", "Dongsheng Li", "De Cheng", "Nannan Wang"], "title": "Reasoning-Driven Multimodal LLM for Domain Generalization", "comment": "Accepted at ICLR 2026 (Poster)", "summary": "This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u95ee\u9898", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6784\u5efa\u63a8\u7406\u94fe\u6765\u63d0\u9ad8\u9884\u6d4b\u7684\u9c81\u68d2\u6027", "result": "\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5305\u542bMTCT\u548cSARR\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5728\u6807\u51c6DomainBed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u63a8\u7406\u662f\u9c81\u68d2\u8de8\u9886\u57df\u6cdb\u5316\u7684\u6709\u5e0c\u671b\u7684\u8865\u5145\u4fe1\u53f7"}}
{"id": "2602.23798", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23798", "abs": "https://arxiv.org/abs/2602.23798", "authors": ["Tiantong Wang", "Xinyu Yan", "Tiantong Wu", "Yurong Hao", "Yong Jiang", "Fei Huang", "Wei Yang Bryan Lim"], "title": "MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models", "comment": null, "summary": "Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.", "code_url": "https://github.com/Tristan-SHU/MPU", "code_stars": 0, "code_last_update": "2026-02-11", "AI": {"tldr": "MPU\u662f\u4e00\u79cd\u7b97\u6cd5\u65e0\u5173\u7684\u9690\u79c1\u4fdd\u62a4\u672a\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u672a\u5b66\u4e60\u6027\u80fd\u4e0a\u4e0e\u65e0\u566a\u58f0\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u57281%\u566a\u58f0\u4e0b\u67d0\u4e9b\u7b97\u6cd5\u751a\u81f3\u4f18\u4e8e\u65e0\u566a\u58f0\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u672a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9690\u79c1\u56f0\u5883\u7684\u95ee\u9898", "method": "\u63d0\u51faMPU\u7b97\u6cd5\uff0c\u5305\u62ec\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u6a21\u5757", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMPU\u5728\u672a\u5b66\u4e60\u6027\u80fd\u4e0a\u4e0e\u65e0\u566a\u58f0\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u57281%\u566a\u58f0\u4e0b\u67d0\u4e9b\u7b97\u6cd5\u751a\u81f3\u4f18\u4e8e\u65e0\u566a\u58f0\u57fa\u7ebf", "conclusion": "MPU\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u672a\u5b66\u4e60\u6846\u67b6"}}
{"id": "2602.23652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23652", "abs": "https://arxiv.org/abs/2602.23652", "authors": ["Haowen Zhu", "Ning Yin", "Xiaogen Zhou"], "title": "3D Modality-Aware Pre-training for Vision-Language Model in MRI Multi-organ Abnormality Detection", "comment": null, "summary": "Vision-language models (VLMs) show strong potential for complex diagnostic tasks in medical imaging. However, applying VLMs to multi-organ medical imaging introduces two principal challenges: (1) modality-specific vision-language alignment and (2) cross-modal feature fusion. In this work, we propose MedMAP, a Medical Modality-Aware Pretraining framework that enhances vision-language representation learning in 3D MRI. MedMAP comprises a modality-aware vision-language alignment stage and a fine-tuning stage for multi-organ abnormality detection. During the pre-training stage, the modality-aware encoders implicitly capture the joint modality distribution and improve alignment between visual and textual representations. We then fine-tune the pre-trained vision encoders (while keeping the text encoder frozen) for downstream tasks. To this end, we curated MedMoM-MRI3D, comprising 7,392 3D MRI volume-report pairs spanning twelve MRI modalities and nine abnormalities tailored for various 3D medical analysis tasks. Extensive experiments on MedMoM-MRI3D demonstrate that MedMAP significantly outperforms existing VLMs in 3D MRI-based multi-organ abnormality detection. Our code is available at https://github.com/RomantiDr/MedMAP.", "code_url": "https://github.com/RomantiDr/MedMAP", "code_stars": 0, "code_last_update": "2025-10-26", "AI": {"tldr": "MedMAP\u662f\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u6846\u67b6\uff0c\u5728\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u8bca\u65ad\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5c06VLMs\u5e94\u7528\u4e8e\u591a\u5668\u5b98\u533b\u5b66\u5f71\u50cf\u5f15\u5165\u4e86\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\uff081\uff09\u6a21\u6001\u7279\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548c\uff082\uff09\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedMAP\u7684\u533b\u5b66\u6a21\u6001\u611f\u77e5\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a3D MRI\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u3002MedMAP\u5305\u62ec\u4e00\u4e2a\u6a21\u6001\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u9636\u6bb5\u548c\u4e00\u4e2a\u7528\u4e8e\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u7684\u5fae\u8c03\u9636\u6bb5\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6a21\u6001\u611f\u77e5\u7f16\u7801\u5668\u9690\u5f0f\u5730\u6355\u83b7\u8054\u5408\u6a21\u6001\u5206\u5e03\uff0c\u5e76\u63d0\u9ad8\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u7136\u540e\uff0c\u6211\u4eec\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff08\u540c\u65f6\u51bb\u7ed3\u6587\u672c\u7f16\u7801\u5668\uff09\u4ee5\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMedMoM-MRI3D\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b7,392\u4e2a3D MRI\u4f53\u79ef\u62a5\u544a\u5bf9\uff0c\u6db5\u76d6\u5341\u4e8c\u79cdMRI\u6a21\u6001\u548c\u4e5d\u79cd\u5f02\u5e38\uff0c\u9488\u5bf9\u5404\u79cd3D\u533b\u5b66\u5206\u6790\u4efb\u52a1\u3002", "result": "\u5728MedMoM-MRI3D\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMedMAP\u5728\u57fa\u4e8e3D MRI\u7684\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VLMs\u3002", "conclusion": "MedMAP\u57283D MRI\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.23792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23792", "abs": "https://arxiv.org/abs/2602.23792", "authors": ["Xiangzhong Luo", "Yilin An", "Zhicheng Yu", "Weichen Liu", "Xu Yang"], "title": "Divide and Conquer: Accelerating Diffusion-Based Large Language Models via Adaptive Parallel Decoding", "comment": "11 pages, 7 figures", "summary": "Diffusion-based large language models (dLLMs) have shown promising performance across various reasoning tasks, establishing themselves as an alternative to autoregressive large language models (LLMs). Unlike autoregressive LLMs that generate one token per step based on all previous tokens, dLLMs theoretically enable parallel generation of multiple tokens at each decoding step. However, recent dLLMs still favor one-token-per-step generation in practice, as directly decoding multiple masked tokens often leads to degraded generation quality and stability. This reveals a substantial gap between the theoretical parallelism and practical performance of dLLMs. To bridge this gap, we introduce an adaptive parallel decoding approach, namely DiCo, which features a three-phase divide-and-conquer paradigm to unleash the inherent parallelism of dLLMs. During the Divide phase, DiCo first explores the input masked sequence and identifies masked tokens as seed tokens, which are then expanded to construct a set of local clusters. During the Conquer phase, DiCo performs parallel decoding across different local clusters constructed in the Divide phase. The divide-and-conquer process repeatedly alternates between the Divide and Conquer phases until convergence. During the Finalize phase, DiCo decodes the remaining few masked tokens using an effective fine-grained compound decoding scheme to finalize the generation. Extensive experiments demonstrate that DiCo can achieve significant inference speedups while maintaining competitive generation quality.", "AI": {"tldr": "DiCo\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8dLLMs\u7684\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6269\u6563\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u5e76\u884c\u89e3\u7801\u65b9\u9762\u7684\u5b9e\u9645\u6027\u80fd\u4e0e\u7406\u8bba\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiCo\u7684\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5212\u5206-\u5f81\u670d\u8303\u5f0f\u6765\u91ca\u653edLLMs\u7684\u5185\u5728\u5e76\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiCo\u53ef\u4ee5\u5728\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "DiCo\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8dLLMs\u7684\u63a8\u7406\u901f\u5ea6\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002"}}
{"id": "2602.23802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23802", "abs": "https://arxiv.org/abs/2602.23802", "authors": ["Yiyang Fang", "Wenke Huang", "Pei Fu", "Yihao Yang", "Kehua Su", "Zhenbo Luo", "Jian Luan", "Mang Ye"], "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models", "comment": "Accepted by CVPR 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.", "AI": {"tldr": "EMO-R3\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "MLLMs\u5728\u89c6\u89c9\u63a8\u7406\u548c\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u7684\u4e3b\u89c2\u6027\u548c\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faReflective Reinforcement Learning for Emotional Reasoning (EMO-R3)\u6846\u67b6\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u60c5\u611f\u601d\u8003\u548c\u8bbe\u8ba1\u53cd\u601d\u60c5\u611f\u5956\u52b1\uff0c\u4ee5\u589e\u5f3aMLLM\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cEMO-R3\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u7684\u89e3\u91ca\u6027\u548c\u60c5\u611f\u667a\u529b\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u60c5\u611f\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "EMO-R3\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u589e\u5f3aMLLM\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u5176\u5728\u89c6\u89c9\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.23653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23653", "abs": "https://arxiv.org/abs/2602.23653", "authors": ["Wei Luo", "Yangfan Ou", "Jin Deng", "Zeshuai Deng", "Xiquan Yan", "Zhiquan Wen", "Mingkui Tan"], "title": "ProtoDCS: Towards Robust and Efficient Open-Set Test-Time Adaptation for Vision-Language Models", "comment": "13 pages, under review", "summary": "Large-scale Vision-Language Models (VLMs) exhibit strong zero-shot recognition, yet their real-world deployment is challenged by distribution shifts. While Test-Time Adaptation (TTA) can mitigate this, existing VLM-based TTA methods operate under a closed-set assumption, failing in open-set scenarios where test streams contain both covariate-shifted in-distribution (csID) and out-of-distribution (csOOD) data. This leads to a critical difficulty: the model must discriminate unknown csOOD samples to avoid interference while simultaneously adapting to known csID classes for accuracy. Current open-set TTA (OSTTA) methods rely on hard thresholds for separation and entropy minimization for adaptation. These strategies are brittle, often misclassifying ambiguous csOOD samples and inducing overconfident predictions, and their parameter-update mechanism is computationally prohibitive for VLMs. To address these limitations, we propose Prototype-based Double-Check Separation (ProtoDCS), a robust framework for OSTTA that effectively separates csID and csOOD samples, enabling safe and efficient adaptation of VLMs to csID data. Our main contributions are: (1) a novel double-check separation mechanism employing probabilistic Gaussian Mixture Model (GMM) verification to replace brittle thresholding; and (2) an evidence-driven adaptation strategy utilizing uncertainty-aware loss and efficient prototype-level updates, mitigating overconfidence and reducing computational overhead. Extensive experiments on CIFAR-10/100-C and Tiny-ImageNet-C demonstrate that ProtoDCS achieves state-of-the-art performance, significantly boosting both known-class accuracy and OOD detection metrics. Code will be available at https://github.com/O-YangF/ProtoDCS.", "code_url": "https://github.com/O-YangF/ProtoDCS", "code_stars": 0, "code_last_update": "2025-12-15", "AI": {"tldr": "ProtoDCS: A robust framework for open-set TTA in VLMs, improving known-class accuracy and OOD detection metrics.", "motivation": "Existing VLM-based TTA methods fail in open-set scenarios with both csID and csOOD data, leading to misclassification and overconfident predictions.", "method": "Prototype-based Double-Check Separation (ProtoDCS) using probabilistic Gaussian Mixture Model (GMM) and evidence-driven adaptation.", "result": "ProtoDCS achieves significant improvements in known-class accuracy and OOD detection metrics.", "conclusion": "ProtoDCS is a robust framework for OSTTA that achieves state-of-the-art performance in OOD detection metrics."}}
{"id": "2602.23826", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23826", "abs": "https://arxiv.org/abs/2602.23826", "authors": ["Sebastian Gerstner", "Hinrich Sch\u00fctze"], "title": "GLUScope: A Tool for Analyzing GLU Neurons in Transformer Language Models", "comment": "6 pages for main body, 9 pages in total. 4 figures", "summary": "We present GLUScope, an open-source tool for analyzing neurons in Transformer-based language models, intended for interpretability researchers. We focus on more recent models than previous tools do; specifically we consider gated activation functions such as SwiGLU. This introduces a new challenge: understanding positive activations is not enough. Instead, both the gate and the in activation of a neuron can be positive or negative, leading to four different possible sign combinations that in some cases have quite different functionalities. Accordingly, for any neuron, our tool shows text examples for each of the four sign combinations, and indicates how often each combination occurs. We describe examples of how our tool can lead to novel insights. A demo is available at https: //sjgerstner.github.io/gluscope.", "AI": {"tldr": "GLUScope\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790Transformer-based\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u5143\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8Transformer-based\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u4e00\u4e2a\u5de5\u5177\u6765\u5206\u6790\u5176\u4e2d\u7684\u795e\u7ecf\u5143\u3002", "method": "\u5f00\u53d1\u4e86GLUScope\uff0c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790Transformer-based\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u5143\u3002\u8be5\u5de5\u5177\u8003\u8651\u4e86\u95e8\u63a7\u6fc0\u6d3b\u51fd\u6570\uff0c\u5982SwiGLU\u3002", "result": "GLUScope\u5de5\u5177\u80fd\u591f\u663e\u793a\u6bcf\u4e2a\u795e\u7ecf\u5143\u7684\u56db\u79cd\u4e0d\u540c\u7b26\u53f7\u7ec4\u5408\u7684\u6587\u672c\u793a\u4f8b\uff0c\u5e76\u6307\u51fa\u6bcf\u79cd\u7ec4\u5408\u51fa\u73b0\u7684\u9891\u7387\u3002\u5b83\u6709\u52a9\u4e8e\u53d1\u73b0\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "GLUScope\u5de5\u5177\u6709\u52a9\u4e8e\u63d0\u9ad8Transformer-based\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u795e\u7ecf\u5143\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.23581", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23581", "abs": "https://arxiv.org/abs/2602.23581", "authors": ["Xiang Ao"], "title": "SDMixer: Sparse Dual-Mixer for Time Series Forecasting", "comment": "12pages,2 figures", "summary": "Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer", "code_url": "https://github.com/SDMixer/SDMixer", "code_stars": 0, "code_last_update": "2025-11-23", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u6d41\u7a00\u758fMixer\u6846\u67b6\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u4ea4\u901a\u3001\u80fd\u6e90\u548c\u91d1\u878d\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u6570\u636e\u901a\u5e38\u5b58\u5728\u591a\u5c3a\u5ea6\u7279\u6027\u3001\u5f31\u76f8\u5173\u6027\u4ee5\u53ca\u566a\u58f0\u5e72\u6270\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u73b0\u6709\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u7a00\u758fMixer\u9884\u6d4b\u6846\u67b6\uff0c\u5206\u522b\u4ece\u9891\u7387\u548c\u65f6\u95f4\u57df\u7684\u5e8f\u5217\u4e2d\u63d0\u53d6\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u52a8\u6001\u7279\u5f81\u3002\u91c7\u7528\u7a00\u758f\u673a\u5236\u8fc7\u6ee4\u65e0\u6548\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u4ea4\u53c9\u53d8\u91cf\u4f9d\u8d56\u5efa\u6a21\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.23676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23676", "abs": "https://arxiv.org/abs/2602.23676", "authors": ["Ao Li", "Rui Liu", "Mingjie Li", "Sheng Liu", "Lei Wang", "Xiaodan Liang", "Lina Yao", "Xiaojun Chang", "Lei Xing"], "title": "Suppressing Prior-Comparison Hallucinations in Radiology Report Generation via Semantically Decoupled Latent Steering", "comment": "15 pages, 5 figures", "summary": "Automated radiology report generation using vision-language models (VLMs) is limited by the risk of prior-comparison hallucination, where the model generates historical findings unsupported by the current study. We address this challenge with a training-free, inference-time control framework termed Semantically Decoupled Latent Steering (SDLS). Unlike generic activation steering, which often suffers from semantic entanglement, our approach constructs a semantic-free intervention vector via large language model (LLM)-driven semantic decomposition followed by $QR$-based orthogonalization. This orthogonalization step is critical. It leverages geometric constraints to filter out the clinical semantics often entangled in standard principal component analysis (PCA) directions, ensuring that the steering vector targets only the ``historical comparison\" axis. We validate our method on the BiomedGPT foundation model, demonstrating that it overcomes the trade-off between hallucination suppression and clinical accuracy. Extensive experiments on MIMIC-CXR, and zero-shot transfer evaluation on CheXpert Plus and IU-Xray, demonstrate the robustness of our approach. Quantitative evaluations on MIMIC-CXR show that our approach significantly reduces the probability of historical hallucinations (FilBERT score decreases from 0.2373 to 0.1889) and improves clinical label fidelity (CheXpert macro-F1 increases from 0.2242 to 0.3208). Supplementary evaluations confirm that the structural integrity of the clinical narrative is maintained.", "AI": {"tldr": "\u901a\u8fc7SDLS\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5386\u53f2\u5e7b\u89c9\uff0c\u63d0\u9ad8\u4e34\u5e8a\u6807\u7b7e\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u65f6\uff0c\u7531\u4e8e\u5386\u53f2\u6bd4\u8f83\u5e7b\u89c9\u5bfc\u81f4\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u8bad\u7ec3\u514d\u8d39\u7684SDLS\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u548c$QR$\u6b63\u4ea4\u5316\u6784\u5efa\u8bed\u4e49\u65e0\u5173\u7684\u5e72\u9884\u5411\u91cf\u3002", "result": "\u5728MIMIC-CXR\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u548cCheXpert Plus\u548cIU-Xray\u4e0a\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528SDLS\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5386\u53f2\u5e7b\u89c9\u7684\u53ef\u80fd\u6027\u5e76\u63d0\u9ad8\u4e86\u4e34\u5e8a\u6807\u7b7e\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.23599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23599", "abs": "https://arxiv.org/abs/2602.23599", "authors": ["Dang Sy Duy", "Nguyen Duy Chien", "Kapil Dev", "Jeff Nijsse"], "title": "Normalisation and Initialisation Strategies for Graph Neural Networks in Blockchain Anomaly Detection", "comment": "14 pages, 5 figures", "summary": "Graph neural networks (GNNs) offer a principled approach to financial fraud detection by jointly learning from node features and transaction graph topology. However, their effectiveness on real-world anti-money laundering (AML) benchmarks depends critically on training practices such as specifically weight initialisation and normalisation that remain underexplored. We present a systematic ablation of initialisation and normalisation strategies across three GNN architectures (GCN, GAT, and GraphSAGE) on the Elliptic Bitcoin dataset. Our experiments reveal that initialisation and normalisation are architecture-dependent: GraphSAGE achieves the strongest performance with Xavier initialisation alone, GAT benefits most from combining GraphNorm with Xavier initialisation, while GCN shows limited sensitivity to these modifications. These findings offer practical, architecture-specific guidance for deploying GNNs in AML pipelines for datasets with severe class imbalance. We release a reproducible experimental framework with temporal data splits, seeded runs, and full ablation results.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23677", "abs": "https://arxiv.org/abs/2602.23677", "authors": ["Nazia Hossain", "Xintong Jiang", "Yu Tian", "Philippe Seguin", "O. Grant Clark", "Shangpeng Sun"], "title": "Vision-Language Semantic Grounding for Multi-Domain Crop-Weed Segmentation", "comment": null, "summary": "Fine-grained crop-weed segmentation is essential for enabling targeted herbicide application in precision agriculture. However, existing deep learning models struggle to generalize across heterogeneous agricultural environments due to reliance on dataset-specific visual features. We propose Vision-Language Weed Segmentation (VL-WS), a novel framework that addresses this limitation by grounding pixel-level segmentation in semantically aligned, domain-invariant representations. Our architecture employs a dual-encoder design, where frozen Contrastive Language-Image Pretraining (CLIP) embeddings and task-specific spatial features are fused and modulated via Feature-wise Linear Modulation (FiLM) layers conditioned on natural language captions. This design enables image level textual descriptions to guide channel-wise feature refinement while preserving fine-grained spatial localization. Unlike prior works restricted to training and evaluation on single-source datasets, VL-WS is trained on a unified corpus that includes close-range ground imagery (robotic platforms) and high-altitude UAV imagery, covering diverse crop types, weed species, growth stages, and sensing conditions. Experimental results across four benchmark datasets demonstrate the effectiveness of our framework, with VL-WS achieving a mean Dice score of 91.64% and outperforming the CNN baseline by 4.98%. The largest gains occur on the most challenging weed class, where VL-WS attains 80.45% Dice score compared to 65.03% for the best baseline, representing a 15.42% improvement. VL-WS further maintains stable weed segmentation performance under limited target-domain supervision, indicating improved generalization and data efficiency. These findings highlight the potential of vision-language alignment to enable scalable, label-efficient segmentation models deployable across diverse real-world agricultural domains.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23928", "abs": "https://arxiv.org/abs/2602.23928", "authors": ["Gary Lupyan", "Senyi Yang"], "title": "The Astonishing Ability of Large Language Models to Parse Jabberwockified Language", "comment": "Submitted to the 2026 Annual Meeting of the Cognitive Science Society", "summary": "We show that large language models (LLMs) have an astonishing ability to recover meaning from severely degraded English texts. Texts in which content words have been randomly substituted by nonsense strings, e.g., \"At the ghybe of the swuint, we are haiveed to Wourge Phrear-gwurr, who sproles into an ghitch flount with his crurp\", can be translated to conventional English that is, in many cases, close to the original text, e.g., \"At the start of the story, we meet a man, Chow, who moves into an apartment building with his wife.\" These results show that structural cues (e.g., morphosyntax, closed-class words) constrain lexical meaning to a much larger degree than imagined. Although the abilities of LLMs to make sense of \"Jabberwockified\" English are clearly superhuman, they are highly relevant to understanding linguistic structure and suggest that efficient language processing either in biological or artificial systems likely benefits from very tight integration between syntax, lexical semantics, and general world knowledge.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23974", "abs": "https://arxiv.org/abs/2602.23974", "authors": ["Fan Zhang", "Baoru Huang", "Xin Zhang"], "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\u3002", "motivation": "\u907f\u514d\u5728\u5b9e\u65f6\u4ea4\u4e92\u4e2d\u51fa\u73b0\u4e0d\u5b89\u5168\u548c\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u79ef\u7d2f\u548c\u8fc7\u4f30\u8ba1\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u6765\u91c7\u6837\u53ef\u9760\u7684\u52a8\u4f5c\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u7f6e\u4fe1\u754c\u6765\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u5176\u4ed6\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u6709\u52a9\u4e8e\u51cf\u5c11\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u8fc7\u4f30\u8ba1\uff0c\u5e76\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2602.23614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23614", "abs": "https://arxiv.org/abs/2602.23614", "authors": ["Kejing Yin", "Haizhou Xu", "Wenfang Yao", "Chen Liu", "Zijie Chen", "Yui Haang Cheung", "William K. Cheung", "Jing Qin"], "title": "When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion", "comment": null, "summary": "Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.", "code_url": "https://github.com/jakeykj/CareBench", "code_stars": 1, "code_last_update": "2026-02-11", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u4e3a\u4e86\u56de\u7b54\u5728\u54ea\u4e9b\u60c5\u51b5\u4e0b\u591a\u6a21\u6001\u5b66\u4e60\u6709\u52a9\u4e8e\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\uff0c\u5982\u4f55\u6bd4\u8f83\u4e0d\u540c\u7684\u878d\u5408\u7b56\u7565\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u5b9e\u73b0\u7b97\u6cd5\u516c\u5e73\u6027\u7b49\u56db\u4e2a\u57fa\u672c\u95ee\u9898\u3002", "method": "\u5bf9\u6765\u81eaMIMIC-IV\u548cMIMIC-CXR\u6807\u51c6\u5316\u961f\u5217\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u548c\u80f8\u90e8X\u5149\u7247\uff08CXR\uff09\u4e4b\u95f4\u7684\u591a\u6a21\u6001\u878d\u5408\u8fdb\u884c\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u591a\u6a21\u6001\u878d\u5408\u5728\u6a21\u6001\u5b8c\u6574\u65f6\u63d0\u9ad8\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981EHR\u548cCXR\u4e92\u8865\u4fe1\u606f\u7684\u75be\u75c5\u4e2d\u3002\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u9664\u975e\u6a21\u578b\u660e\u786e\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u4e0d\u5b8c\u6574\u8f93\u5165\uff0c\u5426\u5219\u591a\u6a21\u6001\u4f18\u52bf\u4f1a\u8fc5\u901f\u964d\u4f4e\u3002\u591a\u6a21\u6001\u878d\u5408\u672c\u8eab\u5e76\u4e0d\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u5b50\u7fa4\u4f53\u5dee\u5f02\u4e3b\u8981\u6765\u81ea\u4e0d\u540c\u4eba\u7fa4\u7684\u654f\u611f\u6027\u4e0d\u5747\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5173\u4e8e\u4f55\u65f6\u591a\u6a21\u6001\u5b66\u4e60\u6709\u5e2e\u52a9\u3001\u4f55\u65f6\u5931\u8d25\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u7684\u5b9e\u7528\u6307\u5bfc\uff0c\u4e3a\u5f00\u53d1\u65e2\u6709\u6548\u53c8\u53ef\u9760\u7684\u4e34\u5e8a\u53ef\u90e8\u7f72\u591a\u6a21\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.23678", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23678", "abs": "https://arxiv.org/abs/2602.23678", "authors": ["Dingqi Ye", "Daniel Kiv", "Wei Hu", "Jimeng Shi", "Shaowen Wang"], "title": "Any Model, Any Place, Any Time: Get Remote Sensing Foundation Model Embeddings On Demand", "comment": null, "summary": "The remote sensing community is witnessing a rapid growth of foundation models, which provide powerful embeddings for a wide range of downstream tasks. However, practical adoption and fair comparison remain challenging due to substantial heterogeneity in model release formats, platforms and interfaces, and input data specifications. These inconsistencies significantly increase the cost of obtaining, using, and benchmarking embeddings across models. To address this issue, we propose rs-embed, a Python library that offers a unified, region of interst (ROI) centric interface: with a single line of code, users can retrieve embeddings from any supported model for any location and any time range. The library also provides efficient batch processing to enable large-scale embedding generation and evaluation. The code is available at: https://github.com/cybergis/rs-embed", "code_url": "https://github.com/cybergis/rs-embed", "code_stars": 4, "code_last_update": "2026-02-28", "AI": {"tldr": "rs-embed\u5e93\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\uff0c\u7b80\u5316\u9065\u611f\u6a21\u578b\u5d4c\u5165\u7684\u83b7\u53d6\u548c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u9065\u611f\u793e\u533a\u4e2d\u6a21\u578b\u53d1\u5e03\u683c\u5f0f\u3001\u5e73\u53f0\u548c\u63a5\u53e3\u4ee5\u53ca\u8f93\u5165\u6570\u636e\u89c4\u8303\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u91c7\u7528\u548c\u516c\u5e73\u6bd4\u8f83\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fars-embed\uff0c\u4e00\u4e2aPython\u5e93\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u3001\u4ee5\u533a\u57df\u5174\u8da3\u4e3a\u4e2d\u5fc3\u7684\u63a5\u53e3\u3002", "result": "\u7528\u6237\u53ef\u4ee5\u4e00\u884c\u4ee3\u7801\u68c0\u7d22\u4efb\u4f55\u652f\u6301\u7684\u6a21\u578b\u5728\u4efb\u4f55\u4f4d\u7f6e\u548c\u4efb\u4f55\u65f6\u95f4\u8303\u56f4\u5185\u7684\u5d4c\u5165\u3002\u5e93\u8fd8\u63d0\u4f9b\u9ad8\u6548\u7684\u6279\u91cf\u5904\u7406\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u5d4c\u5165\u751f\u6210\u548c\u8bc4\u4f30\u3002", "conclusion": "rs-embed\u5e93\u6709\u52a9\u4e8e\u964d\u4f4e\u83b7\u53d6\u3001\u4f7f\u7528\u548c\u57fa\u51c6\u6d4b\u8bd5\u5d4c\u5165\u7684\u6210\u672c\uff0c\u4fc3\u8fdb\u9065\u611f\u793e\u533a\u4e2d\u6a21\u578b\u7684\u5e94\u7528\u548c\u6bd4\u8f83\u3002"}}
{"id": "2602.23940", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23940", "abs": "https://arxiv.org/abs/2602.23940", "authors": ["Nischal Karki", "Bipesh Subedi", "Prakash Poudyal", "Rupak Raj Ghimire", "Bal Krishna Bal"], "title": "Benchmarking BERT-based Models for Sentence-level Topic Classification in Nepali Language", "comment": "5 pages, 2 figures. Accepted and presented at the Regional International Conference on Natural Language Processing (RegICON 2025), Gauhati University, Guwahati, India, November 27-29, 2025. To appear in the conference proceedings. Accepted papers list available at: https://www.regicon2025.in/accepted-papers", "summary": "Transformer-based models such as BERT have significantly advanced Natural Language Processing (NLP) across many languages. However, Nepali, a low-resource language written in Devanagari script, remains relatively underexplored. This study benchmarks multilingual, Indic, Hindi, and Nepali BERT variants to evaluate their effectiveness in Nepali topic classification. Ten pre-trained models, including mBERT, XLM-R, MuRIL, DevBERT, HindiBERT, IndicBERT, and NepBERTa, were fine-tuned and tested on the balanced Nepali dataset containing 25,006 sentences across five conceptual domains and the performance was evaluated using accuracy, weighted precision, recall, F1-score, and AUROC metrics. The results reveal that Indic models, particularly MuRIL-large, achieved the highest F1-score of 90.60%, outperforming multilingual and monolingual models. NepBERTa also performed competitively with an F1-score of 88.26%. Overall, these findings establish a robust baseline for future document-level classification and broader Nepali NLP applications.", "AI": {"tldr": "Evaluates BERT variants for Nepali topic classification, with Indic models and NepBERTa showing strong performance.", "motivation": "Nepali, a low-resource language, has been relatively underexplored in NLP.", "method": "Benchmarking multilingual, Indic, Hindi, and Nepali BERT variants.", "result": "Indic models, particularly MuRIL-large, achieved the highest F1-score of 90.60%. NepBERTa also performed competitively with an F1-score of 88.26%.", "conclusion": "Establishes a robust baseline for future document-level classification and broader Nepali NLP applications."}}
{"id": "2602.23697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23697", "abs": "https://arxiv.org/abs/2602.23697", "authors": ["Jiahui Zhan", "Xianbing Sun", "Xiangnan Zhu", "Yikun Ji", "Ruitong Liu", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Source-Aware Object Swapping with Initial Noise Perturbation", "comment": "This paper is accepted by CVPR 2026 Findings", "summary": "Object swapping aims to replace a source object in a scene with a reference object while preserving object fidelity, scene fidelity, and object-scene harmony. Existing methods either require per-object finetuning and slow inference or rely on extra paired data that mostly depict the same object across contexts, forcing models to rely on background cues rather than learning cross-object alignment. We propose SourceSwap, a self-supervised and source-aware framework that learns cross-object alignment. Our key insight is to synthesize high-quality pseudo pairs from any image via a frequency-separated perturbation in the initial-noise space, which alters appearance while preserving pose, coarse shape, and scene layout, requiring no videos, multi-view data, or additional images. We then train a dual U-Net with full-source conditioning and a noise-free reference encoder, enabling direct inter-object alignment, zero-shot inference without per-object finetuning, and lightweight iterative refinement. We further introduce SourceBench, a high-quality benchmark with higher resolution, more categories, and richer interactions. Experiments demonstrate that SourceSwap achieves superior fidelity, stronger scene preservation, and more natural harmony, and it transfers well to edits such as subject-driven refinement and face swapping.", "AI": {"tldr": "SourceSwap\u662f\u4e00\u79cd\u6709\u6548\u7684\u7269\u4f53\u66ff\u6362\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u66ff\u6362\u6548\u679c\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u573a\u666f\u4e2d\u5bf9\u8c61\u7684\u66ff\u6362\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u573a\u666f\u771f\u5b9e\u6027\u548c\u7269\u4f53\u548c\u8c10\u3002", "method": "\u63d0\u51faSourceSwap\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u4f2a\u5bf9\uff0c\u8fdb\u884c\u8de8\u7269\u4f53\u5bf9\u9f50\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u53ccU-Net\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSourceSwap\u5728\u4fdd\u6301\u771f\u5b9e\u6027\u3001\u573a\u666f\u4fdd\u7559\u548c\u81ea\u7136\u548c\u8c10\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u80fd\u591f\u8fc1\u79fb\u5230\u4e3b\u4f53\u9a71\u52a8\u4f18\u5316\u548c\u9762\u90e8\u4ea4\u6362\u7b49\u7f16\u8f91\u4e2d\u3002", "conclusion": "SourceSwap\u662f\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u76d1\u7763\u548c\u6e90\u611f\u77e5\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u7269\u4f53\u66ff\u6362\uff0c\u5177\u6709\u5f88\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.23941", "categories": ["cs.CL", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23941", "abs": "https://arxiv.org/abs/2602.23941", "authors": ["Ludovic Moncla", "Pierre Nugues", "Thierry Joliveau", "Katherine McDonough"], "title": "EDDA-Coordinata: An Annotated Dataset of Historical Geographic Coordinates", "comment": "Accepted at LREC 2026", "summary": "This paper introduces a dataset of enriched geographic coordinates retrieved from Diderot and d'Alembert's eighteenth-century Encyclopedie. Automatically recovering geographic coordinates from historical texts is a complex task, as they are expressed in a variety of ways and with varying levels of precision. To improve retrieval of coordinates from similar digitized early modern texts, we have created a gold standard dataset, trained models, published the resulting inferred and normalized coordinate data, and experimented applying these models to new texts. From 74,000 total articles in each of the digitized versions of the Encyclopedie from ARTFL and ENCCRE, we examined 15,278 geographical entries, manually identifying 4,798 containing coordinates, and 10,480 with descriptive but non-numerical references. Leveraging our gold standard annotations, we trained transformer-based models to retrieve and normalize coordinates. The pipeline presented here combines a classifier to identify coordinate-bearing entries and a second model for retrieval, tested across encoder-decoder and decoder architectures. Cross-validation yielded an 86% EM score. On an out-of-domain eighteenth-century Trevoux dictionary (also in French), our fine-tuned model had a 61% EM score, while for the nineteenth-century, 7th edition of the Encyclopaedia Britannica in English, the EM was 77%. These findings highlight the gold standard dataset's usefulness as training data, and our two-step method's cross-lingual, cross-domain generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5386\u53f2\u6587\u672c\u4e2d\u81ea\u52a8\u6062\u590d\u5730\u7406\u5750\u6807\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u81ea\u52a8\u4ece\u5386\u53f2\u6587\u672c\u4e2d\u6062\u590d\u5730\u7406\u5750\u6807\u662f\u4e00\u9879\u590d\u6742\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ee5\u5404\u79cd\u65b9\u5f0f\u8868\u8fbe\uff0c\u7cbe\u5ea6\u5404\u5f02\u3002", "method": "\u521b\u5efa\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u53d1\u5e03\u63a8\u65ad\u548c\u5f52\u4e00\u5316\u7684\u5750\u6807\u6570\u636e\uff0c\u5bf9\u65b0\u6587\u672c\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728ARTFL\u548cENCCRE\u6570\u5b57\u5316\u7248\u672c\u7684Encyclopedie\u4e2d\uff0c\u5bf915,278\u4e2a\u5730\u7406\u6761\u76ee\u8fdb\u884c\u4e86\u68c0\u67e5\uff0c\u5176\u4e2d4,798\u4e2a\u5305\u542b\u5750\u6807\uff0c10,480\u4e2a\u5177\u6709\u63cf\u8ff0\u6027\u4f46\u975e\u6570\u503c\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u5177\u6709\u5b9e\u7528\u6027\uff0c\u6211\u4eec\u7684\u4e24\u6b65\u65b9\u6cd5\u5177\u6709\u8de8\u8bed\u8a00\u3001\u8de8\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.24055", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Ta\u00efk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23633", "abs": "https://arxiv.org/abs/2602.23633", "authors": ["Yubo Zhou", "Luo Luo", "Guang Dai", "Haishan Ye"], "title": "On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation", "comment": null, "summary": "Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $\u03ba$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $\u03b5$-stationary point with an oracle complexity of $\\mathcal{O}(\u03ba^7 \u03b5^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\\mathcal{O}(\u03b5^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $\u03ba$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23699", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23699", "abs": "https://arxiv.org/abs/2602.23699", "authors": ["Hao Wu", "Yingqi Fan", "Jinyang Dai", "Junlong Tong", "Yunpu Ma", "Xiaoyu Shen"], "title": "HiDrop: Hierarchical Vision Token Reduction in MLLMs via Late Injection, Concave Pyramid Pruning, and Early Exit", "comment": "Accepted to ICLR 2026", "summary": "The quadratic computational cost of processing vision tokens in Multimodal Large Language Models (MLLMs) hinders their widespread adoption. While progressive vision token pruning offers a promising solution, current methods misinterpret shallow layer functions and use rigid schedules, which fail to unlock the full efficiency potential. To address these issues, we propose HiDrop, a framework that aligns token pruning with the true hierarchical function of MLLM layers. HiDrop features two key innovations: (1) Late Injection, which bypasses passive shallow layers to introduce visual tokens exactly where active fusion begins; and (2) Concave Pyramid Pruning with an Early Exit mechanism to dynamically adjust pruning rates across middle and deep layers. This process is optimized via an inter-layer similarity measure and a differentiable top-k operator. To ensure practical efficiency, HiDrop further incorporates persistent positional encoding, FlashAttention-compatible token selection, and parallel decoupling of vision computation to eliminate hidden overhead associated with dynamic token reduction. Extensive experiments show that HiDrop compresses about 90% visual tokens while matching the original performance and accelerating training by 1.72 times. Our work not only sets a new state-of-the-art for efficient MLLM training and inference but also provides valuable insights into the hierarchical nature of multimodal fusion. The code is released at https://github.com/EIT-NLP/HiDrop.", "code_url": "https://github.com/EIT-NLP/HiDrop", "code_stars": 0, "code_last_update": "2026-02-26", "AI": {"tldr": "HiDrop\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5904\u7406\uff0c\u63d0\u9ad8MLLM\u6548\u7387\u3002", "motivation": "\u5904\u7406\u89c6\u89c9\u6807\u8bb0\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faHiDrop\u6846\u67b6\uff0c\u901a\u8fc7\u665a\u671f\u6ce8\u5165\u548c\u51f9\u9762\u91d1\u5b57\u5854\u526a\u679d\u7b49\u521b\u65b0\u65b9\u6cd5\u6765\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5904\u7406\u3002", "result": "HiDrop\u538b\u7f29\u4e86\u7ea690%\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u540c\u65f6\u5339\u914d\u539f\u59cb\u6027\u80fd\u5e76\u52a0\u901f\u8bad\u7ec31.72\u500d\u3002", "conclusion": "HiDrop\u5728\u9ad8\u6548MLLM\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u65b0\u6c34\u5e73\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u878d\u5408\u7684\u5c42\u6b21\u7ed3\u6784\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2602.23944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23944", "abs": "https://arxiv.org/abs/2602.23944", "authors": ["Peng Liu", "Zhen Tao", "Jihao Zhao", "Ding Chen", "Yansong Zhang", "Cuiping Li", "Zhiyu Li", "Hong Chen"], "title": "MemEmo: Evaluating Emotion in Memory Systems of Agents", "comment": null, "summary": "Memory systems address the challenge of context loss in Large Language Model during prolonged interactions. However, compared to human cognition, the efficacy of these systems in processing emotion-related information remains inconclusive. To address this gap, we propose an emotion-enhanced memory evaluation benchmark to assess the performance of mainstream and state-of-the-art memory systems in handling affective information. We developed the \\textbf{H}uman-\\textbf{L}ike \\textbf{M}emory \\textbf{E}motion (\\textbf{HLME}) dataset, which evaluates memory systems across three dimensions: emotional information extraction, emotional memory updating, and emotional memory question answering. Experimental results indicate that none of the evaluated systems achieve robust performance across all three tasks. Our findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization.", "AI": {"tldr": "Proposed a benchmark for evaluating memory systems in handling emotional information; found deficiencies and suggested future research.", "motivation": "Memory systems face the challenge of context loss in Large Language Model during prolonged interactions.", "method": "Proposed an emotion-enhanced memory evaluation benchmark and developed the HLME dataset.", "result": "None of the evaluated systems achieve robust performance across all three tasks.", "conclusion": "Findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization."}}
{"id": "2602.24080", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.", "AI": {"tldr": "\u9996\u6b21\u5bf9S2S\u7cfb\u7edf\u8fdb\u884c\u56fe\u7075\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u4eba\u6027\u5316\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u4ee3\u8bed\u97f3\u5230\u8bed\u97f3\uff08S2S\uff09\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u5bf9\u8bdd\u7684\u95ee\u9898", "method": "\u8fdb\u884c\u9996\u6b21S2S\u7cfb\u7edf\u7684\u56fe\u7075\u6d4b\u8bd5\uff0c\u6536\u96c6\u4e862,968\u540d\u4eba\u7c7b\u5bf99\u4e2a\u6700\u5148\u8fdb\u7684S2S\u7cfb\u7edf\u548c28\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u5bf9\u8bdd\u7684\u5224\u65ad", "result": "\u6ca1\u6709\u73b0\u6709\u7684S2S\u7cfb\u7edf\u901a\u8fc7\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5728\u4eba\u6027\u5316\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1b\u5206\u6790\u8868\u660e\u74f6\u9888\u4e0d\u662f\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u662f\u6765\u81ea\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5229\u7528\u7ec6\u7c92\u5ea6\u7684\u4eba\u6027\u5316\u8bc4\u5206\uff0c\u63d0\u4f9b\u51c6\u786e\u548c\u900f\u660e\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u533a\u5206\uff0c\u4e3a\u81ea\u52a8\u4eba\u6027\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177", "conclusion": "\u5efa\u7acb\u4e86S2S\u7cfb\u7edf\u7684\u9996\u6b21\u4eba\u6027\u5316\u8bc4\u4f30\uff0c\u8d85\u8d8a\u4e86\u4e8c\u5143\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u8be6\u7ec6\u7684\u8bca\u65ad\u6d1e\u5bdf\uff0c\u4e3a\u5bf9\u8bdd\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7c7b\u4eba\u6539\u8fdb\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2602.23636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23636", "abs": "https://arxiv.org/abs/2602.23636", "authors": ["Zhihao Ding", "Jinming Li", "Ze Lu", "Jieming Shi"], "title": "FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation", "comment": null, "summary": "Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.", "AI": {"tldr": "FlexGuard\uff1a\u4e00\u4e2a\u81ea\u9002\u5e94\u4e25\u683c\u5ea6\u7684LLM\u5ba1\u6838\u5668\uff0c\u63d0\u9ad8\u4e86\u5ba1\u6838\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u786e\u4fddLLM\u751f\u6210\u5185\u5bb9\u7684\u5b89", "method": "\u63d0\u51faFlexBench\u548cFlexGuard\u6a21\u578b", "result": "FlexGuard\u5728FlexBench\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5ba1\u6838\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "FlexGuard\u662f\u4e00\u4e2a\u9c81\u68d2\u4e14\u51c6\u786e\u7684LLM\u5ba1\u6838\u5668"}}
{"id": "2602.23709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23709", "abs": "https://arxiv.org/abs/2602.23709", "authors": ["Shitong Sun", "Ke Han", "Yukai Huang", "Weitong Cai", "Jifei Song"], "title": "EgoGraph: Temporal Knowledge Graph for Egocentric Video Understanding", "comment": "Under review", "summary": "Ultra-long egocentric videos spanning multiple days present significant challenges for video understanding. Existing approaches still rely on fragmented local processing and limited temporal modeling, restricting their ability to reason over such extended sequences. To address these limitations, we introduce EgoGraph, a training-free and dynamic knowledge-graph construction framework that explicitly encodes long-term, cross-entity dependencies in egocentric video streams. EgoGraph employs a novel egocentric schema that unifies the extraction and abstraction of core entities, such as people, objects, locations, and events, and structurally reasons about their attributes and interactions, yielding a significantly richer and more coherent semantic representation than traditional clip-based video models. Crucially, we develop a temporal relational modeling strategy that captures temporal dependencies across entities and accumulates stable long-term memory over multiple days, enabling complex temporal reasoning. Extensive experiments on the EgoLifeQA and EgoR1-bench benchmarks demonstrate that EgoGraph achieves state-of-the-art performance on long-term video question answering, validating its effectiveness as a new paradigm for ultra-long egocentric video understanding.", "AI": {"tldr": "EgoGraph provides a new approach for understanding ultra-long egocentric videos by encoding long-term dependencies and achieving superior performance in long-term video question answering.", "motivation": "Existing approaches in video understanding for ultra-long egocentric videos have limitations in local processing and temporal modeling.", "method": "Introducing EgoGraph, a training-free and dynamic knowledge-graph construction framework for encoding long-term dependencies. It employs a novel egocentric schema and a temporal relational modeling strategy.", "result": "EgoGraph achieves state-of-the-art performance on long-term video question answering, validating its effectiveness.", "conclusion": "EgoGraph is an effective new paradigm for ultra-long egocentric video understanding."}}
{"id": "2602.23993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23993", "abs": "https://arxiv.org/abs/2602.23993", "authors": ["Jonathan Drechsel", "Steffen Herbold"], "title": "The GRADIEND Python Package: An End-to-End System for Gradient-Based Feature Learning", "comment": null, "summary": "We present gradiend, an open-source Python package that operationalizes the GRADIEND method for learning feature directions from factual-counterfactual MLM and CLM gradients in language models. The package provides a unified workflow for feature-related data creation, training, evaluation, visualization, persistent model rewriting via controlled weight updates, and multi-feature comparison. We demonstrate GRADIEND on an English pronoun paradigm and on a large-scale feature comparison that reproduces prior use cases.", "AI": {"tldr": "GRADIEND\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u7528\u4e8e\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\u7279\u5f81\u65b9\u5411\uff0c\u652f\u6301\u5168\u9762\u5de5\u4f5c\u6d41\u7a0b\u548c\u7279\u5f81\u6bd4\u8f83", "motivation": "\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u65b9\u5411", "method": "GRADIEND\u65b9\u6cd5", "result": "\u5f00\u6e90Python\u5305gradiend\uff0c\u63d0\u4f9b\u7edf\u4e00\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u7279\u5f81\u76f8\u5173\u6570\u636e\u521b\u5efa\u3001\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3001\u6a21\u578b\u91cd\u5199\u548c\u7279\u5f81\u6bd4\u8f83", "conclusion": "GRADIEND\u5728\u82f1\u8bed\u4ee3\u8bcd\u8303\u5f0f\u548c\u5927\u89c4\u6a21\u7279\u5f81\u6bd4\u8f83\u4e2d\u8868\u73b0\u826f\u597d"}}
{"id": "2602.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23638", "abs": "https://arxiv.org/abs/2602.23638", "authors": ["Haoran Zhang", "Dongjun Kim", "Seohyeon Cha", "Haris Vikalo"], "title": "FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA", "comment": "preprint", "summary": "Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.", "AI": {"tldr": "FedRot-LoRA\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u8054\u90a6LoRA\u8bad\u7ec3\u4e2d\u7684\u805a\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Federated LoRA\u5728\u5206\u5e03\u5f0f\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u56e0\u5b50\u5e73\u5747\u4e0e\u5c40\u90e8\u66f4\u65b0\u7684\u6b63\u786e\u805a\u5408\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u805a\u5408\u8bef\u5dee\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faFedRot-LoRA\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\uff0c\u4ee5\u51cf\u5c11\u805a\u5408\u8bef\u5dee\u548c\u8de8\u5ba2\u6237\u7aef\u5b50\u7a7a\u95f4\u4e0d\u5339\u914d\u3002", "result": "FedRot-LoRA\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6LoRA\u57fa\u7ebf\uff0c\u4e14\u5728\u4e0d\u540c\u5f02\u6784\u6027\u548cLoRA\u79e9\u6c34\u5e73\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "FedRot-LoRA\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\uff0c\u63d0\u9ad8\u4e86\u8054\u90a6LoRA\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.23711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23711", "abs": "https://arxiv.org/abs/2602.23711", "authors": ["Hongbo Jiang", "Jie Li", "Yunhang Shen", "Pingyang Dai", "Xing Sun", "Haoyu Cao", "Liujuan Cao"], "title": "Can Unified Generation and Understanding Models Maintain Semantic Equivalence Across Different Output Modalities?", "comment": "Equal contribution by Jie Li", "summary": "Unified Multimodal Large Language Models (U-MLLMs) integrate understanding and generation within a single architecture. However, existing evaluations typically assess these capabilities separately, overlooking semantic equivalence, i.e., the ability to manifest consistent reasoning results regardless of the output modality. In this work, we investigate whether current U-MLLMs satisfy this premise. We observe that while models demonstrate robust textual reasoning, they fail to maintain semantic equivalence when required to render the same results in the image modality. To rigorously diagnose this discrepancy, we introduce VGUBench, a framework to decouple reasoning logic from generation fidelity. VGUBench comprises three diagnostic tasks: (1)Textual Generative Understanding, establishing a baseline for reasoning accuracy in textual response; (2)Visual Generative Understanding, evaluating the ability to generate visual responses that represent the correct answer; and (3)a Visual Rendering control task, which assesses the ability to directly render explicit visual descriptions into images without complex reasoning. Our evaluation reveals a significant disparity: despite strong performance in textual understanding and visual rendering, U-MLLMs exhibit a marked performance collapse when required to generate visual answers to questions. Furthermore, we find a negligible correlation between visual answering performance and basic rendering quality. These results suggest that the failure stems not from insufficient generation fidelity, but from a breakdown in cross-modal semantic alignment. We provide diagnostic insights to address this challenge in future Unified Generation and Understanding Models.", "AI": {"tldr": "U-MLLMs\u5728\u751f\u6210\u89c6\u89c9\u7b54\u6848\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u7684\u5931\u8d25\u3002", "motivation": "\u8bc4\u4f30U-MLLMs\u5728\u8bed\u4e49\u7b49\u4ef7\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5373\u5728\u4e0d\u540c\u8f93\u51fa\u6a21\u5f0f\u4e0b\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u63a8\u7406\u7ed3\u679c\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165VGUBench\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u8bca\u65ad\u4efb\u52a1\uff1a\u6587\u672c\u751f\u6210\u7406\u89e3\u3001\u89c6\u89c9\u751f\u6210\u7406\u89e3\u548c\u89c6\u89c9\u6e32\u67d3\u63a7\u5236\u4efb\u52a1\u3002", "result": "U-MLLMs\u5728\u6587\u672c\u7406\u89e3\u548c\u89c6\u89c9\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u89c6\u89c9\u7b54\u6848\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u89c6\u89c9\u56de\u7b54\u6027\u80fd\u4e0e\u57fa\u672c\u6e32\u67d3\u8d28\u91cf\u4e4b\u95f4\u76f8\u5173\u6027\u6781\u4f4e\u3002", "conclusion": "U-MLLMs\u5728\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u751f\u6210\u89c6\u89c9\u7b54\u6848\u65f6\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2602.24002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24002", "abs": "https://arxiv.org/abs/2602.24002", "authors": ["Iris Dania Jimenez", "Christoph Kern"], "title": "Dialect and Gender Bias in YouTube's Spanish Captioning System", "comment": "21 pages, 4 tables", "summary": "Spanish is the official language of twenty-one countries and is spoken by over 441 million people. Naturally, there are many variations in how Spanish is spoken across these countries. Media platforms such as YouTube rely on automatic speech recognition systems to make their content accessible to different groups of users. However, YouTube offers only one option for automatically generating captions in Spanish. This raises the question: could this captioning system be biased against certain Spanish dialects? This study examines the potential biases in YouTube's automatic captioning system by analyzing its performance across various Spanish dialects. By comparing the quality of captions for female and male speakers from different regions, we identify systematic disparities which can be attributed to specific dialects. Our study provides further evidence that algorithmic technologies deployed on digital platforms need to be calibrated to the diverse needs and experiences of their user populations.", "AI": {"tldr": "\u7814\u7a76\u6307\u51faYouTube\u7684\u897f\u73ed\u7259\u8bed\u5b57\u5e55\u7cfb\u7edf\u53ef\u80fd\u5b58\u5728\u504f\u89c1\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u65b9\u8a00\u8fdb\u884c\u6821\u51c6\u3002", "motivation": "YouTube\u4ec5\u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u897f\u73ed\u7259\u8bed\u5b57\u5e55\u7684\u9009\u9879\uff0c\u5f15\u53d1\u4e86\u5bf9\u8be5\u5b57\u5e55\u7cfb\u7edf\u662f\u5426\u5bf9\u67d0\u4e9b\u897f\u73ed\u7259\u65b9\u8a00\u5b58\u5728\u504f\u89c1\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790YouTube\u81ea\u52a8\u5b57\u5e55\u7cfb\u7edf\u5728\u4e0d\u540c\u897f\u73ed\u7259\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u7814\u7a76\u4e86\u6f5c\u5728\u7684\u504f\u89c1\u3002\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u5730\u533a\u5973\u6027\u548c\u7537\u6027\u8bf4\u8bdd\u8005\u7684\u5b57\u5e55\u8d28\u91cf\uff0c\u786e\u5b9a\u4e86\u53ef\u5f52\u56e0\u4e8e\u7279\u5b9a\u65b9\u8a00\u7684\u7cfb\u7edf\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u8bc1\u636e\uff0c\u8868\u660e\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u7b97\u6cd5\u6280\u672f\u9700\u8981\u9488\u5bf9\u7528\u6237\u7fa4\u4f53\u7684\u591a\u6837\u6027\u548c\u9700\u6c42\u8fdb\u884c\u8c03\u6574\u3002", "conclusion": "YouTube\u7684\u81ea\u52a8\u5b57\u5e55\u7cfb\u7edf\u53ef\u80fd\u5b58\u5728\u9488\u5bf9\u67d0\u4e9b\u897f\u73ed\u7259\u65b9\u8a00\u7684\u504f\u89c1\uff0c\u8fd9\u8868\u660e\u7b97\u6cd5\u6280\u672f\u5728\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u9700\u8981\u8003\u8651\u5230\u7528\u6237\u7fa4\u4f53\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2602.24100", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24100", "abs": "https://arxiv.org/abs/2602.24100", "authors": ["Richard Csaky"], "title": "Artificial Agency Program: Curiosity, compression, and communication in agents", "comment": "This is a working draft. Feedback and criticism is most welcome", "summary": "This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u5176\u4f5c\u4e3a\u6269\u5c55\u7684\u4eba\u7c7b-\u5de5\u5177\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u3002", "motivation": "\u63d0\u51fa\u5c06\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6784\u5efa\u4e3a\u73b0\u5b9e\u5d4c\u5165\u3001\u8d44\u6e90\u53d7\u9650\u7684\u667a\u80fd\u4f53\uff0c\u5176\u53d1\u5c55\u7531\u597d\u5947\u5fc3\u9a71\u52a8\uff0c\u5728\u7269\u7406\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u8fdb\u884c\u5b66\u4e60\u3002", "method": "\u5c06\u9884\u6d4b\u538b\u7f29\u3001\u5185\u5728\u52a8\u673a\u3001\u8d4b\u6743\u4e0e\u63a7\u5236\u3001\u754c\u9762\u8d28\u91cf\uff08\u7edf\u4e00\uff09\u548c\u8bed\u8a00/\u81ea\u6211\u901a\u4fe1\u4f5c\u4e3a\u9009\u62e9\u6027\u7684\u4fe1\u606f\u74f6\u9888\uff0c\u6784\u5efa\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u7a0b\u5e8f\uff0c\u5305\u62ec\u660e\u786e\u7684\u6210\u672c\u3001\u5206\u9636\u6bb5\u7684\u5b9e\u9a8c\u548c\u4e00\u4e2a\u5177\u4f53\u7684\u8de8\u6a21\u6001\u6807\u8bb0\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u63d0\u4f9b\u4e00\u4e2a\u6982\u5ff5\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u5c06\u5185\u5728\u52a8\u673a\u3001\u4fe1\u606f\u7406\u8bba\u3001\u70ed\u529b\u5b66\u3001\u6709\u9650\u7406\u6027\u548c\u73b0\u4ee3\u63a8\u7406\u7cfb\u7edf\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "AI\u4f5c\u4e3a\u6269\u5c55\u7684\u4eba\u7c7b-\u5de5\u5177\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\uff0c\u53ef\u4ee5\u589e\u52a0\u611f\u77e5\u3001\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4eba\u4e0e\u4eba\u3001\u5de5\u5177\u548c\u73af\u5883\u4e4b\u95f4\u7684\u6469\u64e6\u3002"}}
{"id": "2602.23662", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23662", "abs": "https://arxiv.org/abs/2602.23662", "authors": ["Kohei Obata", "Zheng Chen", "Yasuko Matsubara", "Lingwei Zhu", "Yasushi Sakurai"], "title": "Selective Denoising Diffusion Model for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684TSAD\u65b9\u6cd5AnomalyFilter\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u9664\u5f02\u5e38\u90e8\u5206\u7684\u540c\u65f6\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\uff0c\u901a\u8fc7\u63a9\u7801\u566a\u58f0\u548c\u53bb\u566a\u8fc7\u7a0b\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff08TSAD\uff09\u662f\u51e0\u5341\u5e74\u6765\u7814\u7a76\u7684\u91cd\u8981\u9886\u57df\uff0c\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u662f\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u5e76\u53d6\u5f97\u4e86\u6210\u529f\u3002\u6269\u6563\u6a21\u578b\u6700\u8fd1\u56e0\u5176\u5148\u8fdb\u7684\u751f\u6210\u80fd\u529b\u800c\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u6ce8\u610f\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684TSAD\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6761\u4ef6\u7b56\u7565\uff0c\u5373\u901a\u8fc7\u6761\u4ef6\u5668\u7684\u5e2e\u52a9\u4ece\u767d\u566a\u58f0\u4e2d\u91cd\u5efa\u8f93\u5165\u5b9e\u4f8b\u3002\u7136\u800c\uff0c\u8fd9\u7ed9\u51c6\u786e\u91cd\u5efa\u6b63\u5e38\u90e8\u5206\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aAnomalyFilter\uff0c\u5b83\u4f5c\u4e3a\u4e00\u79cd\u9009\u62e9\u6027\u8fc7\u6ee4\u5668\uff0c\u4ec5\u5728\u5b9e\u4f8b\u4e2d\u53bb\u9664\u5f02\u5e38\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\u3002\u4e3a\u4e86\u6784\u5efa\u8fd9\u6837\u7684\u8fc7\u6ee4\u5668\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9ad8\u65af\u566a\u58f0\u8fdb\u884c\u63a9\u7801\uff0c\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4e0d\u5bf9\u5b9e\u4f8b\u6dfb\u52a0\u566a\u58f0\u3002\u8fd9\u4e24\u4e2a\u7b80\u5355\u7ec4\u4ef6\u7684\u7ed3\u5408\u5927\u5927\u63d0\u9ad8\u4e86\u6734\u7d20\u6269\u6563\u6a21\u578b\u7684\u8868\u73b0\u3002\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAnomalyFilter\u5728\u6b63\u5e38\u90e8\u5206\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u4f4e\u91cd\u5efa\u8bef\u5dee\uff0c\u4e3a\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u652f\u6301\u3002AnomalyFilter\u4ee3\u8868\u4e86\u4e00\u79cd\u5f00\u521b\u6027\u7684\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4e3aTSAD\u4e13\u95e8\u8bbe\u8ba1\u7684\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnomalyFilter\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u9009\u62e9\u6027\u8fc7\u6ee4\u5668\uff0c\u4ec5\u5728\u5b9e\u4f8b\u4e2d\u53bb\u9664\u5f02\u5e38\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9ad8\u65af\u566a\u58f0\u8fdb\u884c\u63a9\u7801\uff0c\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4e0d\u5bf9\u5b9e\u4f8b\u6dfb\u52a0\u566a\u58f0\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAnomalyFilter\u5728\u6b63\u5e38\u90e8\u5206\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u4f4e\u91cd\u5efa\u8bef\u5dee\uff0c\u4e3a\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u652f\u6301\u3002", "conclusion": "AnomalyFilter\u4ee3\u8868\u4e86\u4e00\u79cd\u5f00\u521b\u6027\u7684\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4e3aTSAD\u4e13\u95e8\u8bbe\u8ba1\u7684\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u8bbe\u8ba1\u3002"}}
{"id": "2602.23732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23732", "abs": "https://arxiv.org/abs/2602.23732", "authors": ["Xinyi Qi", "Kai Ye", "Chengchun Shi", "Ying Yang", "Hongyi Zhou", "Jin Zhu"], "title": "A Difference-in-Difference Approach to Detecting AI-Generated Images", "comment": null, "summary": "Diffusion models are able to produce AI-generated images that are almost indistinguishable from real ones. This raises concerns about their potential misuse and poses substantial challenges for detecting them. Many existing detectors rely on reconstruction error -- the difference between the input image and its reconstructed version -- as the basis for distinguishing real from fake images. However, these detectors become less effective as modern AI-generated images become increasingly similar to real ones. To address this challenge, we propose a novel difference-in-difference method. Instead of directly using the reconstruction error (a first-order difference), we compute the difference in reconstruction error -- a second-order difference -- for variance reduction and improving detection accuracy. Extensive experiments demonstrate that our method achieves strong generalization performance, enabling reliable detection of AI-generated images in the era of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5dee\u5f02-\u5dee\u5f02\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u91cd\u5efa\u8bef\u5dee\u7684\u5dee\u5f02\u6765\u63d0\u9ad8AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u548cAI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5dee\u5f02-\u5dee\u5f02\u65b9\u6cd5\u3002", "method": "\u8ba1\u7b97\u91cd\u5efa\u8bef\u5dee\u7684\u5dee\u5f02\uff08\u4e8c\u9636\u5dee\u5f02\uff09\u4ee5\u51cf\u5c11\u65b9\u5dee\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u80fd\u591f\u5728\u751f\u6210AI\u65f6\u4ee3\u53ef\u9760\u5730\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5e94\u5bf9\u751f\u6210AI\u65f6\u4ee3\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.24060", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24060", "abs": "https://arxiv.org/abs/2602.24060", "authors": ["Donghao Huang", "Zhaoxia Wang"], "title": "Task Complexity Matters: An Empirical Study of Reasoning in LLMs for Sentiment Analysis", "comment": "12 pages, 1 figure, 3 tables. Accepted at PAKDD 2026", "summary": "Large language models (LLMs) with reasoning capabilities have fueled a compelling narrative that reasoning universally improves performance across language tasks. We test this claim through a comprehensive evaluation of 504 configurations across seven model families--including adaptive, conditional, and reinforcement learning-based reasoning architectures--on sentiment analysis datasets of varying granularity (binary, five-class, and 27-class emotion). Our findings reveal that reasoning effectiveness is strongly task-dependent, challenging prevailing assumptions: (1) Reasoning shows task-complexity dependence--binary classification degrades up to -19.9 F1 percentage points (pp), while 27-class emotion recognition gains up to +16.0pp; (2) Distilled reasoning variants underperform base models by 3-18 pp on simpler tasks, though few-shot prompting enables partial recovery; (3) Few-shot learning improves over zero-shot in most cases regardless of model type, with gains varying by architecture and task complexity; (4) Pareto frontier analysis shows base models dominate efficiency-performance trade-offs, with reasoning justified only for complex emotion recognition despite 2.1x-54x computational overhead. We complement these quantitative findings with qualitative error analysis revealing that reasoning degrades simpler tasks through systematic over-deliberation, offering mechanistic insight beyond the high-level overthinking hypothesis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24110", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24110", "abs": "https://arxiv.org/abs/2602.24110", "authors": ["Yanwei Ren", "Haotian Zhang", "Likang Xiao", "Xikai Zhang", "Jiaxing Huang", "Jiayan Qiu", "Baosheng Yu", "Quan Chen", "Liu Liu"], "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.", "AI": {"tldr": "SCOPE \u6846\u67b6\u901a\u8fc7\u7cbe\u786e\u4fee\u590d\u63d0\u5347\u4e86 RLVR \u6027\u80fd\uff0c\u62d3\u5bbd\u4e86\u63a2\u7d22\u7a7a\u95f4\u3002", "motivation": "Reinforcement Learning from Verifiable Rewards (RLVR) \u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u6807\u51c6\u7ed3\u679c\u76d1\u7763\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u63d0\u51fa SCOPE \u6846\u67b6\uff0c\u5229\u7528 Process Reward Models \u7cbe\u786e\u4fee\u590d\u4e0d\u7406\u60f3\u8fd0\u884c\u8f68\u8ff9\u4e2d\u7684\u7b2c\u4e00\u6b65\u9519\u8bef\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5e73\u5747\u51c6\u786e\u7387 46.6%\uff0c\u5728\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230 53.4% \u7684\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u5f97\u5206\u3002", "conclusion": "SCOPE \u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86 RLVR \u7684\u6027\u80fd\uff0c\u62d3\u5bbd\u4e86\u63a2\u7d22\u7a7a\u95f4\u3002"}}
{"id": "2602.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23663", "abs": "https://arxiv.org/abs/2602.23663", "authors": ["Kohei Obata", "Taichi Murayama", "Zheng Chen", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning", "comment": null, "summary": "Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST.", "code_url": "https://github.com/KoheiObata/MoST", "code_stars": 1, "code_last_update": "2025-04-02", "AI": {"tldr": "MoST\u662f\u4e00\u79cd\u65b0\u7684TTS\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b66\u4e60\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff08TTS\uff09\u7684\u8868\u793a\u5bf9\u4e8e\u5404\u79cd\u5e94\u7528\u6709\u76ca\uff0c\u4f46\u4e5f\u5f88\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5f20\u91cf\u672c\u8eab\u7684\u590d\u6742\u6027\u963b\u788d\u4e86\u4e30\u5bcc\u8868\u793a\u7684\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoST\u7684\u65b0\u9896\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5f20\u91cf\u5207\u7247\u65b9\u6cd5\u6765\u964d\u4f4eTTS\u7ed3\u6784\u7684\u590d\u6742\u6027\uff0c\u5e76\u5b66\u4e60\u53ef\u4ee5\u5206\u89e3\u4e3a\u5355\u4e2a\u975e\u65f6\u95f4\u6a21\u5f0f\u7684\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoST\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MoST\u662f\u4e00\u79cd\u6709\u6548\u7684TTS\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23734", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23734", "abs": "https://arxiv.org/abs/2602.23734", "authors": ["Hao Wu", "Xudong Wang", "Jialiang Zhang", "Junlong Tong", "Xinghao Chen", "Junyan Lin", "Yunpu Ma", "Xiaoyu Shen"], "title": "UTPTrack: Towards Simple and Unified Token Pruning for Visual Tracking", "comment": "Accepted to CVPR 2026", "summary": "One-stream Transformer-based trackers achieve advanced performance in visual object tracking but suffer from significant computational overhead that hinders real-time deployment. While token pruning offers a path to efficiency, existing methods are fragmented. They typically prune the search region, dynamic template, and static template in isolation, overlooking critical inter-component dependencies, which yields suboptimal pruning and degraded accuracy. To address this, we introduce UTPTrack, a simple and Unified Token Pruning framework that, for the first time, jointly compresses all three components. UTPTrack employs an attention-guided, token type-aware strategy to holistically model redundancy, a design that seamlessly supports unified tracking across multimodal and language-guided tasks within a single model. Extensive evaluations on 10 benchmarks demonstrate that UTPTrack achieves a new state-of-the-art in the accuracy-efficiency trade-off for pruning-based trackers, pruning 65.4% of vision tokens in RGB-based tracking and 67.5% in unified tracking while preserving 99.7% and 100.5% of baseline performance, respectively. This strong performance across both RGB and multimodal scenarios underlines its potential as a robust foundation for future research in efficient visual tracking. Code will be released at https://github.com/EIT-NLP/UTPTrack.", "code_url": "https://github.com/EIT-NLP/UTPTrack", "code_stars": 0, "code_last_update": "2026-02-26", "AI": {"tldr": "UTPTrack\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u6d41Transformer\u7684\u89c6\u89c9\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u8054\u5408\u526a\u679d\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5355\u6d41Transformer\u7684\u8ddf\u8e2a\u5668\u5728\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u526a\u679d\u65b9\u9762\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faUTPTrack\uff0c\u4e00\u4e2a\u7b80\u5355\u4e14\u7edf\u4e00\u7684\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u538b\u7f29\u641c\u7d22\u533a\u57df\u3001\u52a8\u6001\u6a21\u677f\u548c\u9759\u6001\u6a21\u677f\u4e09\u4e2a\u7ec4\u4ef6\u3002\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u3001\u4ee4\u724c\u7c7b\u578b\u611f\u77e5\u7684\u7b56\u7565\u6765\u6574\u4f53\u5efa\u6a21\u5197\u4f59\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUTPTrack\u5728\u526a\u679d\u8ddf\u8e2a\u5668\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u65b9\u9762\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728RGB\u8ddf\u8e2a\u4e2d\u526a\u679d\u4e8665.4%\u7684\u89c6\u89c9\u4ee4\u724c\uff0c\u5728\u7edf\u4e00\u8ddf\u8e2a\u4e2d\u526a\u679d\u4e8667.5%\uff0c\u540c\u65f6\u5206\u522b\u4fdd\u7559\u4e8699.7%\u548c100.5%\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "UTPTrack\u5728RGB\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u9ad8\u6548\u89c6\u89c9\u8ddf\u8e2a\u7814\u7a76\u7684\u57fa\u7840\u3002"}}
{"id": "2602.24082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24082", "abs": "https://arxiv.org/abs/2602.24082", "authors": ["Jaekyung Cho"], "title": "Preference Packing: Efficient Preference Optimization for Large Language Models", "comment": null, "summary": "Resource-efficient training optimization techniques are becoming increasingly important as the size of large language models (LLMs) continues to grow. In particular, batch packing is commonly used in pre-training and supervised fine-tuning to achieve resource-efficient training. We propose preference packing, a method to enhance resource efficiency in training techniques that use data with different responses for the same input prompt, such as reward models or Direct Preference Optimization (DPO). Preference packing improves resource efficiency by reducing the attention operations for duplicate input prompts and decreasing KV cache memory usage. We conducted experiments on text-only datasets and image-included datasets and achieved at least 37% reduction in training time. Notably, this method can be applied alongside existing optimization techniques such as batch sorting, resulting in a 3.22x speedup.", "AI": {"tldr": "\u63d0\u51fa\u504f\u597d\u6253\u5305\u65b9\u6cd5\uff0c\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\uff0c\u5b9e\u73b0\u81f3\u5c1137%\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u53ef\u52a0\u901f3.22\u500d\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0c\u8d44\u6e90\u9ad8\u6548\u7684\u8bad\u7ec3\u4f18\u5316\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u504f\u597d\u6253\u5305\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u4f7f\u7528\u5177\u6709\u4e0d\u540c\u54cd\u5e94\u7684\u6570\u636e\uff08\u5982\u5956\u52b1\u6a21\u578b\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff09\u8fdb\u884c\u8bad\u7ec3\u7684\u6280\u672f\u4e2d\u7684\u8d44\u6e90\u6548\u7387\u3002", "result": "\u5728\u6587\u672c\u6570\u636e\u96c6\u548c\u5305\u542b\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u81f3\u5c1137%\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u4f18\u5316\u6280\u672f\uff08\u5982\u6279\u91cf\u6392\u5e8f\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u4ece\u800c\u5b9e\u73b03.22\u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2602.24173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24173", "abs": "https://arxiv.org/abs/2602.24173", "authors": ["Antoine Peyronnet", "Fabian Gloeckle", "Amaury Hayat"], "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics", "comment": "15 pages, 3 figures, 5 Tables", "summary": "We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684LLM\u6570\u5b66\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46LLM\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7814\u7a76\u7ea7\u6570\u5b66\u65b9\u9762\u7684\u80fd\u529b", "method": "\u5efa\u7acb\u81ea\u52a8\u7ba1\u9053\uff0c\u4ecearXiv\u4e2d\u63d0\u53d6\u5f15\u7406\u5e76\u91cd\u5199\u4e3a\u72ec\u7acb\u9648\u8ff0", "result": "LLM\u5728\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u51c6\u786e\u7387\u7ea6\u4e3a10-15%\uff0c\u8868\u660eLLM\u5728\u7814\u7a76\u73af\u5883\u4e2d\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u8bc1\u660e\u80fd\u529b\u4ecd\u6709\u8f83\u5927\u8fdb\u6b65\u7a7a\u95f4", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30LLM\u5728\u6570\u5b66\u7814\u7a76\u65b9\u9762\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f46LLM\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u9ad8"}}
{"id": "2602.23696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23696", "abs": "https://arxiv.org/abs/2602.23696", "authors": ["Yongzhong Xu"], "title": "Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training", "comment": "18 pages, 4 figures", "summary": "We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24109", "abs": "https://arxiv.org/abs/2602.24109", "authors": ["Sara Nabhani", "Federico Pianzola", "Khalid Al-Khatib", "Malvina Nissim"], "title": "ARGUS: Seeing the Influence of Narrative Features on Persuasion in Argumentative Texts", "comment": "22 pages, 8 figures, submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Can narratives make arguments more persuasive? And to this end, which narrative features matter most? Although stories are often seen as powerful tools for persuasion, their specific role in online, unstructured argumentation remains underexplored. To address this gap, we present ARGUS, a framework for studying the impact of narration on persuasion in argumentative discourse. ARGUS introduces a new ChangeMyView corpus annotated for story presence and six key narrative features, integrating insights from two established theoretical frameworks that capture both textual narrative features and their effects on recipients. Leveraging both encoder-based classifiers and zero-shot large language models (LLMs), ARGUS identifies stories and narrative features and applies them at scale to examine how different narrative dimensions influence persuasion success in online argumentation.", "AI": {"tldr": "ARGUS\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u6545\u4e8b\u548c\u53d9\u4e8b\u7279\u5f81\uff0c\u6709\u6548\u5730\u7814\u7a76\u4e86\u53d9\u4e8b\u5bf9\u5728\u7ebf\u8bba\u8bc1\u8bf4\u670d\u7684\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u6545\u4e8b\u901a\u5e38\u88ab\u89c6\u4e3a\u8bf4\u670d\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u5b83\u4eec\u5728\u5728\u7ebf\u975e\u7ed3\u6784\u5316\u8bba\u8bc1\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86ARGUS\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u53d9\u8ff0\u5bf9\u8bba\u8bc1\u6027\u8bdd\u8bed\u4e2d\u8bf4\u670d\u5f71\u54cd\u7684\u7814\u7a76\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684ChangeMyView\u8bed\u6599\u5e93\uff0c\u8be5\u8bed\u6599\u5e93\u9488\u5bf9\u6545\u4e8b\u5b58\u5728\u548c\u516d\u4e2a\u5173\u952e\u53d9\u4e8b\u7279\u5f81\u8fdb\u884c\u4e86\u6ce8\u91ca\uff0c\u5e76\u6574\u5408\u4e86\u4e24\u4e2a\u5df2\u5efa\u7acb\u7684\u7406\u8bba\u6846\u67b6\u7684\u89c1\u89e3\uff0c\u8fd9\u4e24\u4e2a\u6846\u67b6\u6355\u6349\u4e86\u6587\u672c\u53d9\u4e8b\u7279\u5f81\u53ca\u5176\u5bf9\u63a5\u53d7\u8005\u7684\u5f71\u54cd\u3002\u5229\u7528\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u5206\u7c7b\u5668\u548c\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0cARGUS\u8bc6\u522b\u6545\u4e8b\u548c\u53d9\u4e8b\u7279\u5f81\uff0c\u5e76\u5927\u89c4\u6a21\u5e94\u7528\u5b83\u4eec\u6765\u68c0\u9a8c\u4e0d\u540c\u7684\u53d9\u4e8b\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u8bf4\u670d\u6210\u529f\u3002", "result": "ARGUS\u6846\u67b6\u53ef\u4ee5\u8bc6\u522b\u6545\u4e8b\u548c\u53d9\u4e8b\u7279\u5f81\uff0c\u5e76\u5927\u89c4\u6a21\u5e94\u7528\u5b83\u4eec\u6765\u68c0\u9a8c\u4e0d\u540c\u7684\u53d9\u4e8b\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u8bf4\u670d\u6210\u529f\u3002", "conclusion": "\u53d9\u4e8b\u5728\u5728\u7ebf\u975e\u7ed3\u6784\u5316\u8bba\u8bc1\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0cARGUS\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u8bc6\u522b\u548c\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u3002"}}
{"id": "2602.23737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23737", "abs": "https://arxiv.org/abs/2602.23737", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Bridging Dynamics Gaps via Diffusion Schr\u00f6dinger Bridge for Cross-Domain Reinforcement Learning", "comment": null, "summary": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schr\u00f6dinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.", "AI": {"tldr": "BDGxRL is a novel framework for cross-domain reinforcement learning that outperforms existing methods and shows strong adaptability.", "motivation": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains.", "method": "BDGxRL framework leverages Diffusion Schr\u00f6dinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. A reward modulation mechanism estimates rewards based on state transitions.", "result": "Experiments demonstrate BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.", "conclusion": "BDGxRL is an effective framework for cross-domain reinforcement learning."}}
{"id": "2602.23759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23759", "abs": "https://arxiv.org/abs/2602.23759", "authors": ["Zuyao You", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Learning Accurate Segmentation Purely from Self-Supervision", "comment": null, "summary": "Accurately segmenting objects without any manual annotations remains one of the core challenges in computer vision. In this work, we introduce Selfment, a fully self-supervised framework that segments foreground objects directly from raw images without human labels, pretrained segmentation models, or any post-processing. Selfment first constructs patch-level affinity graphs from self-supervised features and applies NCut to obtain an initial coarse foreground--background separation. We then introduce Iterative Patch Optimization (IPO), a feature-space refinement procedure that progressively enforces spatial coherence and semantic consistency through iterative patch clustering. The refined masks are subsequently used as supervisory signals to train a lightweight segmentation head with contrastive and region-consistency objectives, allowing the model to learn stable and transferable object representations. Despite its simplicity and complete absence of manual supervision, Selfment sets new state-of-the-art (SoTA) results across multiple benchmarks. It achieves substantial improvements on $F_{\\max}$ over previous unsupervised saliency detection methods on ECSSD ($+4.0\\%$), HKUIS ($+4.6\\%$), and PASCAL-S ($+5.7\\%$). Moreover, without any additional fine-tuning, Selfment demonstrates remarkable zero-shot generalization to camouflaged object detection tasks (e.g., $0.910$ $S_m$ on CHAMELEON and $0.792$ $F_\u03b2^\u03c9$ on CAMO), outperforming all existing unsupervised approaches and even rivaling the SoTA fully supervised methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24119", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24119", "abs": "https://arxiv.org/abs/2602.24119", "authors": ["James L. Zainaldin", "Cameron Pattison", "Manuela Marai", "Jacob Wu", "Mark J. Schiefsky"], "title": "Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek", "comment": "Article + supplementary information", "summary": "This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5728\u53e4\u5e0c\u814a\u6280\u672f\u6563\u6587\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u65b9\u9762\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u5b58\u5728\u672f\u8bed\u7ffb\u8bd1\u6311\u6218\uff0c\u5bf9\u53e4\u5178\u5b66\u672f\u7814\u7a76\u548c\u4f4e\u8d44\u6e90\u53e4\u8001\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6709\u91cd\u8981\u610f\u4e49", "motivation": "\u5bf9\u53e4\u5e0c\u814a\u6280\u672f\u6563\u6587\u8fdb\u884c\u673a\u5668\u7ffb\u8bd1\u7684\u7cfb\u7edf\u6027\u3001\u65e0\u53c2\u8003\u7684\u4eba\u8bc4\u7814\u7a76", "method": "\u8bc4\u4f30\u4e86\u4e09\u4e2a\u5546\u4e1aLLM\uff08Claude\uff0cGemini\uff0cChatGPT\uff09\u5bf920\u6bb5\u53e4\u5e0c\u814a\u533b\u5bb6\u52a0\u4f26\u7684\u4e24\u7bc7\u4f5c\u54c1\u7684\u7ffb\u8bd1\uff0c\u5305\u62ec\u4f7f\u7528\u6807\u51c6\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u548c\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30", "result": "LLM\u5728\u5df2\u7ffb\u8bd1\u7684\u8bf4\u660e\u6027\u6587\u672c\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f46\u5728\u672a\u7ffb\u8bd1\u7684\u836f\u7406\u5b66\u6587\u672c\u4e0a\u8d28\u91cf\u8f83\u4f4e\uff0c\u672f\u8bed\u7684\u7a00\u6709\u5ea6\u662f\u7ffb\u8bd1\u5931\u8d25\u7684\u5173\u952e\u9884\u6d4b\u56e0\u7d20", "conclusion": "LLM\u5728\u53e4\u5178\u5b66\u672f\u7814\u7a76\u548c\u4f4e\u8d44\u6e90\u53e4\u8001\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u7ba1\u9053\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.", "AI": {"tldr": "UMPIRE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u63d0\u9ad8MLLMs\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53ef\u80fd\u4ea7\u751f\u5408\u7406\u4f46\u9519\u8bef\u7684\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u53ef\u9760\u7684\u90e8\u7f72\u3002\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u53ef\u4ee5\u542f\u7528\u5c06\u4e0d\u53ef\u9760\u7684\u67e5\u8be2\u5347\u7ea7\u5230\u4eba\u7c7b\u4e13\u5bb6\u6216\u66f4\u5927\u6a21\u578b\u4ee5\u6539\u8fdb\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u5b58\u5728\u5b9e\u9645\u9650\u5236\uff0c\u4f8b\u5982\u4ec5\u9488\u5bf9\u7279\u5b9a\u6a21\u6001\u8bbe\u8ba1\uff0c\u4f9d\u8d56\u4e8e\u5916\u90e8\u5de5\u5177\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUMPIRE\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5404\u79cd\u8f93\u5165\u548c\u8f93\u51fa\u6a21\u6001\uff0c\u65e0\u9700\u5916\u90e8\u5de5\u5177\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u5185\u90e8\u6a21\u6001\u7279\u5f81\u3002UMPIRE\u8ba1\u7b97\u7ed9\u5b9a\u4efb\u52a1\u5b9e\u4f8b\u4e2d\u91c7\u6837MLLM\u54cd\u5e94\u7684\u5931\u8c03\u8c03\u6574\u540e\u7684\u8bed\u4e49\u4f53\u79ef\uff0c\u6709\u6548\u5730\u6355\u6349\u6837\u672c\u7684\u5168\u5c40\u8bed\u4e49\u591a\u6837\u6027\u548c\u57fa\u4e8e\u5185\u90e8\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u54cd\u5e94\u7684\u5c40\u90e8\u5931\u8c03\u3002", "result": "UMPIRE\u5728\u9519\u8bef\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u5ea6\u91cf\uff0c\u5305\u62ec\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u6587\u672c\u57fa\u51c6\uff0c\u5305\u62ec\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u3002\u8fd8\u8bc1\u660e\u4e86UMPIRE\u5bf9\u975e\u6587\u672c\u8f93\u51fa\u4efb\u52a1\u7684\u6cdb\u5316\uff0c\u5305\u62ec\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u3002", "conclusion": "UMPIRE\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8MLLMs\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.23761", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23761", "abs": "https://arxiv.org/abs/2602.23761", "authors": ["Yuyu Geng", "Lei Sun", "Yao Gao", "Xinxin Hu", "Zhonghua Yi", "Xiaolong Qian", "Weijian Hu", "Jian Bai", "Kaiwei Wang"], "title": "OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design", "comment": null, "summary": "Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.", "AI": {"tldr": "\u9996\u6b21\u5c06LLM\u5e94\u7528\u4e8e\u5149\u5b66\u8bbe\u8ba1\uff0c\u6210\u529f\u5f25\u5408\u4e13\u4e1a\u77e5\u8bc6\u5dee\u8ddd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u5149\u5b66\u8bbe\u8ba1\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4f7f\u6ca1\u6709\u6b63\u5f0f\u5149\u5b66\u8bad\u7ec3\u7684\u7528\u6237\u80fd\u591f\u6210\u529f\u5f00\u53d1\u529f\u80fd\u6027\u955c\u5934\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u521b\u5efaOptiDesignQA\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7cfb\u7edf\u5408\u6210\u548c\u955c\u5934\u5b8c\u6210\u7684\u6df7\u5408\u76ee\u6807\uff0c\u4f7f\u7528DrGRPO\u8fdb\u884c\u6307\u5bfc\uff0c\u5e76\u96c6\u6210\u4e13\u95e8\u7684\u5149\u5b66\u4f18\u5316\u7a0b\u5e8f\u8fdb\u884c\u7aef\u5230\u7aef\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u7b97\u6cd5\u548cLLM\u5bf9\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528LLM\u8fdb\u884c\u5149\u5b66\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5149\u5b66\u8bbe\u8ba1\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u5dee\u8ddd\u7684\u5f25\u5408\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.23783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23783", "abs": "https://arxiv.org/abs/2602.23783", "authors": ["Benlei Cui", "Bukun Huang", "Zhizeng Ye", "Xuemei Dong", "Tuo Chen", "Hui Xue", "Dingkang Yang", "Longtao Huang", "Jingqun Tang", "Haiwen Hong"], "title": "Diffusion Probe: Generated Image Result Prediction Using CNN Probes", "comment": null, "summary": "Text-to-image (T2I) diffusion models lack an efficient mechanism for early quality assessment, leading to costly trial-and-error in multi-generation scenarios such as prompt iteration, agent-based generation, and flow-grpo. We reveal a strong correlation between early diffusion cross-attention distributions and final image quality. Based on this finding, we introduce Diffusion Probe, a framework that leverages internal cross-attention maps as predictive signals.\n  We design a lightweight predictor that maps statistical properties of early-stage cross-attention extracted from initial denoising steps to the final image's overall quality. This enables accurate forecasting of image quality across diverse evaluation metrics long before full synthesis is complete.\n  We validate Diffusion Probe across a wide range of settings. On multiple T2I models, across early denoising windows, resolutions, and quality metrics, it achieves strong correlation (PCC > 0.7) and high classification performance (AUC-ROC > 0.9).\n  Its reliability translates into practical gains. By enabling early quality-aware decisions in workflows such as prompt optimization, seed selection, and accelerated RL training, the probe supports more targeted sampling and avoids computation on low-potential generations. This reduces computational overhead while improving final output quality.\n  Diffusion Probe is model-agnostic, efficient, and broadly applicable, offering a practical solution for improving T2I generation efficiency through early quality prediction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24273", "abs": "https://arxiv.org/abs/2602.24273", "authors": ["Borja Requena Pozo", "Austin Letson", "Krystian Nowakowski", "Izan Beltran Ferreiro", "Leopoldo Sarra"], "title": "A Minimal Agent for Automated Theorem Proving", "comment": null, "summary": "We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23770", "abs": "https://arxiv.org/abs/2602.23770", "authors": ["Chenxing Lin", "Xinhui Gao", "Haipeng Zhang", "Xinran Li", "Haitao Wang", "Songzhu Mei", "Chenglu Wen", "Weiquan Liu", "Siqi Shen", "Cheng Wang"], "title": "MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning", "comment": "ICLR2026", "summary": "Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.", "AI": {"tldr": "MAGE\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u5668\u7684\u79bb\u7ebfRL\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u6709\u6548\u751f\u6210\u53ef\u63a7\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5ffd\u7565\u8f68\u8ff9\u4e2d\u56fa\u6709\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u7ed3\u6784\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAGE\u7684\u57fa\u4e8e\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u5668\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u3002MAGE\u7ed3\u5408\u4e86\u6761\u4ef6\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\u6765\u5b66\u4e60\u5c42\u6b21\u5316\u7684\u8f68\u8ff9\u8868\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a\u591a\u5c3a\u5ea6Transformer\u6765\u81ea\u56de\u5f52\u5730\u751f\u6210\u4ece\u7c97\u5230\u7ec6\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u8f68\u8ff9\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u79bb\u7ebfRL\u57fa\u51c6\u4e0a\u4e0e\u5341\u4e94\u4e2a\u57fa\u7ebf\u7b97\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eMAGE\u6210\u529f\u5730\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u8f68\u8ff9\u5efa\u6a21\u548c\u6761\u4ef6\u5f15\u5bfc\uff0c\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u751f\u6210\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u8f68\u8ff9\u3002", "conclusion": "MAGE\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u7684\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.23790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23790", "abs": "https://arxiv.org/abs/2602.23790", "authors": ["Changyu Gu", "Linwei Chen", "Lin Gu", "Ying Fu"], "title": "Fourier Angle Alignment for Oriented Object Detection in Remote Sensing", "comment": "Accepted by CVPR 2026", "summary": "In remote sensing rotated object detection, mainstream methods suffer from two bottlenecks, directional incoherence at detector neck and task conflict at detecting head. Ulitising fourier rotation equivariance, we introduce Fourier Angle Alignment, which analyses angle information through frequency spectrum and aligns the main direction to a certain orientation. Then we propose two plug and play modules : FAAFusion and FAA Head. FAAFusion works at the detector neck, aligning the main direction of higher-level features to the lower-level features and then fusing them. FAA Head serves as a new detection head, which pre-aligns RoI features to a canonical angle and adds them to the original features before classification and regression. Experiments on DOTA-v1.0, DOTA-v1.5 and HRSC2016 show that our method can greatly improve previous work. Particularly, our method achieves new state-of-the-art results of 78.72% mAP on DOTA-v1.0 and 72.28% mAP on DOTA-v1.5 datasets with single scale training and testing, validating the efficacy of our approach in remote sensing object detection. The code is made publicly available at https://github.com/gcy0423/Fourier-Angle-Alignment .", "code_url": "https://github.com/gcy0423/Fourier-Angle-Alignmen", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u65cb\u8f6c\u7b49\u53d8\u6027\u7684\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u4e3b\u6d41\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a\u68c0\u6d4b\u9888\u90e8\u7684\u65b9\u5411\u4e0d\u4e00\u81f4\u6027\u548c\u68c0\u6d4b\u5934\u90e8\u7684\u4efb\u52a1\u51b2\u7a81\u3002", "method": "\u5229\u7528\u5085\u91cc\u53f6\u65cb\u8f6c\u7b49\u53d8\u6027\uff0c\u5f15\u5165\u5085\u91cc\u53f6\u89d2\u5ea6\u5bf9\u9f50\uff0c\u901a\u8fc7\u9891\u8c31\u5206\u6790\u89d2\u5ea6\u4fe1\u606f\uff0c\u5e76\u5c06\u4e3b\u8981\u65b9\u5411\u5bf9\u9f50\u5230\u7279\u5b9a\u65b9\u5411\u3002\u7136\u540e\u63d0\u51fa\u4e24\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff1aFAAFusion\u548cFAA Head\u3002FAAFusion\u5728\u68c0\u6d4b\u9888\u90e8\u5de5\u4f5c\uff0c\u5c06\u9ad8\u7ea7\u7279\u5f81\u7684 \u4e3b\u8981\u65b9\u5411\u5bf9\u9f50\u5230\u4f4e\u7ea7\u7279\u5f81\u5e76\u878d\u5408\u5b83\u4eec\u3002FAA Head\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u68c0\u6d4b\u5934\u90e8\uff0c\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4e4b\u524d\u9884\u5148\u5bf9\u9f50RoI\u7279\u5f81\u5230\u89c4\u8303\u89d2\u5ea6\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u539f\u59cb\u7279\u5f81\u4e2d\u3002", "result": "\u5728DOTA-v1.0\u3001DOTA-v1.5\u548cHRSC2016\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5148\u524d\u7684\u5de5\u4f5c\u3002\u7279\u522b\u662f\u5728DOTA-v1.0\u6570\u636e\u96c6\u4e0a\u8fbe\u523078.72%\u7684mAP\uff0c\u5728DOTA-v1.5\u6570\u636e\u96c6\u4e0a\u8fbe\u523072.28%\u7684mAP\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002"}}
{"id": "2602.24288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24288", "abs": "https://arxiv.org/abs/2602.24288", "authors": ["Fan Shu", "Yite Wang", "Ruofan Wu", "Boyi Liu", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science", "comment": "Published as a conference paper at ICLR 2026. 10 pages plus appendix", "summary": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.", "AI": {"tldr": "DARE-bench is a new benchmark for LLMs, significantly improving model performance in complex data science tasks.", "motivation": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking.", "method": "Introducing DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. It consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets.", "result": "DARE-bench helps improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x.", "conclusion": "DARE-bench is an important evaluation benchmark and critical training data for LLMs."}}
{"id": "2602.23806", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23806", "abs": "https://arxiv.org/abs/2602.23806", "authors": ["Tianci Tang", "Tielong Cai", "Hongwei Wang", "Gaoang Wang"], "title": "See, Act, Adapt: Active Perception for Unsupervised Cross-Domain Visual Adaptation via Personalized VLM-Guided Agent", "comment": null, "summary": "Pre-trained perception models excel in generic image domains but degrade significantly in novel environments like indoor scenes. The conventional remedy is fine-tuning on downstream data which incurs catastrophic forgetting of prior knowledge and demands costly, scene-specific annotations. We propose a paradigm shift through Sea$^2$ (See, Act, Adapt): rather than adapting the perception modules themselves, we adapt how they are deployed through an intelligent pose-control agent. Sea$^2$ keeps all perception modules frozen, requiring no downstream labels during training, and uses only scalar perceptual feedback to navigate the agent toward informative viewpoints. Specially, we transform a vision-language model (VLM) into a low-level pose controller through a two-stage training pipeline: first fine-tuning it on rule-based exploration trajectories that systematically probe indoor scenes, and then refining the policy via unsupervised reinforcement learning that constructs rewards from the perception module's outputs and confidence. Unlike prior active perception methods that couple exploration with specific models or collect data for retraining them, Sea$^2$ directly leverages off-the-shelf perception models for various tasks without the need for retraining. We conducted experiments on three visual perception tasks, including visual grounding, segmentation and 3D box estimation, with performance improvements of 13.54%, 15.92% and 27.68% respectively on dataset ReplicaCAD.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24174", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.24174", "abs": "https://arxiv.org/abs/2602.24174", "authors": ["Dor Tsur", "Sharon Adar", "Ran Levy"], "title": "Task-Centric Acceleration of Small-Language Models", "comment": null, "summary": "Small language models (SLMs) have emerged as efficient alternatives to large language models for task-specific applications. However, they are often employed in high-volume, low-latency settings, where efficiency is crucial. We propose TASC, Task-Adaptive Sequence Compression, a framework for SLM acceleration comprising two use-cases: When performing SLM fine-tuning, we propose TASC-ft, which iteratively enriches the tokenizer vocabulary with high-frequency output n-grams and then fine-tunes the model to utilize the expanded vocabulary. Next, we propose an inference-time method, termed TASC-spec. TASC-spec is a lightweight, training-free speculative decoding method that constructs an n-gram draft model from the task's output corpus, mixing task and context n-gram information.TASC-spec avoids any additional training, while bypassing draft-target vocabulary alignment constraints. We demonstrate the effectiveness of both methods across multiple low output-variability generation tasks. Our methods show consistent improvements in inference efficiency while maintaining task performance.", "AI": {"tldr": "TASC\u65b9\u6cd5\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\u63d0\u9ad8\u4e86SLMs\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u5e94\u7528\u4e2d\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u9ad8\u5bb9\u91cf\u3001\u4f4e\u5ef6\u8fdf\u73af\u5883\u4e2d\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTASC\uff0c\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u7528\u4f8b\u7684\u6846\u67b6\uff1aTASC-ft\u548cTASC-spec\u3002TASC-ft\u901a\u8fc7\u8fed\u4ee3\u4e30\u5bcc\u6807\u8bb0\u5668\u8bcd\u6c47\u8868\u5e76\u4f7f\u7528\u6269\u5c55\u8bcd\u6c47\u5fae\u8c03\u6a21\u578b\u3002TASC-spec\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u8bad\u7ec3\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u4ece\u4efb\u52a1\u7684\u8f93\u51fa\u8bed\u6599\u5e93\u6784\u5efan-gram\u8349\u6848\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u4f4e\u8f93\u51fa\u53ef\u53d8\u6027\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "TASC\u65b9\u6cd5\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SLMs\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.23785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23785", "abs": "https://arxiv.org/abs/2602.23785", "authors": ["Zhiwei Han", "Stefan Matthes", "Hao Shen"], "title": "Provable Subspace Identification of Nonlinear Multi-view CCA", "comment": null, "summary": "We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \\geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u89c6\u89d2\u975e\u7ebf\u6027CCA\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u89c6\u89d2\u975e\u7ebf\u6027\u5178\u578b\u76f8\u5173\u5206\u6790\uff08CCA\uff09\u7684\u53ef\u8bc6\u522b\u6027", "method": "\u5c06\u591a\u89c6\u89d2CCA\u91cd\u6784\u6210\u57fa\u4e0d\u53d8\u5b50\u7a7a\u95f4\u8bc6\u522b\u95ee\u9898\uff0c\u4f7f\u7528\u8c31\u6270\u52a8\u7406\u8bba\u786e\u4fdd\u6709\u9650\u6837\u672c\u4e00\u81f4\u6027", "result": "\u8bc1\u660e\u4e86\u5728\u5408\u9002\u7684\u6f5c\u5728\u5148\u9a8c\u548c\u9891\u8c31\u5206\u79bb\u6761\u4ef6\u4e0b\uff0c\u591a\u89c6\u89d2CCA\u53ef\u4ee5\u6062\u590d\u6210\u5bf9\u76f8\u5173\u4fe1\u53f7\u5b50\u7a7a\u95f4\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0", "conclusion": "\u591a\u89c6\u89d2CCA\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u6709\u6548\u8bc6\u522b\u975e\u7ebf\u6027\u76f8\u5173\u4fe1\u53f7\u5b50\u7a7a\u95f4"}}
{"id": "2602.23814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23814", "abs": "https://arxiv.org/abs/2602.23814", "authors": ["Chongyang Xu", "Haipeng Li", "Shen Cheng", "Jingyu Hu", "Haoqiang Fan", "Ziliang Feng", "Shuaicheng Liu"], "title": "Action-Geometry Prediction with 3D Geometric Prior for Bimanual Manipulation", "comment": "Accepted by CVPR 2026", "summary": "Bimanual manipulation requires policies that can reason about 3D geometry, anticipate how it evolves under action, and generate smooth, coordinated motions. However, existing methods typically rely on 2D features with limited spatial awareness, or require explicit point clouds that are difficult to obtain reliably in real-world settings. At the same time, recent 3D geometric foundation models show that accurate and diverse 3D structure can be reconstructed directly from RGB images in a fast and robust manner. We leverage this opportunity and propose a framework that builds bimanual manipulation directly on a pre-trained 3D geometric foundation model. Our policy fuses geometry-aware latents, 2D semantic features, and proprioception into a unified state representation, and uses diffusion model to jointly predict a future action chunk and a future 3D latent that decodes into a dense pointmap. By explicitly predicting how the 3D scene will evolve together with the action sequence, the policy gains strong spatial understanding and predictive capability using only RGB observations. We evaluate our method both in simulation on the RoboTwin benchmark and in real-world robot executions. Our approach consistently outperforms 2D-based and point-cloud-based baselines, achieving state-of-the-art performance in manipulation success, inter-arm coordination, and 3D spatial prediction accuracy. Code is available at https://github.com/Chongyang-99/GAP.git.", "code_url": "https://github.com/Chongyang-99/GAP", "code_stars": 1, "code_last_update": "2026-02-21", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u53cc\u81c2\u64cd\u4f5c\u6846\u67b6\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53cc\u81c2\u64cd\u4f5c\u9700\u8981\u5904\u74063D\u51e0\u4f55\u3001\u9884\u6d4b\u52a8\u4f5c\u5f71\u54cd\u4ee5\u53ca\u751f\u6210\u5e73\u6ed1\u534f\u8c03\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u611f\u77e5\u548c\u70b9\u4e91\u83b7\u53d6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u76843D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u6846\u67b6\uff0c\u878d\u5408\u51e0\u4f55\u611f\u77e5\u7684\u6f5c\u5728\u8868\u793a\u30012D\u8bed\u4e49\u7279\u5f81\u548c\u81ea\u8eab\u4f53\u611f\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\u5757\u548c\u672a\u67653D\u6f5c\u5728\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u53cc\u81c2\u64cd\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u64cd\u4f5c\u6210\u529f\u7387\u3001\u624b\u81c2\u534f\u8c03\u548c3D\u7a7a\u95f4\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e2D\u548c\u70b9\u4e91\u7684\u57fa\u7ebf\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53cc\u81c2\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.24188", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24188", "abs": "https://arxiv.org/abs/2602.24188", "authors": ["Jacob Eisenstein", "Fantine Huot", "Adam Fisch", "Jonathan Berant", "Mirella Lapata"], "title": "MT-PingEval: Evaluating Multi-Turn Collaboration with Private Information Games", "comment": null, "summary": "We present a scalable methodology for evaluating language models in multi-turn interactions, using a suite of collaborative games that require effective communication about private information. This enables an interactive scaling analysis, in which a fixed token budget is divided over a variable number of turns. We find that in many cases, language models are unable to use interactive collaboration to improve over the non-interactive baseline scenario in which one agent attempts to summarize its information and the other agent immediately acts -- despite substantial headroom. This suggests that state-of-the-art models still suffer from significant weaknesses in planning and executing multi-turn collaborative conversations. We analyze the linguistic features of these dialogues, assessing the roles of sycophancy, information density, and discourse coherence. While there is no single linguistic explanation for the collaborative weaknesses of contemporary language models, we note that humans achieve comparable task success at superior token efficiency by producing dialogues that are more coherent than those produced by most language models. The proactive management of private information is a defining feature of real-world communication, and we hope that MT-PingEval will drive further work towards improving this capability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23817", "abs": "https://arxiv.org/abs/2602.23817", "authors": ["Pratibha Kumari", "Daniel Reisenb\u00fcchler", "Afshin Bozorgpour", "yousef Sadegheih", "Priyankar Choudhary", "Dorit Merhof"], "title": "Footprint-Guided Exemplar-Free Continual Histopathology Report Generation", "comment": null, "summary": "Rapid progress in vision-language modeling has enabled pathology report generation from gigapixel whole-slide images, but most approaches assume static training with simultaneous access to all data. In clinical deployment, however, new organs, institutions, and reporting conventions emerge over time, and sequential fine-tuning can cause catastrophic forgetting. We introduce an exemplar-free continual learning framework for WSI-to-report generation that avoids storing raw slides or patch exemplars. The core idea is a compact domain footprint built in a frozen patch-embedding space: a small codebook of representative morphology tokens together with slide-level co-occurrence summaries and lightweight patch-count priors. These footprints support generative replay by synthesizing pseudo-WSI representations that reflect domain-specific morphological mixtures, while a teacher snapshot provides pseudo-reports to supervise the updated model without retaining past data. To address shifting reporting conventions, we distill domain-specific linguistic characteristics into a compact style descriptor and use it to steer generation. At inference, the model identifies the most compatible descriptor directly from the slide signal, enabling domain-agnostic setup without requiring explicit domain identifiers. Evaluated across multiple public continual learning benchmarks, our approach outperforms exemplar-free and limited-buffer rehearsal baselines, highlighting footprint-based generative replay as a practical solution for deployment in evolving clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u793a\u4f8b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u75c5\u7406\u62a5\u544a\uff0c\u6709\u6548\u907f\u514d\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u75c5\u7406\u62a5\u544a\u751f\u6210\u4e2d\u7531\u4e8e\u4e34\u5e8a\u90e8\u7f72\u4e2d\u5668\u5b98\u3001\u673a\u6784\u548c\u62a5\u544a\u89c4\u8303\u968f\u65f6\u95f4\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u793a\u4f8b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u51bb\u7ed3\u7684\u8865\u4e01\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6784\u5efa\u7d27\u51d1\u7684\u9886\u57df\u8db3\u8ff9\uff1a\u4e00\u7ec4\u4ee3\u8868\u6027\u7684\u5f62\u6001\u6807\u8bb0\u7684\u5c0f\u4ee3\u7801\u7c3f\u4ee5\u53ca\u5e7b\u706f\u7247\u7ea7\u522b\u7684\u5171\u73b0\u6458\u8981\u548c\u8f7b\u91cf\u7ea7\u7684\u8865\u4e01\u8ba1\u6570\u5148\u9a8c\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u65e0\u793a\u4f8b\u548c\u6709\u9650\u7f13\u51b2\u91cd\u6392\u57fa\u7ebf\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8e\u8db3\u8ff9\u7684\u751f\u6210\u91cd\u653e\u4f5c\u4e3a\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u75c5\u7406\u62a5\u544a\u751f\u6210\u3002"}}
{"id": "2602.24210", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24210", "abs": "https://arxiv.org/abs/2602.24210", "authors": ["Haritz Puerto", "Haonan Li", "Xudong Han", "Timothy Baldwin", "Iryna Gurevych"], "title": "Controllable Reasoning Models Are Private Thinkers", "comment": null, "summary": "AI agents powered by reasoning models require access to sensitive user data. However, their reasoning traces are difficult to control, which can result in the unintended leakage of private information to external parties. We propose training models to follow instructions not only in the final answer, but also in reasoning traces, potentially under different constraints. We hypothesize that improving their instruction following abilities in the reasoning traces can improve their privacy-preservation skills. To demonstrate this, we fine-tune models on a new instruction-following dataset with explicit restrictions on reasoning traces. We further introduce a generation strategy that decouples reasoning and answer generation using separate LoRA adapters. We evaluate our approach on six models from two model families, ranging from 1.7B to 14B parameters, across two instruction-following benchmarks and two privacy benchmarks. Our method yields substantial improvements, achieving gains of up to 20.9 points in instruction-following performance and up to 51.9 percentage points on privacy benchmarks. These improvements, however, can come at the cost of task utility, due to the trade-off between reasoning performance and instruction-following abilities. Overall, our results show that improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents. Our code and data are available at https://github.com/UKPLab/arxiv2026-controllable-reasoning-models", "code_url": "https://github.com/UKPLab/arxiv2026-controllable-reasoning-models", "code_stars": 0, "code_last_update": "2026-02-27", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u884c\u4e3a\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u9690\u79c1\u611f\u77e5\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "motivation": "AI agents powered by reasoning models require access to sensitive user data, which can result in the unintended leakage of private information.", "method": "Proposing training models to follow instructions in reasoning traces, introducing a generation strategy that decouples reasoning and answer generation using separate LoRA adapters, and evaluating on six models across two benchmarks.", "result": "Significant improvements in instruction-following performance and privacy benchmarks, but with a trade-off in task utility.", "conclusion": "Improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents."}}
{"id": "2602.23795", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23795", "abs": "https://arxiv.org/abs/2602.23795", "authors": ["Wenwu Tang", "Dong Wang", "Lothar Thiele", "Olga Saukh"], "title": "GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks", "comment": "Conference on Parsimony and Learning (CPAL)", "summary": "Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.", "code_url": "https://github.com/TWWinde/GRAIL", "code_stars": 1, "code_last_update": "2026-02-05", "AI": {"tldr": "GRAIL\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u5904\u7406\u5757\u8865\u507f\u6062\u590d\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u6df1\u5ea6\u6a21\u578b\u538b\u7f29\u540e\uff0c\u7531\u4e8e\u538b\u7f29\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\uff0c\u901a\u5e38\u9700\u8981\u540e\u538b\u7f29\u5fae\u8c03\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u5757\u8865\u507f\u65b9\u6cd5\uff0c\u79f0\u4e3aGRAIL\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u538b\u7f29\u540e\u4f7f\u7528\u4e00\u4e2a\u5c0f\u578b\u6821\u51c6\u96c6\u6062\u590d\u6bcf\u4e2a\u5757\u7684\u8f93\u5165\u8f93\u51fa\u884c\u4e3a\u3002", "result": "GRAIL\u5728ResNets\u3001ViTs\u548c\u4ec5\u89e3\u7801\u5668LLMs\u4e0a\uff0c\u5728\u5b9e\u7528\u538b\u7f29\u8303\u56f4\u5185\uff0c\u4e0e\u65e0\u6570\u636e\u548c\u6570\u636e\u611f\u77e5\u526a\u679d\u6216\u6298\u53e0\u57fa\u7ebf\u76f8\u6bd4\uff0c\u59cb\u7ec8\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u6216\u56f0\u60d1\u5ea6\u3002", "conclusion": "GRAIL\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u3001\u6570\u636e\u611f\u77e5\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.23820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23820", "abs": "https://arxiv.org/abs/2602.23820", "authors": ["Xiaojing Zhao", "Shiyang Li", "Zena Chu", "Ying Zhang", "Peinan Hao", "Tianzi Yan", "Jiajia Chen", "Huicong Ning"], "title": "Denoising-Enhanced YOLO for Robust SAR Ship Detection", "comment": null, "summary": "With the rapid advancement of deep learning, synthetic aperture radar (SAR) imagery has become a key modality for ship detection. However, robust performance remains challenging in complex scenes, where clutter and speckle noise can induce false alarms and small targets are easily missed. To address these issues, we propose CPN-YOLO, a high-precision ship detection framework built upon YOLOv8 with three targeted improvements. First, we introduce a learnable large-kernel denoising module for input pre-processing, producing cleaner representations and more discriminative features across diverse ship types. Second, we design a feature extraction enhancement strategy based on the PPA attention mechanism to strengthen multi-scale modeling and improve sensitivity to small ships. Third, we incorporate a Gaussian similarity loss derived from the normalized Wasserstein distance (NWD) to better measure similarity under complex bounding-box distributions and improve generalization. Extensive experiments on HRSID and SSDD demonstrate the effectiveness of our method. On SSDD, CPN-YOLO surpasses the YOLOv8 baseline, achieving 97.0% precision, 95.1% recall, and 98.9% mAP, and consistently outperforms other representative deep-learning detectors in overall performance.", "AI": {"tldr": "CPN-YOLO\uff1a\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u8239\u8236\u68c0\u6d4b\u6846\u67b6\uff0c\u6709\u6548\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u8239\u8236\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u6742\u6ce2\u548c\u6591\u70b9\u566a\u58f0\u5f15\u8d77\u7684\u8bef\u62a5\u548c\u5c0f\u76ee\u6807\u6613\u88ab\u9057\u6f0f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCPN-YOLO\uff0c\u57fa\u4e8eYOLOv8\u8fdb\u884c\u6539\u8fdb\uff0c\u5305\u62ec\u53ef\u5b66\u4e60\u7684\u6838\u5927\u5c0f\u53ef\u53d8\u53bb\u566a\u6a21\u5757\u3001\u57fa\u4e8ePPA\u6ce8\u610f\u529b\u673a\u5236\u7684\u7279\u5f81\u63d0\u53d6\u589e\u5f3a\u7b56\u7565\u4ee5\u53ca\u57fa\u4e8e\u5f52\u4e00\u5316Wasserstein\u8ddd\u79bb\u7684\u9ad8\u65af\u76f8\u4f3c\u5ea6\u635f\u5931\u3002", "result": "\u5728HRSID\u548cSSDD\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCPN-YOLO\u5728SSDD\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eYOLOv8\u57fa\u7ebf\uff0c\u8fbe\u523097.0%\u7684\u7cbe\u5ea6\u300195.1%\u7684\u53ec\u56de\u7387\u548c98.9%\u7684mAP\uff0c\u603b\u4f53\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u5668\u3002", "conclusion": "CPN-YOLO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8239\u8236\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5c0f\u76ee\u6807\u548c\u907f\u514d\u8bef\u62a5\u3002"}}
{"id": "2602.24287", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24287", "abs": "https://arxiv.org/abs/2602.24287", "authors": ["Jenny Y. Huang", "Leshem Choshen", "Ramon Astudillo", "Tamara Broderick", "Jacob Andreas"], "title": "Do LLMs Benefit From Their Own Words?", "comment": null, "summary": "Multi-turn interactions with large language models typically retain the assistant's own past responses in the conversation history. In this work, we revisit this design choice by asking whether large language models benefit from conditioning on their own prior responses. Using in-the-wild, multi-turn conversations, we compare standard (full-context) prompting with a user-turn-only prompting approach that omits all previous assistant responses, across three open reasoning models and one state-of-the-art model. To our surprise, we find that removing prior assistant responses does not affect response quality on a large fraction of turns. Omitting assistant-side history can reduce cumulative context lengths by up to 10x. To explain this result, we find that multi-turn conversations consist of a substantial proportion (36.4%) of self-contained prompts, and that many follow-up prompts provide sufficient instruction to be answered using only the current user turn and prior user turns. When analyzing cases where user-turn-only prompting substantially outperforms full context, we identify instances of context pollution, in which models over-condition on their previous responses, introducing errors, hallucinations, or stylistic artifacts that propagate across turns. Motivated by these findings, we design a context-filtering approach that selectively omits assistant-side context. Our findings suggest that selectively omitting assistant history can improve response quality while reducing memory consumption.", "AI": {"tldr": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4e0d\u9700\u8981\u4f9d\u8d56\u4e8e\u81ea\u8eab\u5148\u524d\u54cd\u5e94\uff0c\u9009\u62e9\u6027\u7701\u7565\u52a9\u624b\u5386\u53f2\u53ef\u4ee5\u6539\u5584\u6027\u80fd", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u662f\u5426\u4ece\u4f9d\u8d56\u4e8e\u81ea\u8eab\u5148\u524d\u54cd\u5e94\u4e2d\u53d7\u76ca", "method": "\u4f7f\u7528\u91ce\u5916\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6bd4\u8f83\u6807\u51c6\uff08\u5b8c\u6574\u4e0a\u4e0b\u6587\uff09\u63d0\u793a\u4e0e\u4ec5\u7528\u6237\u54cd\u5e94\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u5f00\u653e\u63a8\u7406\u6a21\u578b\u548c\u4e00\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5206\u6790", "result": "\u53d1\u73b0\u79fb\u9664\u5148\u524d\u52a9\u624b\u54cd\u5e94\u4e0d\u4f1a\u5f71\u54cd\u5927\u90e8\u5206\u8f6e\u6b21\u7684\u54cd\u5e94\u8d28\u91cf\uff0c\u7701\u7565\u52a9\u624b\u5386\u53f2\u53ef\u4ee5\u51cf\u5c11\u7d2f\u79ef\u4e0a\u4e0b\u6587\u957f\u5ea6\u9ad8\u8fbe10\u500d\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u5730\u7701\u7565\u52a9\u624b\u4e0a\u4e0b\u6587\uff0c\u7ed3\u679c\u8868\u660e\u9009\u62e9\u6027\u7701\u7565\u52a9\u624b\u5386\u53f2\u53ef\u4ee5\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u6d88\u8017", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4e0d\u9700\u8981\u4f9d\u8d56\u4e8e\u81ea\u8eab\u5148\u524d\u54cd\u5e94\uff0c\u9009\u62e9\u6027\u7701\u7565\u52a9\u624b\u5386\u53f2\u53ef\u4ee5\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u6d88\u8017"}}
{"id": "2602.23823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23823", "abs": "https://arxiv.org/abs/2602.23823", "authors": ["Henghui Du", "Chang Zhou", "Xi Chen", "Di Hu"], "title": "APPO: Attention-guided Perception Policy Optimization for Video Reasoning", "comment": null, "summary": "Complex video reasoning, actually, relies excessively on fine-grained perception rather than on expert (e.g., Ph.D, Science)-level reasoning. Through extensive empirical observation, we have recognized the critical impact of perception. In particular, when perception ability is almost fixed, enhancing reasoning from Qwen3-8B to OpenAI-o3 yields only 0.7% performance improvement. Conversely, even minimal change in perception model scale (from 7B to 32B) boosts performance by 1.4%, indicating enhancing perception, rather than reasoning, is more critical to improve performance. Therefore, exploring how to enhance perception ability through reasoning without the need for expensive fine-grained annotation information is worthwhile. To achieve this goal, we specially propose APPO, the Attention-guided Perception Policy Optimization algorithm that leverages token-level dense rewards to improve model's fine-grained perception. The core idea behind APPO is to optimize those tokens from different responses that primarily focus on the same crucial video frame (called intra-group perception tokens). Experimental results on diverse video benchmarks and models with different scales (3/7B) demonstrate APPO consistently outperforms GRPO and DAPO (0.5%~4%). We hope our work provides a promising approach to effectively enhance model's perception abilities through reasoning in a low-cost manner, serving diverse scenarios and demands.", "AI": {"tldr": "APPO\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316\u5173\u952e\u89c6\u9891\u5e27\u7684token\u611f\u77e5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u590d\u6742\u89c6\u9891\u63a8\u7406\u8fc7\u5ea6\u4f9d\u8d56\u7ec6\u7c92\u5ea6\u611f\u77e5\u800c\u975e\u4e13\u5bb6\u63a8\u7406\uff0c\u901a\u8fc7\u5927\u91cf\u89c2\u5bdf\u8ba4\u8bc6\u5230\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faAPPO\u7b97\u6cd5\uff0c\u5229\u7528token\u7ea7\u522b\u7684\u5bc6\u96c6\u5956\u52b1\u6765\u4f18\u5316\u6a21\u578b\u5bf9\u5173\u952e\u89c6\u9891\u5e27\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u3002", "result": "APPO\u5728\u591a\u4e2a\u89c6\u9891\u57fa\u51c6\u548c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u4f18\u4e8eGRPO\u548cDAPO\uff0c\u63d0\u5347\u6027\u80fd0.5%~4%\u3002", "conclusion": "APPO\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u4f4e\u6210\u672c\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.23863", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23863", "abs": "https://arxiv.org/abs/2602.23863", "authors": ["Xiaoyu Guo", "Arkaitz Zubiaga"], "title": "NAU-QMUL: Utilizing BERT and CLIP for Multi-modal AI-Generated Image Detection", "comment": null, "summary": "With the aim of detecting AI-generated images and identifying the specific models responsible for their generation, we propose a multi-modal multi-task model. The model leverages pre-trained BERT and CLIP Vision encoders for text and image feature extraction, respectively, and employs cross-modal feature fusion with a tailored multi-task loss function. Additionally, a pseudo-labeling-based data augmentation strategy was utilized to expand the training dataset with high-confidence samples. The model achieved fifth place in both Tasks A and B of the `CT2: AI-Generated Image Detection' competition, with F1 scores of 83.16\\% and 48.88\\%, respectively. These findings highlight the effectiveness of the proposed architecture and its potential for advancing AI-generated content detection in real-world scenarios. The source code for our method is published on https://github.com/xxxxxxxxy/AIGeneratedImageDetection.", "code_url": "https://github.com/xxxxxxxxy/AIGeneratedImageDetection", "code_stars": 0, "code_last_update": "2025-01-11", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23811", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23811", "abs": "https://arxiv.org/abs/2602.23811", "authors": ["Xiang Li", "Nan Jiang", "Yuheng Zhang"], "title": "Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies", "comment": null, "summary": "We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4ec5\u9002\u7528\u4e8e\u6709\u9650\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4e14\u9700\u8981\u4ece\u8bc4\u8bba\u5bb6\u51fd\u6570\u4e2d\u9690\u5f0f\u5730\u8bf1\u5bfc\u52a8\u4f5c\u6267\u884c\u8005\uff0c\u8fd9\u9650\u5236\u4e86\u72ec\u7acb\u7b56\u7565\u53c2\u6570\u5316\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5c06\u955c\u50cf\u4e0b\u964d\u6269\u5c55\u5230\u53c2\u6570\u5316\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u8026\u5408\u6765\u514b\u670d\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u8be5\u7b97\u6cd5\u5c06\u7406\u8bba\u4fdd\u8bc1\u6269\u5c55\u5230\u53c2\u6570\u5316\u7b56\u7565\u7c7b\uff0c\u9002\u7528\u4e8e\u5927\u6216\u8fde\u7eed\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u63ed\u793a\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u7684\u7edf\u4e00\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u89c1\u89e3\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.23869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23869", "abs": "https://arxiv.org/abs/2602.23869", "authors": ["Mohammadreza Heidarianbaei", "Mareike Dorozynski", "Hubert Kanyamahanga", "Max Mehltretter", "Franz Rottensteiner"], "title": "Open-Vocabulary Semantic Segmentation in Remote Sensing via Hierarchical Attention Masking and Model Composition", "comment": "Published in the proceedings of the British Machine Vision Conference Workshops 2025", "summary": "In this paper, we propose ReSeg-CLIP, a new training-free Open-Vocabulary Semantic Segmentation method for remote sensing data. To compensate for the problems of vision language models, such as CLIP in semantic segmentation caused by inappropriate interactions within the self-attention layers, we introduce a hierarchical scheme utilizing masks generated by SAM to constrain the interactions at multiple scales. We also present a model composition approach that averages the parameters of multiple RS-specific CLIP variants, taking advantage of a new weighting scheme that evaluates representational quality using varying text prompts. Our method achieves state-of-the-art results across three RS benchmarks without additional training.", "AI": {"tldr": "\u63d0\u51faReSeg-CLIP\uff0c\u4e00\u79cd\u65e0\u76d1\u7763\u9065\u611f\u6570\u636e\u8bed\u4e49\u5206\u5272\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5982CLIP\u5728\u9065\u611f\u6570\u636e\u8bed\u4e49\u5206\u5272\u4e2d\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u5c42\u5185\u90e8\u4e0d\u5f53\u4ea4\u4e92\u5bfc\u81f4\u7684\u9519\u8bef\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u5229\u7528SAM\u751f\u6210\u7684\u63a9\u7801\u7684\u5206\u5c42\u65b9\u6848\u6765\u7ea6\u675f\u591a\u5c3a\u5ea6\u4ea4\u4e92\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u7ec4\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u52a0\u6743\u65b9\u6848\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u6587\u672c\u63d0\u793a\u6765\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\uff0c\u5e73\u5747\u591a\u4e2aRS\u7279\u5b9a\u7684CLIP\u53d8\u4f53\u7684\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u9065\u611f\u6570\u636e\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "ReSeg-CLIP\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u9065\u611f\u6570\u636e\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2602.23816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23816", "abs": "https://arxiv.org/abs/2602.23816", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective", "comment": "Accepted for publication at AAMAS 2026", "summary": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise\" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.", "AI": {"tldr": "This paper proposes the SafeQIL algorithm for learning a policy that balances safety and reward in constrained MDPs.", "motivation": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps.", "method": "Learning a policy that maximizes the probability of the most promising trajectories with respect to the demonstrations. Formulating the 'promise' of individual state-action pairs in terms of Q values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. Safe Q-learning perspective of the inverse learning problem under constraints.", "result": "The devised Safe Q Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the-art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.", "conclusion": "The SafeQIL algorithm is effective in learning a policy that balances safety and reward in constrained MDPs."}}
{"id": "2602.23824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23824", "abs": "https://arxiv.org/abs/2602.23824", "authors": ["Pavlin G. Poli\u010dar", "Dalibor Stanimirovi\u0107", "Bla\u017e Zupan"], "title": "Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach", "comment": null, "summary": "Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5904\u65b9\u52a8\u6001\u7684\u75be\u75c5\u53d1\u75c5\u63a8\u65ad\u65b9\u6cd5\uff0c\u6709\u6548\u5904\u7406\u4e86\u5de6\u622a\u65ad\u6570\u636e\uff0c\u4f46\u68c0\u6d4b\u6027\u80fd\u5728\u4e0d\u540c\u75be\u75c5\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5e38\u5b58\u5728\u5de6\u622a\u65ad\uff0c\u5bfc\u81f4\u8bca\u65ad\u8bb0\u5f55\u4e0d\u5b8c\u6574\u4e14\u4e0d\u53ef\u9760\uff0c\u65e0\u6cd5\u786e\u5b9a\u75be\u75c5\u53d1\u75c5\u65f6\u95f4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u95e8\u8bca\u5904\u65b9\u5f62\u6210\u57fa\u4e8e\u66f4\u65b0\u7684\u8f68\u8ff9\uff0c\u4e3a\u75be\u75c5\u7ba1\u7406\u63d0\u4f9b\u8fde\u7eed\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5904\u65b9\u52a8\u6001\u5efa\u6a21\u4e3a\u66f4\u65b0\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5728\u57fa\u7ebf\u6cca\u677e\uff08\u5076\u7136\u5904\u65b9\uff09\u5236\u5ea6\u548c\u7279\u5b9a\u5236\u5ea6Weibull\uff08\u6301\u7eed\u6cbb\u7597\uff09\u66f4\u65b0\u6a21\u578b\u4e4b\u95f4\u7684\u53d8\u5316\u70b9\u68c0\u6d4b\uff0c\u68c0\u6d4b\u4ece\u5076\u7136\u5230\u6301\u7eed\u6cbb\u7597\u7684\u8fc7\u6e21\u3002", "result": "\u4f7f\u7528\u6db5\u76d6240\u4e07\u4eba\u6b21\u7684\u5168\u56fd\u7535\u5b50\u5904\u65b9\u6570\u636e\u96c6\uff0c\u8be5\u65b9\u6cd5\u6bd4\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u89e6\u53d1\u65b9\u5f0f\u4ea7\u751f\u66f4\u5408\u7406\u7684\u65f6\u95f4\u53d1\u75c5\u4f30\u8ba1\uff0c\u5728\u5f3a\u5de6\u622a\u65ad\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5408\u7406\u65e9\u671f\u68c0\u6d4b\u3002\u68c0\u6d4b\u6027\u80fd\u5728\u4e0d\u540c\u75be\u75c5\u4e2d\u6709\u6240\u4e0d\u540c\uff0c\u4e0e\u5904\u65b9\u5bc6\u5ea6\u5bc6\u5207\u76f8\u5173\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8e\u6cbb\u7597\u53d1\u75c5\u63a8\u65ad\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u57fa\u4e8e\u5904\u65b9\u7684\u75be\u75c5\u53d1\u75c5\u63a8\u65ad\u65b9\u6cd5\u5728\u5904\u7406\u5de6\u622a\u65ad\u6570\u636e\u65f6\u4f18\u4e8e\u7b80\u5355\u89c4\u5219\u89e6\u53d1\u65b9\u6cd5\uff0c\u4f46\u68c0\u6d4b\u6027\u80fd\u5728\u4e0d\u540c\u75be\u75c5\u4e2d\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u4e0e\u5904\u65b9\u5bc6\u5ea6\u76f8\u5173\u3002"}}
{"id": "2602.23872", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23872", "abs": "https://arxiv.org/abs/2602.23872", "authors": ["Xingyu Shao", "Mengfan He", "Chunyu Li", "Liangzheng Sun", "Ziyang Meng"], "title": "Altitude-Aware Visual Place Recognition in Top-Down View", "comment": null, "summary": "To address the challenge of aerial visual place recognition (VPR) problem under significant altitude variations, this study proposes an altitude-adaptive VPR approach that integrates ground feature density analysis with image classification techniques. The proposed method estimates airborne platforms' relative altitude by analyzing the density of ground features in images, then applies relative altitude-based cropping to generate canonical query images, which are subsequently used in a classification-based VPR strategy for localization. Extensive experiments across diverse terrains and altitude conditions demonstrate that the proposed approach achieves high accuracy and robustness in both altitude estimation and VPR under significant altitude changes. Compared to conventional methods relying on barometric altimeters or Time-of-Flight (ToF) sensors, this solution requires no additional hardware and offers a plug-and-play solution for downstream applications, {making it suitable for small- and medium-sized airborne platforms operating in diverse environments, including rural and urban areas.} Under significant altitude variations, incorporating our relative altitude estimation module into the VPR retrieval pipeline boosts average R@1 and R@5 by 29.85\\% and 60.20\\%, respectively, compared with applying VPR retrieval alone. Furthermore, compared to traditional {Monocular Metric Depth Estimation (MMDE) methods}, the proposed method reduces the mean error by 202.1 m, yielding average additional improvements of 31.4\\% in R@1 and 44\\% in R@5. These results demonstrate that our method establishes a robust, vision-only framework for three-dimensional visual place recognition, offering a practical and scalable solution for accurate airborne platforms localization under large altitude variations and limited sensor availability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u4e2d\u89c6\u89c9\u573a\u6240\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u5206\u6790\u548c\u56fe\u50cf\u5206\u7c7b\u6280\u672f\uff0c\u5728\u5927\u8303\u56f4\u9ad8\u5ea6\u53d8\u5316\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u7684\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u5728\u5927\u8303\u56f4\u9ad8\u5ea6\u53d8\u5316\u4e0b\u7a7a\u4e2d\u89c6\u89c9\u573a\u6240\u8bc6\u522b\uff08VPR\uff09\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u5206\u6790\u548c\u56fe\u50cf\u5206\u7c7b\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u4e2d\u7684\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u6765\u4f30\u8ba1\u98de\u884c\u5e73\u53f0\u7684\u76f8\u5bf9\u9ad8\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u751f\u6210\u6807\u51c6\u67e5\u8be2\u56fe\u50cf\uff0c\u4ece\u800c\u5728\u57fa\u4e8e\u5206\u7c7b\u7684\u89c6\u89c9\u573a\u6240\u8bc6\u522b\u7b56\u7565\u4e2d\u8fdb\u884c\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6c14\u538b\u8ba1\u6216\u98de\u884c\u65f6\u95f4\uff08ToF\uff09\u4f20\u611f\u5668\u7684\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u989d\u5916\u7684\u786c\u4ef6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u73af\u5883\uff0c\u5305\u62ec\u519c\u6751\u548c\u57ce\u5e02\u5730\u533a\u7684\u5c0f\u578b\u548c\u4e2d\u7b49\u5c3a\u5bf8\u7a7a\u4e2d\u5e73\u53f0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u89c6\u89c9\u573a\u6240\u8bc6\u522b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u8303\u56f4\u9ad8\u5ea6\u53d8\u5316\u4e0b\u7684\u7a7a\u4e2d\u5e73\u53f0\u5b9a\u4f4d\u3002"}}
{"id": "2602.23827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23827", "abs": "https://arxiv.org/abs/2602.23827", "authors": ["Junkang Liu", "Fanhua Shang", "Yuxuan Tian", "Hongying Liu", "Yuanyuan Liu"], "title": "FedNSAM:Consistency of Local and Global Flatness for Federated Learning", "comment": null, "summary": "In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \\textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \\textbf{flatness distance}, we propose a novel \\textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \\textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \\textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.", "code_url": "https://github.com/junkangLiu0/FedNSAM", "code_stars": 65, "code_last_update": "2025-11-27", "AI": {"tldr": "\u63d0\u51faFedNSAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u6765\u52a0\u901fSAM\u7b97\u6cd5\uff0c\u63d0\u9ad8\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u591a\u6b65\u672c\u5730\u66f4\u65b0\u548c\u6570\u636e\u5f02\u6784\u6027\u901a\u5e38\u4f1a\u5bfc\u81f4\u66f4\u5c16\u9510\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u4ece\u800c\u964d\u4f4e\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002\u6d41\u884c\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5c06\u5c16\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u96c6\u6210\u5230\u672c\u5730\u8bad\u7ec3\u4e2d\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u9ad8\u5ea6\u6570\u636e\u5f02\u6784\u7684\u8bbe\u7f6e\u4e2d\uff0c\u672c\u5730\u8bad\u7ec3\u4e2d\u7684\u5e73\u5766\u6027\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5168\u5c40\u6a21\u578b\u7684\u5e73\u5766\u6027\u3002\u56e0\u6b64\uff0c\u5728\u5ba2\u6237\u7aef\u6570\u636e\u4e0a\u6700\u5c0f\u5316\u672c\u5730\u635f\u5931\u8868\u9762\u7684\u5c16\u9510\u5ea6\u5e76\u4e0d\u80fd\u4f7fSAM\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u63d0\u9ad8\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6211\u4eec\u5b9a\u4e49\u4e86\u5e73\u5766\u5ea6\u8ddd\u79bb\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u8054\u90a6\u5b66\u4e60\u4e2d\u7684SAM\u5e76\u7406\u8bba\u5206\u6790\u5e73\u5766\u5ea6\u8ddd\u79bb\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684FedNSAM\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u5230\u672c\u5730\u66f4\u65b0\u4e2d\u6765\u534f\u8c03\u5168\u5c40\u548c\u5c40\u90e8\u5e73\u5766\u6027\u7684\u4e00\u81f4\u6027\u3002FedNSAM\u4f7f\u7528\u5168\u5c40Nesterov\u52a8\u91cf\u4f5c\u4e3a\u5ba2\u6237\u7aef\u5168\u5c40\u6270\u52a8\u4f30\u8ba1\u548c\u6269\u5c55\u7684\u65b9\u5411\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u901a\u8fc7Nesterov\u5916\u63a8\u8bc1\u660e\u4e86\u6bd4FedSAM\u66f4\u7d27\u7684\u6536\u655b\u754c\u9650\u3002\u5728CNN\u548cTransformer\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1FedNSAM\u7684\u4f18\u5f02\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "FedNSAM\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u6765\u52a0\u901fSAM\u7b97\u6cd5\uff0c\u5e76\u63d0\u9ad8\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728CNN\u548cTransformer\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedNSAM\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8eFedSAM\u3002", "conclusion": "FedNSAM\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684SAM\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.23881", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23881", "abs": "https://arxiv.org/abs/2602.23881", "authors": ["Alexander Samarin", "Sergei Krutikov", "Anton Shevtsov", "Sergei Skvortsov", "Filipp Fisin", "Alexander Golubev"], "title": "LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding", "comment": null, "summary": "Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.", "AI": {"tldr": "LK losses \u901a\u8fc7\u4f18\u5316\u63a5\u53d7\u7387\u6765\u52a0\u901f\u81ea\u52a8\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u3002", "motivation": "\u6807\u51c6\u8bad\u7ec3\u901a\u8fc7\u6700\u5c0f\u5316 KL \u6563\u5ea6\u6765\u4f18\u5316\u63a5\u53d7\u7387\uff0c\u4f46 KL \u6563\u5ea6\u4e0d\u4fdd\u8bc1\u6700\u5927\u5316\u63a5\u53d7\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u76ee\u6807 LK losses\uff0c\u7528\u4e8e\u4f18\u5316\u63a5\u53d7\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLK losses \u76f8\u6bd4\u6807\u51c6 KL-based \u8bad\u7ec3\uff0c\u5728\u6240\u6709\u914d\u7f6e\u4e2d\u5747\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\u3002", "conclusion": "LK losses \u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\uff0c\u4e14\u6613\u4e8e\u5b9e\u73b0\uff0c\u65e0\u9700\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7684\u6295\u673a\u5668\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002"}}
{"id": "2602.23852", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.23852", "abs": "https://arxiv.org/abs/2602.23852", "authors": ["Zhaowen Wang", "Dongdong Zhou", "Qi Xu", "Fengyu Cong", "Mohammad Al-Sa'd", "Jenni Raitoharju"], "title": "ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring", "comment": "Accepted to ICASSP 2026", "summary": "Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.", "code_url": "https://github.com/wzw999/ULW-SLEEPNET", "code_stars": 0, "code_last_update": "2026-01-19", "AI": {"tldr": "ULW-SleepNet\uff1a\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u7761\u7720\u9636\u6bb5\u8bc4\u5206\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u9ad8\u7761\u7720\u76d1\u6d4b\u5b9e\u7528\u6027\u3002", "motivation": "\u81ea\u52a8\u7761\u7720\u9636\u6bb5\u8bc4\u5206\u5bf9\u4e8e\u7761\u7720\u969c\u788d\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7761\u7720\u7814\u7a76\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8bb8\u591a\u73b0\u6709\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u4e13\u4e3a\u5355\u901a\u9053\u8111\u7535\u56fe\uff08EEG\uff09\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u591a\u6a21\u5f0f\u7761\u7720\u76d1\u6d4b\uff08PSG\uff09\u6570\u636e\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faULW-SleepNet\uff0c\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u7761\u7720\u9636\u6bb5\u8bc4\u5206\u6846\u67b6\uff0c\u6709\u6548\u5730\u6574\u5408\u4e86\u6765\u81ea\u591a\u4e2a\u751f\u7406\u4fe1\u53f7\u7684\u4fe1\u606f\u3002ULW-SleepNet\u91c7\u7528\u4e86\u65b0\u9896\u7684Dual-Stream Separable Convolution (DSSC)\u5757\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u901a\u9053\u53c2\u6570\u5171\u4eab\u548c\u5168\u5c40\u5e73\u5747\u6c60\u5316\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "result": "\u5728Sleep-EDF-20\u548cSleep-EDF-78\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cULW-SleepNet\u5206\u522b\u8fbe\u523086.9%\u548c81.4%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u752813.3K\u53c2\u6570\u548c7.89M FLOPs\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e86\u9ad8\u8fbe98.6%\uff0c\u6027\u80fd\u635f\u5931\u5fae\u4e4e\u5176\u5fae\uff0c\u663e\u793a\u51fa\u5176\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u5b9e\u65f6\u7761\u7720\u76d1\u6d4b\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "ULW-SleepNet\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u7761\u7720\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23893", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23893", "abs": "https://arxiv.org/abs/2602.23893", "authors": ["Bowen Yang", "Zishuo Li", "Yang Sun", "Changtao Miao", "Yifan Yang", "Man Luo", "Xiaotong Yan", "Feng Jiang", "Jinchuan Shi", "Yankai Fu", "Ning Chen", "Junkai Zhao", "Pengwei Wang", "Guocai Yao", "Shanghang Zhang", "Hao Chen", "Zhe Li", "Kai Zhu"], "title": "AoE: Always-on Egocentric Human Video Collection for Embodied AI", "comment": null, "summary": "Embodied foundation models require large-scale, high-quality real-world interaction data for pre-training and scaling. However, existing data collection methods suffer from high infrastructure costs, complex hardware dependencies, and limited interaction scope, making scalable expansion challenging. In fact, humans themselves are ideal physically embodied agents. Therefore, obtaining egocentric real-world interaction data from globally distributed \"human agents\" offers advantages of low cost and sustainability. To this end, we propose the Always-on Egocentric (AoE) data collection system, which aims to simplify hardware dependencies by leveraging humans themselves and their smartphones, enabling low-cost, highly efficient, and scene-agnostic real-world interaction data collection to address the challenge of data scarcity. Specifically, we first employ an ergonomic neck-mounted smartphone holder to enable low-barrier, large-scale egocentric data collection through a cloud-edge collaborative architecture. Second, we develop a cross-platform mobile APP that leverages on-device compute for real-time processing, while the cloud hosts automated labeling and filtering pipelines that transform raw videos into high-quality training data. Finally, the AoE system supports distributed Ego video data collection by anyone, anytime, and anywhere. We evaluate AoE on data preprocessing quality and downstream tasks, demonstrating that high-quality egocentric data significantly boosts real-world generalization.", "AI": {"tldr": "AoE\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u548c\u667a\u80fd\u624b\u673a\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u4e14\u573a\u666f\u65e0\u5173\u7684\u4ea4\u4e92\u6570\u636e\u6536\u96c6\uff0c\u663e\u8457\u63d0\u5347\u5177\u8eab\u57fa\u7840\u6a21\u578b\u7684\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Embodied foundation models require large-scale, high-quality real-world interaction data for pre-training and scaling.", "method": "Propose the Always-on Egocentric (AoE) data collection system, leveraging humans and their smartphones.", "result": "AoE enables low-cost, highly efficient, and scene-agnostic real-world interaction data collection.", "conclusion": "AoE significantly boosts real-world generalization in embodied foundation models."}}
{"id": "2602.23898", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23898", "abs": "https://arxiv.org/abs/2602.23898", "authors": ["Qihua Dong", "Kuo Yang", "Lin Ju", "Handong Zhao", "Yitian Zhang", "Yizhou Wang", "Huimin Zeng", "Jianglin Lu", "Yun Fu"], "title": "Ref-Adv: Exploring MLLM Visual Reasoning in Referring Expression Tasks", "comment": "ICLR 2026", "summary": "Referring Expression Comprehension (REC) links language to region level visual perception. Standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) have progressed rapidly with multimodal LLMs but remain weak tests of visual reasoning and grounding: (i) many expressions are very short, leaving little reasoning demand; (ii) images often contain few distractors, making the target easy to find; and (iii) redundant descriptors enable shortcut solutions that bypass genuine text understanding and visual reasoning. We introduce Ref-Adv, a modern REC benchmark that suppresses shortcuts by pairing linguistically nontrivial expressions with only the information necessary to uniquely identify the target. The dataset contains referring expressions on real images, curated with hard distractors and annotated with reasoning facets including negation. We conduct comprehensive ablations (word order perturbations and descriptor deletion sufficiency) to show that solving Ref-Adv requires reasoning beyond simple cues, and we evaluate a broad suite of contemporary multimodal LLMs on Ref-Adv. Despite strong results on RefCOCO, RefCOCO+, and RefCOCOg, models drop markedly on Ref-Adv, revealing reliance on shortcuts and gaps in visual reasoning and grounding. We provide an in depth failure analysis and aim for Ref-Adv to guide future work on visual reasoning and grounding in MLLMs.", "AI": {"tldr": "Ref-Adv\u662f\u4e00\u4e2a\u65b0\u7684\u6307\u4ee3\u8868\u8fbe\u5f0f\u57fa\u51c6\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u89c9\u63a8\u7406\u548c\u5b9a\u4f4d\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8bed\u8a00\u4e0e\u89c6\u89c9\u611f\u77e5\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6807\u51c6\u57fa\u51c6\u5728\u89c6\u89c9\u63a8\u7406\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165Ref-Adv\uff0c\u4e00\u4e2a\u73b0\u4ee3\u7684\u6307\u4ee3\u8868\u8fbe\u5f0f\u57fa\u51c6\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u4e0a\u975e\u5e73\u51e1\u7684\u6307\u4ee3\u8868\u8fbe\u5f0f\u4e0e\u552f\u4e00\u8bc6\u522b\u76ee\u6807\u6240\u9700\u7684\u4fe1\u606f\u914d\u5bf9\u6765\u6291\u5236\u6377\u5f84\u3002", "result": "\u5728RefCOCO\u3001RefCOCO+\u548cRefCOCOg\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728Ref-Adv\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\u4ee5\u53ca\u89c6\u89c9\u63a8\u7406\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "Ref-Adv\u4e3a\u672a\u6765\u5728\u591a\u6a21\u6001LLMs\u4e2d\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u548c\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.23880", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23880", "abs": "https://arxiv.org/abs/2602.23880", "authors": ["Zhang Wan", "Tingting Mu", "Samuel Kaski"], "title": "A Theory of Random Graph Shift in Truncated-Spectrum vRKHS", "comment": null, "summary": "This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u56fe\u751f\u6210\u6a21\u578b\u7684\u7406\u8bba\uff0c\u7528\u4e8e\u5206\u6790\u56fe\u5206\u7c7b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u9488\u5bf9\u56fe\u6837\u672c\u7684\u7ed3\u6784\u4fe1\u606f\u4ee5\u53ca\u56fe\u5206\u5e03\u504f\u79fb\u7684\u7cbe\u7ec6\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u56fe\u751f\u6210\u6a21\u578b\u7684\u7406\u8bba\u3002", "method": "\u901a\u8fc7\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08vRKHS\uff09\u516c\u5f0f\u7684\u63a8\u5bfc\uff0c\u5f97\u5230\u4e00\u4e2a\u6cdb\u5316\u754c\uff0c\u5176\u4e2d\u504f\u79fb\u60e9\u7f5a\u53ef\u4ee5\u5206\u89e3\u4e3a\u9886\u57df\u5dee\u5f02\u9879\u3001\u5149\u8c31\u51e0\u4f55\u9879\u548c\u632f\u5e45\u9879\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u548c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u56fe\u5206\u7c7b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23894", "abs": "https://arxiv.org/abs/2602.23894", "authors": ["Xavier Timoneda", "Markus Herb", "Fabian Duerr", "Daniel Goehring"], "title": "SelfOccFlow: Towards end-to-end self-supervised 3D Occupancy Flow prediction", "comment": "Accepted version. Final version is published in IEEE Robotics and Automation Letters, DOI: 10.1109/LRA.2026.3665447", "summary": "Estimating 3D occupancy and motion at the vehicle's surroundings is essential for autonomous driving, enabling situational awareness in dynamic environments. Existing approaches jointly learn geometry and motion but rely on expensive 3D occupancy and flow annotations, velocity labels from bounding boxes, or pretrained optical flow models. We propose a self-supervised method for 3D occupancy flow estimation that eliminates the need for human-produced annotations or external flow supervision. Our method disentangles the scene into separate static and dynamic signed distance fields and learns motion implicitly through temporal aggregation. Additionally, we introduce a strong self-supervised flow cue derived from features' cosine similarities. We demonstrate the efficacy of our 3D occupancy flow method on SemanticKITTI, KITTI-MOT, and nuScenes.", "AI": {"tldr": "This paper proposes a self-supervised 3D occupancy flow estimation method, achieving promising results without human annotations.", "motivation": "Estimating 3D occupancy and motion at the vehicle's surroundings is essential for autonomous driving.", "method": "Proposed a self-supervised method for 3D occupancy flow estimation.", "result": "Demonstrated the efficacy of the method on SemanticKITTI, KITTI-MOT, and nuScenes.", "conclusion": "The method eliminates the need for human-produced annotations or external flow supervision and shows promising results in 3D occupancy flow estimation."}}
{"id": "2602.24040", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna P\u00e1sztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.", "code_url": "https://github.com/lasgroup/rewarduq", "code_stars": 12, "code_last_update": "2026-02-21", "AI": {"tldr": "This paper introduces a framework for evaluating uncertainty quantification in reward models, with a focus on improving LLM performance.", "motivation": "Reward models are central to aligning large language models (LLMs) with human preferences, but most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback.", "method": "This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison.", "result": "Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. We release our open-source framework as a Python package.", "conclusion": "RewardUQ provides a systematic way to evaluate uncertainty quantification for reward models, which helps to improve the performance of LLMs aligned with human preferences."}}
{"id": "2602.23899", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23899", "abs": "https://arxiv.org/abs/2602.23899", "authors": ["Pramit Saha", "Mohammad Alsharid", "Joshua Strong", "J. Alison Noble"], "title": "Experience-Guided Self-Adaptive Cascaded Agents for Breast Cancer Screening and Diagnosis with Reduced Biopsy Referrals", "comment": null, "summary": "We propose an experience-guided cascaded multi-agent framework for Breast Ultrasound Screening and Diagnosis, called BUSD-Agent, that aims to reduce diagnostic escalation and unnecessary biopsy referrals. Our framework models screening and diagnosis as a two-stage, selective decision-making process. A lightweight `screening clinic' agent, restricted to classification models as tools, selectively filters out benign and normal cases from further diagnostic escalation when malignancy risk and uncertainty are estimated as low. Cases that have higher risks are escalated to the `diagnostic clinic' agent, which integrates richer perception and radiological description tools to make a secondary decision on biopsy referral. To improve agent performance, past records of pathology-confirmed outcomes along with image embeddings, model predictions, and historical agent actions are stored in a memory bank as structured decision trajectories. For each new case, BUSD-Agent retrieves similar past cases based on image, model response and confidence similarity to condition the agent's current decision policy. This enables retrieval-conditioned in-context adaptation that dynamically adjusts model trust and escalation thresholds from prior experiences without parameter updates. Evaluation across 10 breast ultrasound datasets shows that the proposed experience-guided workflow reduces diagnostic escalation in BUSD-Agent from 84.95% to 58.72% and overall biopsy referrals from 59.50% to 37.08%, compared to the same architecture without trajectory conditioning, while improving average screening specificity by 68.48% and diagnostic specificity by 6.33%.", "AI": {"tldr": "BUSD-Agent\uff1a\u4e00\u79cd\u57fa\u4e8e\u7ecf\u9a8c\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6709\u6548\u964d\u4f4e\u8bca\u65ad\u5347\u7ea7\u548c\u6d3b\u68c0\u8f6c\u8bca\uff0c\u63d0\u9ad8\u7b5b\u67e5\u548c\u8bca\u65ad\u7684\u7279\u5f02\u6027", "motivation": "\u964d\u4f4e\u8bca\u65ad\u5347\u7ea7\u548c\u4e0d\u5fc5\u8981\u7684\u6d3b\u68c0\u8f6c\u8bca", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aBUSD-Agent\u7684\u7ecf\u9a8c\u5f15\u5bfc\u7684\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u7b5b\u67e5\u548c\u8bca\u65ad\u5efa\u6a21\u4e3a\u4e24\u9636\u6bb5\u9009\u62e9\u6027\u51b3\u7b56\u8fc7\u7a0b", "result": "\u4e0e\u672a\u8fdb\u884c\u8f68\u8ff9\u6761\u4ef6\u5316\u7684\u76f8\u540c\u67b6\u6784\u76f8\u6bd4\uff0c\u7ecf\u9a8c\u5f15\u5bfc\u7684\u5de5\u4f5c\u6d41\u7a0b\u5c06BUSD-Agent\u7684\u8bca\u65ad\u5347\u7ea7\u4ece84.95%\u964d\u4f4e\u523058.72%\uff0c\u5c06\u6574\u4f53\u6d3b\u68c0\u8f6c\u8bca\u4ece59.50%\u964d\u4f4e\u523037.08%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5e73\u5747\u7b5b\u67e5\u7279\u5f02\u602768.48%\u548c\u8bca\u65ad\u7279\u5f02\u60276.33%", "conclusion": "BUSD-Agent\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8bca\u65ad\u5347\u7ea7\u548c\u6d3b\u68c0\u8f6c\u8bca\uff0c\u63d0\u9ad8\u7b5b\u67e5\u548c\u8bca\u65ad\u7684\u7279\u5f02\u6027"}}
{"id": "2602.23903", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23903", "abs": "https://arxiv.org/abs/2602.23903", "authors": ["Andrei-Alexandru Bunea", "Dan-Matei Popovici", "Radu Tudor Ionescu"], "title": "SegMate: Asymmetric Attention-Based Lightweight Architecture for Efficient Multi-Organ Segmentation", "comment": null, "summary": "State-of-the-art models for medical image segmentation achieve excellent accuracy but require substantial computational resources, limiting deployment in resource-constrained clinical settings. We present SegMate, an efficient 2.5D framework that achieves state-of-the-art accuracy, while considerably reducing computational requirements. Our efficient design is the result of meticulously integrating asymmetric architectures, attention mechanisms, multi-scale feature fusion, slice-based positional conditioning, and multi-task optimization. We demonstrate the efficiency-accuracy trade-off of our framework across three modern backbones (EfficientNetV2-M, MambaOut-Tiny, FastViT-T12). We perform experiments on three datasets: TotalSegmentator, SegTHOR and AMOS22. Compared with the vanilla models, SegMate reduces computation (GFLOPs) by up to 2.5x and memory footprint (VRAM) by up to 2.1x, while generally registering performance gains of around 1%. On TotalSegmentator, we achieve a Dice score of 93.51% with only 295MB peak GPU memory. Zero-shot cross-dataset evaluations on SegTHOR and AMOS22 demonstrate strong generalization, with Dice scores of up to 86.85% and 89.35%, respectively. We release our open-source code at https://github.com/andreibunea99/SegMate.", "code_url": "https://github.com/andreibunea99/SegMate", "code_stars": 0, "code_last_update": "2026-02-27", "AI": {"tldr": "SegMate\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b0\u578b\u533b\u7597\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u7684\u533b\u7597\u56fe\u50cf\u5206\u5272\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u4f46\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5176\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSegMate\u7684\u9ad8\u65482.5D\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6574\u5408\u975e\u5bf9\u79f0\u67b6\u6784\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3001\u57fa\u4e8e\u5207\u7247\u7684\u4f4d\u7f6e\u6761\u4ef6\u548c\u591a\u4efb\u52a1\u4f18\u5316\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08TotalSegmentator\u3001SegTHOR\u548cAMOS22\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u666e\u901a\u6a21\u578b\u76f8\u6bd4\uff0cSegMate\u5c06\u8ba1\u7b97\uff08GFLOPs\uff09\u51cf\u5c11\u4e86\u9ad8\u8fbe2.5\u500d\uff0c\u5185\u5b58\u5360\u7528\uff08VRAM\uff09\u51cf\u5c11\u4e86\u9ad8\u8fbe2.1\u500d\uff0c\u540c\u65f6\u901a\u5e38\u5c06\u6027\u80fd\u63d0\u5347\u7ea61%\u3002\u5728TotalSegmentator\u4e0a\uff0cSegMate\u5b9e\u73b0\u4e8693.51%\u7684Dice\u5206\u6570\uff0c\u540c\u65f6\u53ea\u4f7f\u7528\u4e86295MB\u7684\u5cf0\u503cGPU\u5185\u5b58\u3002\u5728SegTHOR\u548cAMOS22\u4e0a\u7684\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5206\u522b\u8fbe\u5230\u4e8686.85%\u548c89.35%\u7684Dice\u5206\u6570\u3002", "conclusion": "SegMate\u6846\u67b6\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23906", "abs": "https://arxiv.org/abs/2602.23906", "authors": ["Bora Kargi", "Arnas Uselis", "Seong Joon Oh"], "title": "Half-Truths Break Similarity-Based Retrieval", "comment": null, "summary": "When a text description is extended with an additional detail, image-text similarity should drop if that detail is wrong. We show that CLIP-style dual encoders often violate this intuition: appending a plausible but incorrect object or relation to an otherwise correct description can increase the similarity score. We call such cases half-truths. On COCO, CLIP prefers the correct shorter description only 40.6% of the time, and performance drops to 32.9% when the added detail is a relation. We trace this vulnerability to weak supervision on caption parts: contrastive training aligns full sentences but does not explicitly enforce that individual entities and relations are grounded. We propose CS-CLIP (Component-Supervised CLIP), which decomposes captions into entity and relation units, constructs a minimally edited foil for each unit, and fine-tunes the model to score the correct unit above its foil while preserving standard dual-encoder inference. CS-CLIP raises half-truth accuracy to 69.3% and improves average performance on established compositional benchmarks by 5.7 points, suggesting that reducing half-truth errors aligns with broader gains in compositional understanding. Code is publicly available at: https://github.com/kargibora/CS-CLIP", "code_url": "https://github.com/kargibora/CS-CLIP", "code_stars": 2, "code_last_update": "2026-02-27", "AI": {"tldr": "\u63d0\u51faCS-CLIP\uff0c\u901a\u8fc7\u7ec4\u4ef6\u76d1\u7763\u548c\u5bf9\u7167\u6837\u672c\uff0c\u63d0\u9ad8CLIP-style\u53cc\u7f16\u7801\u5668\u5bf9\u534a\u771f\u63cf\u8ff0\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3CLIP-style\u53cc\u7f16\u7801\u5668\u5728\u6587\u672c\u63cf\u8ff0\u4e2d\u52a0\u5165\u9519\u8bef\u7ec6\u8282\u65f6\uff0c\u76f8\u4f3c\u5ea6\u5f97\u5206\u53cd\u800c\u63d0\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCS-CLIP\uff08\u7ec4\u4ef6\u76d1\u7763CLIP\uff09\uff0c\u5c06\u63cf\u8ff0\u5206\u89e3\u4e3a\u5b9e\u4f53\u548c\u5173\u7cfb\u5355\u5143\uff0c\u4e3a\u6bcf\u4e2a\u5355\u5143\u6784\u5efa\u6700\u5c0f\u7f16\u8f91\u7684\u5bf9\u7167\u6837\u672c\uff0c\u5e76\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u6b63\u786e\u5355\u5143\u7684\u5f97\u5206\u9ad8\u4e8e\u5bf9\u7167\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u53cc\u7f16\u7801\u5668\u63a8\u7406\u3002", "result": "CS-CLIP\u5c06\u534a\u771f\u7387\u63d0\u9ad8\u523069.3%\uff0c\u5e76\u5c06\u73b0\u6709\u7ec4\u5408\u57fa\u51c6\u7684\u5e73\u5747\u6027\u80fd\u63d0\u9ad8\u4e865.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u51cf\u5c11\u534a\u771f\u9519\u8bef\u4e0e\u66f4\u5e7f\u6cdb\u7684\u7ec4\u5408\u7406\u89e3\u6536\u76ca\u76f8\u4e00\u81f4\u3002"}}
{"id": "2602.24134", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24134", "abs": "https://arxiv.org/abs/2602.24134", "authors": ["Zhengren Wang", "Dongsheng Ma", "Huaping Zhong", "Jiayu Li", "Wentao Zhang", "Bin Wang", "Conghui He"], "title": "AgenticOCR: Parsing Only What You Need for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "The expansion of retrieval-augmented generation (RAG) into multimodal domains has intensified the challenge for processing complex visual documents, such as financial reports. While page-level chunking and retrieval is a natural starting point, it creates a critical bottleneck: delivering entire pages to the generator introduces excessive extraneous context. This not only overloads the generator's attention mechanism but also dilutes the most salient evidence. Moreover, compressing these information-rich pages into a limited visual token budget further increases the risk of hallucinations. To address this, we introduce AgenticOCR, a dynamic parsing paradigm that transforms optical character recognition (OCR) from a static, full-text process into a query-driven, on-demand extraction system. By autonomously analyzing document layout in a \"thinking with images\" manner, AgenticOCR identifies and selectively recognizes regions of interest. This approach performs on-demand decompression of visual tokens precisely where needed, effectively decoupling retrieval granularity from rigid page-level chunking. AgenticOCR has the potential to serve as the \"third building block\" of the visual document RAG stack, operating alongside and enhancing standard Embedding and Reranking modules. Experimental results demonstrate that AgenticOCR improves both the efficiency and accuracy of visual RAG systems, achieving expert-level performance in long document understanding. Code and models are available at https://github.com/OpenDataLab/AgenticOCR.", "code_url": "https://github.com/OpenDataLab/AgenticOCR", "AI": {"tldr": "AgenticOCR\u63d0\u9ad8\u89c6\u89c9RAG\u7cfb\u7edf\u6027\u80fd", "motivation": "\u5904\u7406\u590d\u6742\u89c6\u89c9\u6587\u6863\uff08\u5982\u8d22\u52a1\u62a5\u544a\uff09\u7684\u6311\u6218", "method": "\u5f15\u5165AgenticOCR\uff0c\u52a8\u6001\u89e3\u6790\u8303\u5f0f\uff0c\u5c06OCR\u4ece\u9759\u6001\u5168\u6587\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u67e5\u8be2\u9a71\u52a8\u7684\u6309\u9700\u63d0\u53d6\u7cfb\u7edf", "result": "\u63d0\u9ad8\u89c6\u89c9RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u957f\u6587\u6863\u7406\u89e3\u65b9\u9762\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd", "conclusion": "AgenticOCR\u4f5c\u4e3a\u89c6\u89c9\u6587\u6863RAG\u5806\u6808\u7684\u201c\u7b2c\u4e09\u5757\u57fa\u77f3\u201d\uff0c\u80fd\u591f\u6709\u6548\u89e3\u8026\u68c0\u7d22\u7c92\u5ea6\u4e0e\u521a\u6027\u9875\u9762\u7ea7\u5206\u5757"}}
{"id": "2602.23994", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.", "AI": {"tldr": "This paper proposes a foundation world model for efficient, reliable, and adaptable autonomous agents.", "motivation": "Next generation of autonomous agents need to learn efficiently, act reliably, and adapt in open worlds.", "method": "Proposing a foundation world model with four components: learnable reward models, adaptive formal verification, online abstraction calibration, and test-time synthesis and world-model generation.", "result": "Framework enables agents to synthesize verifiable programs, derive new policies, and adapt to novelty while maintaining correctness.", "conclusion": "Foundation world models are a substrate for learning, reasoning, and adaptation, leading to agents that can explain and justify their behavior."}}
{"id": "2602.23926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23926", "abs": "https://arxiv.org/abs/2602.23926", "authors": ["Qiyu Feng", "Jiwei Shan", "Shing Shin Cheng", "Hesheng Wang"], "title": "Leveraging Geometric Prior Uncertainty and Complementary Constraints for High-Fidelity Neural Indoor Surface Reconstruction", "comment": "Accepted by ICRA 2026", "summary": "Neural implicit surface reconstruction with signed distance function has made significant progress, but recovering fine details such as thin structures and complex geometries remains challenging due to unreliable or noisy geometric priors. Existing approaches rely on implicit uncertainty that arises during optimization to filter these priors, which is indirect and inefficient, and masking supervision in high-uncertainty regions further leads to under-constrained optimization. To address these issues, we propose GPU-SDF, a neural implicit framework for indoor surface reconstruction that leverages geometric prior uncertainty and complementary constraints. We introduce a self-supervised module that explicitly estimates prior uncertainty without auxiliary networks. Based on this estimation, we design an uncertainty-guided loss that modulates prior influence rather than discarding it, thereby retaining weak but informative cues. To address regions with high prior uncertainty, GPU-SDF further incorporates two complementary constraints: an edge distance field that strengthens boundary supervision and a multi-view consistency regularization that enforces geometric coherence. Extensive experiments confirm that GPU-SDF improves the reconstruction of fine details and serves as a plug-and-play enhancement for existing frameworks. Source code will be available at https://github.com/IRMVLab/GPU-SDF", "code_url": "https://github.com/IRMVLab/GPU-SDF", "code_stars": 0, "code_last_update": "2026-02-27", "AI": {"tldr": "GPU-SDF\uff1a\u4e00\u79cd\u7528\u4e8e\u5ba4\u5185\u8868\u9762\u91cd\u5efa\u7684\u795e\u7ecf\u9690\u5f0f\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u7ec6\u5c0f\u7ed3\u6784\u7684\u91cd\u5efa\u8d28\u91cf", "motivation": "\u89e3\u51b3\u7531\u4e8e\u4e0d\u53ef\u9760\u6216\u566a\u58f0\u7684\u51e0\u4f55\u5148\u9a8c\u800c\u96be\u4ee5\u6062\u590d\u7ec6\u5c0f\u7ed3\u6784\u7b49\u7ec6\u8282\u7684\u95ee\u9898", "method": "\u63d0\u51faGPU-SDF\uff0c\u4e00\u4e2a\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u4e0d\u786e\u5b9a\u6027\u548c\u4e92\u8865\u7ea6\u675f\u7684\u795e\u7ecf\u9690\u5f0f\u6846\u67b6", "result": "\u5b9e\u9a8c\u8868\u660eGPU-SDF\u63d0\u9ad8\u4e86\u7ec6\u5c0f\u7ed3\u6784\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u6846\u67b6\u7684\u5373\u63d2\u5373\u7528\u589e\u5f3a", "conclusion": "GPU-SDF\u662f\u4e00\u79cd\u6709\u6548\u7684\u5ba4\u5185\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u7ec6\u5c0f\u7ed3\u6784\u7684\u91cd\u5efa\u8d28\u91cf"}}
{"id": "2602.24283", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24283", "abs": "https://arxiv.org/abs/2602.24283", "authors": ["Zhengbo Wang", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation", "comment": "Camera-ready version. Accepted as Oral at ICLR 2026", "summary": "Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre.", "code_url": "https://github.com/mrflogs/LoRA-Pre", "code_stars": 6, "code_last_update": "2026-02-26", "AI": {"tldr": "LoRA-Pre\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u79e9\u4f18\u5316\u5668\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u4f18\u5316\u5668\u5982Adam\u548cMuon\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u5bf9\u4e00\u9636\u548c\u4e8c\u9636\u52a8\u91cf\u7684\u4f9d\u8d56\u5f15\u5165\u4e86\u663e\u8457\u7684\u5185\u5b58\u5f00\u9500\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5c06\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u7ebf\u68af\u5ea6\u6d41\u4e2d\u7ebf\u6027\u56de\u5f52\u5668\u7684\u8bad\u7ec3\uff0c\u5e76\u57fa\u4e8e\u6b64\u7b49\u4ef7\u6027\uff0c\u5f15\u5165\u4e86LoRA-Pre\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3a\u9ad8\u6548\u9884\u8bad\u7ec3\u8bbe\u8ba1\u7684\u4f4e\u79e9\u4f18\u5316\u5668\u3002LoRA-Pre\u901a\u8fc7\u5c06\u5b8c\u6574\u52a8\u91cf\u77e9\u9635\u5206\u89e3\u4e3a\u5728\u7ebf\u7ebf\u6027\u5b66\u4e60\u8005\u4e2d\u7684\u7d27\u51d1\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u4f18\u5316\u5668\u7684\u5185\u5b58\u5360\u7528\u3002", "result": "LoRA-Pre\u5728\u4eceLlama\u67b6\u6784\u5bb6\u65cf\u768460M\u52301B\u53c2\u6570\u7684\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u6240\u6709\u6a21\u578b\u5c3a\u5bf8\u7684\u6700\u9ad8\u6027\u80fd\u3002\u5728\u5fae\u8c03\u573a\u666f\u4e2d\uff0cLoRA-Pre\u5728\u76f8\u540c\u7684\u79e9\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u9ad8\u6548\u7684\u5fae\u8c03\u57fa\u7ebf\u3002\u4e0e\u6807\u51c6LoRA\u76f8\u6bd4\uff0cLoRA-Pre\u5728Llama-3.1-8B\u4e0a\u5b9e\u73b0\u4e863.14\u4e2a\u767e\u5206\u70b9\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728Llama-2-7B\u4e0a\u5b9e\u73b0\u4e866.17\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\u3002", "conclusion": "LoRA-Pre\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u79e9\u4f18\u5316\u5668\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6548\u7387\u3002"}}
{"id": "2602.24012", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.24012", "abs": "https://arxiv.org/abs/2602.24012", "authors": ["Roy Betser", "Eyal Gofer", "Meir Yossef Levi", "Guy Gilboa"], "title": "InfoNCE Induces Gaussian Distribution", "comment": "Accepted to ICLR 2026, Oral", "summary": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.", "AI": {"tldr": "This paper analyzes the Gaussian structure in contrastive learning representations and its implications for analysis and applications.", "motivation": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models.", "method": "Analysis of InfoNCE objective in contrastive training, establishing Gaussian structure in representations, and conducting experiments on synthetic and CIFAR-10 datasets.", "result": "Demonstrating consistent Gaussian behavior in representations, providing a principled explanation for Gaussianity in contrastive representations, and enabling principled analytical treatment of learned representations.", "conclusion": "The Gaussian model in contrastive learning enables principled analytical treatment of learned representations and supports a wide range of applications."}}
{"id": "2602.23945", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.23945", "abs": "https://arxiv.org/abs/2602.23945", "authors": ["Dongxu Zhang", "Yiding Sun", "Pengcheng Li", "Yumou Liu", "Hongqiang Lin", "Haoran Xu", "Xiaoxuan Mu", "Liang Lin", "Wenbiao Yan", "Ning Yang", "Chaowei Fang", "Juanjuan Zhao", "Jihua Zhu", "Conghui He", "Cheng Tan"], "title": "PointCoT: A Multi-modal Benchmark for Explicit 3D Geometric Reasoning", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) demonstrate proficiency in 2D scenes, extending their perceptual intelligence to 3D point cloud understanding remains a significant challenge. Current approaches focus primarily on aligning 3D features with pre-trained models. However, they typically treat geometric reasoning as an implicit mapping process. These methods bypass intermediate logical steps and consequently suffer from geometric hallucinations. They confidently generate plausible responses that fail to ground in precise structural details. To bridge this gap, we present PointCoT, a novel framework that empowers MLLMs with explicit Chain-of-Thought (CoT) reasoning for 3D data. We advocate for a \\textit{Look, Think, then Answer} paradigm. In this approach, the model is supervised to generate geometry-grounded rationales before predicting final answers. To facilitate this, we construct Point-Reason-Instruct, a large-scale benchmark comprising $\\sim$86k instruction-tuning samples with hierarchical CoT annotations. By leveraging a dual-stream multi-modal architecture, our method synergizes semantic appearance with geometric truth. Extensive experiments demonstrate that PointCoT achieves state-of-the-art performance on complex reasoning tasks.", "AI": {"tldr": "PointCoT\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u589e\u5f3aMLLMs\u76843D\u70b9\u4e91\u7406\u89e3\u80fd\u529b\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "While Multimodal Large Language Models (MLLMs) demonstrate proficiency in 2D scenes, extending their perceptual intelligence to 3D point cloud understanding remains a significant challenge.", "method": "We present PointCoT, a novel framework that empowers MLLMs with explicit Chain-of-Thought (CoT) reasoning for 3D data. We advocate for a \\textit{Look, Think, then Answer} paradigm. In this approach, the model is supervised to generate geometry-grounded rationales before predicting final answers. To facilitate this, we construct Point-Reason-Instruct, a large-scale benchmark comprising $\\sim$86k instruction-tuning samples with hierarchical CoT annotations. By leveraging a dual-stream multi-modal architecture, our method synergizes semantic appearance with geometric truth.", "result": "Extensive experiments demonstrate that PointCoT achieves state-of-the-art performance on complex reasoning tasks.", "conclusion": "PointCoT is a novel framework that enhances MLLMs' 3D point cloud understanding through explicit Chain-of-Thought (CoT) reasoning, achieving state-of-the-art performance on complex reasoning tasks."}}
{"id": "2602.24066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24066", "abs": "https://arxiv.org/abs/2602.24066", "authors": ["Tobias Nygaard"], "title": "pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures", "comment": null, "summary": "Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.", "AI": {"tldr": "pathsig\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u7b7e\u540d\u5e93\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u5e93\u5728\u5927\u578b\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898", "method": "\u5f15\u5165pathsig\u5e93\uff0c\u4f7f\u7528CUDA\u5185\u6838\u5728\u8bcd\u57fa\u7840\u4e0a\u76f4\u63a5\u8ba1\u7b97\u8def\u5f84\u7b7e\u540d", "result": "pathsig\u5728\u8ba1\u7b97\u622a\u65ad\u7b7e\u540d\u65f6\u901f\u5ea6\u6bd4\u5176\u4ed6\u5e93\u5feb10-30\u500d\uff0c\u5728\u9700\u8981\u901a\u8fc7\u7b7e\u540d\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u8bad\u7ec3\u4e2d\u5feb4-10\u500d", "conclusion": "pathsig\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u7b7e\u540d\u5e93\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd"}}
{"id": "2602.24069", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.24069", "abs": "https://arxiv.org/abs/2602.24069", "authors": ["Ryan DeWolfe"], "title": "Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding", "comment": "13 pages, 6 figures", "summary": "Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm.", "AI": {"tldr": "COVE UMAP HDBSCAN pipeline improves clustering and link prediction performance", "motivation": "Leveraging non-linear dimension reduction techniques to remove the low dimension constraint from node embedding", "method": "Proposing COVE, an explainable high dimensional embedding that uses UMAP for low dimension reduction, inspired by neural embedding methods and diffusion process", "result": "Slightly increases performance on clustering and link prediction tasks, similar to the popular Louvain algorithm in community detection", "conclusion": "COVE UMAP HDBSCAN pipeline is effective for node embedding and community detection"}}
{"id": "2602.23952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23952", "abs": "https://arxiv.org/abs/2602.23952", "authors": ["Yuyang Hong", "Jiaqi Gu", "Yujin Lou", "Lubin Fan", "Qi Yang", "Ying Wang", "Kun Ding", "Yue Wu", "Shiming Xiang", "Jieping Ye"], "title": "CC-VQA: Conflict- and Correlation-Aware Method for Mitigating Knowledge Conflict in Knowledge-Based Visual Question Answering", "comment": "Accepted by CVPR2026", "summary": "Knowledge-based visual question answering (KB-VQA) demonstrates significant potential for handling knowledge-intensive tasks. However, conflicts arise between static parametric knowledge in vision language models (VLMs) and dynamically retrieved information due to the static model knowledge from pre-training. The outputs either ignore retrieved contexts or exhibit inconsistent integration with parametric knowledge, posing substantial challenges for KB-VQA. Current knowledge conflict mitigation methods primarily adapted from language-based approaches, focusing on context-level conflicts through engineered prompting strategies or context-aware decoding mechanisms. However, these methods neglect the critical role of visual information in conflicts and suffer from redundant retrieved contexts, which impair accurate conflict identification and effective mitigation. To address these limitations, we propose \\textbf{CC-VQA}: a novel training-free, conflict- and correlation-aware method for KB-VQA. Our method comprises two core components: (1) Vision-Centric Contextual Conflict Reasoning, which performs visual-semantic conflict analysis across internal and external knowledge contexts; and (2) Correlation-Guided Encoding and Decoding, featuring positional encoding compression for low-correlation statements and adaptive decoding using correlation-weighted conflict scoring. Extensive evaluations on E-VQA, InfoSeek, and OK-VQA benchmarks demonstrate that CC-VQA achieves state-of-the-art performance, yielding absolute accuracy improvements of 3.3\\% to 6.4\\% compared to existing methods. Code is available at https://github.com/cqu-student/CC-VQA.", "code_url": "https://github.com/cqu-student/CC-VQA", "code_stars": 1, "code_last_update": "2026-03-02", "AI": {"tldr": "CC-VQA\u662fKB-VQA\u7684\u4e00\u4e2a\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u51b2\u7a81\u7f13\u89e3\u65b9\u6cd5\u5ffd\u7565\u89c6\u89c9\u4fe1\u606f\u5728\u51b2\u7a81\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u906d\u53d7\u5197\u4f59\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u3001\u51b2\u7a81\u548c\u76f8\u5173\u6027\u611f\u77e5\u7684\u65b9\u6cd5CC-VQA\uff0c\u5305\u62ec\u89c6\u89c9\u4e2d\u5fc3\u5316\u4e0a\u4e0b\u6587\u51b2\u7a81\u63a8\u7406\u548c\u76f8\u5173\u6027\u5f15\u5bfc\u7684\u7f16\u7801\u4e0e\u89e3\u7801\u3002", "result": "\u5728E-VQA\u3001InfoSeek\u548cOK-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCC-VQA\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.3%\u81f36.4%\u3002", "conclusion": "CC-VQA\u5728KB-VQA\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.3%\u81f36.4%."}}
{"id": "2602.23953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23953", "abs": "https://arxiv.org/abs/2602.23953", "authors": ["Caner Beldek", "Emre Sariyildiz", "Son Lam Phung", "Gursel Alici"], "title": "GDA-YOLO11: Amodal Instance Segmentation for Occlusion-Robust Robotic Fruit Harvesting", "comment": "9 pages, journal pre-print", "summary": "Occlusion remains a critical challenge in robotic fruit harvesting, as undetected or inaccurately localised fruits often results in substantial crop losses. To mitigate this issue, we propose a harvesting framework using a new amodal segmentation model, GDA-YOLO11, which incorporates architectural improvements and an updated asymmetric mask loss. The proposed model is trained on a modified version of a public citrus dataset and evaluated on both the base dataset and occlusion-sensitive subsets with varying occlusion levels. Within the framework, full fruit masks, including invisible regions, are inferred by GDA-YOLO11, and picking points are subsequently estimated using the Euclidean distance transform. These points are then projected into 3D coordinates for robotic harvesting execution. Experiments were conducted using real citrus fruits in a controlled environment simulating occlusion scenarios. Notably, to the best of our knowledge, this study provides the first practical demonstration of amodal instance segmentation in robotic fruit harvesting. GDA-YOLO11 achieves a precision of 0.844, recall of 0.846, mAP@50 of 0.914, and mAP@50:95 of 0.636, outperforming YOLO11n by 5.1%, 1.3%, and 1.0% in precision, mAP@50, and mAP@50:95, respectively. The framework attains harvesting success rates of 92.59%, 85.18%, 48.14%, and 22.22% at zero to high occlusion levels, improving success by 3.5% under medium and high occlusion. These findings demonstrate that GDA-YOLO11 enhances occlusion robust segmentation and streamlines perception-to-action integration, paving the way for more reliable autonomous systems in agriculture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6c34\u679c\u6536\u83b7\u7684\u906e\u6321\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u51cf\u5c11\u519c\u4e1a\u6536\u83b7\u4e2d\u7684\u906e\u6321\u95ee\u9898\u5bfc\u81f4\u7684\u635f\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b0\u6a21\u578bGDA-YOLO11\u7684\u6536\u83b7\u6846\u67b6\uff0c\u5e76\u5728\u53d7\u906e\u6321\u5f71\u54cd\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "GDA-YOLO11\u5728\u7cbe\u5ea6\u3001mAP@50\u548cmAP@50:95\u65b9\u9762\u5747\u4f18\u4e8eYOLO11n\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "GDA-YOLO11\u663e\u8457\u63d0\u9ad8\u4e86\u906e\u6321\u9c81\u68d2\u5206\u5272\uff0c\u5e76\u7b80\u5316\u4e86\u611f\u77e5\u5230\u884c\u52a8\u7684\u96c6\u6210\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u519c\u4e1a\u81ea\u4e3b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.23956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23956", "abs": "https://arxiv.org/abs/2602.23956", "authors": ["Qianxun Xu", "Chenxi Song", "Yujun Cai", "Chi Zhang"], "title": "SwitchCraft: Training-Free Multi-Event Video Generation with Attention Controls", "comment": "CVPR 2026", "summary": "Recent advances in text-to-video diffusion models have enabled high-fidelity and temporally coherent videos synthesis. However, current models are predominantly optimized for single-event generation. When handling multi-event prompts, without explicit temporal grounding, such models often produce blended or collapsed scenes that break the intended narrative. To address this limitation, we present SwitchCraft, a training-free framework for multi-event video generation. Our key insight is that uniform prompt injection across time ignores the correspondence between events and frames. To this end, we introduce Event-Aligned Query Steering (EAQS), which steers frame-level attention to align with relevant event prompts. Furthermore, we propose Auto-Balance Strength Solver (ABSS), which adaptively balances steering strength to preserve temporal consistency and visual fidelity. Extensive experiments demonstrate that SwitchCraft substantially improves prompt alignment, event clarity, and scene consistency compared with existing baselines, offering a simple yet effective solution for multi-event video generation.", "AI": {"tldr": "SwitchCraft\u901a\u8fc7EAQS\u548cABSS\u6539\u8fdb\u4e86\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u591a\u4e8b\u4ef6\u63d0\u793a\u65f6\uff0c\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u95f4\u57fa\u7840\uff0c\u5bfc\u81f4\u573a\u666f\u6df7\u5408\u6216\u5d29\u6e83\uff0c\u7834\u574f\u4e86\u9884\u671f\u7684\u53d9\u4e8b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSwitchCraft\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u3002\u5f15\u5165\u4e86Event-Aligned Query Steering (EAQS)\u548cAuto-Balance Strength Solver (ABSS)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0cSwitchCraft\u5728\u63d0\u793a\u5bf9\u9f50\u3001\u4e8b\u4ef6\u6e05\u6670\u5ea6\u548c\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "SwitchCraft\u4e3a\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23959", "abs": "https://arxiv.org/abs/2602.23959", "authors": ["Kesen Zhao", "Beier Zhu", "Junbao Zhou", "Xingyu Zhu", "Zhongqi Yue", "Hanwang Zhang"], "title": "Thinking with Images as Continuous Actions: Numerical Visual Chain-of-Thought", "comment": null, "summary": "Recent multimodal large language models (MLLMs) increasingly rely on visual chain-of-thought to perform region-grounded reasoning over images. However, existing approaches ground regions via either textified coordinates-causing modality mismatch and semantic fragmentation or fixed-granularity patches that both limit precise region selection and often require non-trivial architectural changes. In this paper, we propose Numerical Visual Chain-of-Thought (NV-CoT), a framework that enables MLLMs to reason over images using continuous numerical coordinates. NV-CoT expands the MLLM action space from discrete vocabulary tokens to a continuous Euclidean space, allowing models to directly generate bounding-box coordinates as actions with only minimal architectural modification. The framework supports both supervised fine-tuning and reinforcement learning. In particular, we replace categorical token policies with a Gaussian (or Laplace) policy over coordinates and introduce stochasticity via reparameterized sampling, making NV-CoT fully compatible with GRPO-style policy optimization. Extensive experiments on three benchmarks against eight representative visual reasoning baselines demonstrate that NV-CoT significantly improves localization precision and final answer accuracy, while also accelerating training convergence, validating the effectiveness of continuous-action visual reasoning in MLLMs. The code is available in https://github.com/kesenzhao/NV-CoT.", "code_url": "https://github.com/kesenzhao/NV-CoT", "AI": {"tldr": "NV-CoT\u901a\u8fc7\u8fde\u7eed\u6570\u503c\u5750\u6807\u63d0\u9ad8MLLM\u56fe\u50cf\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u533a\u57df\u5b9a\u4f4d\u4e0a\u5b58\u5728\u6a21\u6001\u4e0d\u5339\u914d\u548c\u8bed\u4e49\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u4e14\u56fa\u5b9a\u7c92\u5ea6\u7684\u8865\u4e01\u9650\u5236\u4e86\u7cbe\u786e\u7684\u533a\u57df\u9009\u62e9\uff0c\u901a\u5e38\u9700\u8981\u975e\u5e73\u51e1\u7684\u67b6\u6784\u4fee\u6539\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNumerical Visual Chain-of-Thought (NV-CoT)\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7fMLLM\u80fd\u591f\u4f7f\u7528\u8fde\u7eed\u7684\u6570\u503c\u5750\u6807\u5bf9\u56fe\u50cf\u8fdb\u884c\u63a8\u7406\u3002NV-CoT\u5c06MLLM\u7684\u52a8\u4f5c\u7a7a\u95f4\u4ece\u79bb\u6563\u8bcd\u6c47\u6807\u8bb0\u6269\u5c55\u5230\u8fde\u7eed\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u5141\u8bb8\u6a21\u578b\u4ec5\u901a\u8fc7\u6700\u5c0f\u7684\u67b6\u6784\u4fee\u6539\u76f4\u63a5\u751f\u6210\u8fb9\u754c\u6846\u5750\u6807\u4f5c\u4e3a\u52a8\u4f5c\u3002\u8be5\u6846\u67b6\u652f\u6301\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNV-CoT\u4e0e\u516b\u4e2a\u4ee3\u8868\u6027\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u7ebf\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eNV-CoT\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u8fde\u7eed\u52a8\u4f5c\u89c6\u89c9\u63a8\u7406\u5728MLLM\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "NV-CoT\u901a\u8fc7\u4f7f\u7528\u8fde\u7eed\u6570\u503c\u5750\u6807\uff0c\u63d0\u9ad8\u4e86MLLM\u5bf9\u56fe\u50cf\u8fdb\u884c\u63a8\u7406\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24146", "abs": "https://arxiv.org/abs/2602.24146", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Learning with a Budget: Identifying the Best Arm with Resource Constraints", "comment": "A preliminary version of this work, titled 'Best Arm Identification with Resource Constraints,' was presented at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). This manuscript extends the original conference paper by providing improved theoretical results and more generalized conclusions, aiming for future journal submission. arXiv admin note: substantial text overlap with arXiv:2402.19090", "summary": "In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \\textit{effective consumption measure", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23963", "abs": "https://arxiv.org/abs/2602.23963", "authors": ["Qiuyang Zhang", "Jiujun Cheng", "Qichao Mao", "Cong Liu", "Yu Fang", "Yuhong Li", "Mengying Ge", "Shangce Gao"], "title": "SpikeTrack: A Spike-driven Framework for Efficient Visual Tracking", "comment": "Accepted by CVPR2026", "summary": "Spiking Neural Networks (SNNs) promise energy-efficient vision, but applying them to RGB visual tracking remains difficult: Existing SNN tracking frameworks either do not fully align with spike-driven computation or do not fully leverage neurons' spatiotemporal dynamics, leading to a trade-off between efficiency and accuracy. To address this, we introduce SpikeTrack, a spike-driven framework for energy-efficient RGB object tracking. SpikeTrack employs a novel asymmetric design that uses asymmetric timestep expansion and unidirectional information flow, harnessing spatiotemporal dynamics while cutting computation. To ensure effective unidirectional information transfer between branches, we design a memory-retrieval module inspired by neural inference mechanisms. This module recurrently queries a compact memory initialized by the template to retrieve target cues and sharpen target perception over time. Extensive experiments demonstrate that SpikeTrack achieves the state-of-the-art among SNN-based trackers and remains competitive with advanced ANN trackers. Notably, it surpasses TransT on LaSOT dataset while consuming only 1/26 of its energy. To our knowledge, SpikeTrack is the first spike-driven framework to make RGB tracking both accurate and energy efficient. The code and models are available at https://github.com/faicaiwawa/SpikeTrack.", "code_url": "https://github.com/faicaiwawa/SpikeTrack", "code_stars": 0, "code_last_update": "2026-03-02", "AI": {"tldr": "SpikeTrack\uff1a\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u8282\u80fd\u7684SNN RGB\u8ddf\u8e2a\u6846\u67b6\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709SNN\u89c6\u89c9\u8ddf\u8e2a\u6846\u67b6\u7684\u6548\u7387\u4e0e\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpikeTrack\u7684\u65b0\u6846\u67b6\u3002", "method": "SpikeTrack\u91c7\u7528\u65b0\u9896\u7684\u975e\u5bf9\u79f0\u8bbe\u8ba1\uff0c\u5229\u7528\u975e\u5bf9\u79f0\u65f6\u95f4\u6b65\u6269\u5c55\u548c\u5355\u5411\u4fe1\u606f\u6d41\uff0c\u540c\u65f6\u5229\u7528\u795e\u7ecf\u5143\u7684\u7a7a\u95f4\u65f6\u95f4\u52a8\u6001\u7279\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "SpikeTrack\u5728LaSOT\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86TransT\uff0c\u540c\u65f6\u80fd\u8017\u4ec5\u4e3a\u51761/26\uff0c\u5b9e\u73b0\u4e86RGB\u8ddf\u8e2a\u7684\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u80fd\u8017\u3002", "conclusion": "SpikeTrack\u662f\u7b2c\u4e00\u4e2a\u5b9e\u73b0RGB\u8ddf\u8e2a\u65e2\u51c6\u786e\u53c8\u8282\u80fd\u7684SNN\u6846\u67b6\u3002"}}
{"id": "2602.23980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23980", "abs": "https://arxiv.org/abs/2602.23980", "authors": ["Tianxiang Du", "Hulingxiao He", "Yuxin Peng"], "title": "Venus: Benchmarking and Empowering Multimodal Large Language Models for Aesthetic Guidance and Cropping", "comment": "Accepted by CVPR 2026", "summary": "The widespread use of smartphones has made photography ubiquitous, yet a clear gap remains between ordinary users and professional photographers, who can identify aesthetic issues and provide actionable shooting guidance during capture. We define this capability as aesthetic guidance (AG) -- an essential but largely underexplored domain in computational aesthetics. Existing multimodal large language models (MLLMs) primarily offer overly positive feedback, failing to identify issues or provide actionable guidance. Without AG capability, they cannot effectively identify distracting regions or optimize compositional balance, thus also struggling in aesthetic cropping, which aims to refine photo composition through reframing after capture. To address this, we introduce AesGuide, the first large-scale AG dataset and benchmark with 10,748 photos annotated with aesthetic scores, analyses, and guidance. Building upon it, we propose Venus, a two-stage framework that first empowers MLLMs with AG capability through progressively complex aesthetic questions and then activates their aesthetic cropping power via CoT-based rationales. Extensive experiments show that Venus substantially improves AG capability and achieves state-of-the-art (SOTA) performance in aesthetic cropping, enabling interpretable and interactive aesthetic refinement across both stages of photo creation. Code is available at https://github.com/PKU-ICST-MIPL/Venus_CVPR2026.", "code_url": "https://github.com/PKU-ICST-MIPL/Venus_CVPR2026", "code_stars": 0, "code_last_update": "2026-02-22", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24178", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.24178", "abs": "https://arxiv.org/abs/2602.24178", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension", "comment": "30 pages", "summary": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.\n  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.", "AI": {"tldr": "Improved sandwiching polynomial approximators with significant theoretical improvements.", "motivation": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in challenging learning settings.", "method": "A new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions.", "result": "Improved degree bounds for functions of k halfspaces under the Gaussian distribution, doubly exponential improvements for low-dimensional polynomial threshold functions with respect to Gaussians.", "conclusion": "The method is simple and utilizes the smoothness of the target function's boundary, and provides significant improvements over previous bounds."}}
{"id": "2602.23996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23996", "abs": "https://arxiv.org/abs/2602.23996", "authors": ["Kaiwen Zhu", "Quansheng Zeng", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Yi Xin", "Qi Qin", "Jiayang Li", "Yu Qiao", "Jinjin Gu", "Yihao Liu"], "title": "Accelerating Masked Image Generation by Learning Latent Controlled Dynamics", "comment": null, "summary": "Masked Image Generation Models (MIGMs) have achieved great success, yet their efficiency is hampered by the multiple steps of bi-directional attention. In fact, there exists notable redundancy in their computation: when sampling discrete tokens, the rich semantics contained in the continuous features are lost. Some existing works attempt to cache the features to approximate future features. However, they exhibit considerable approximation error under aggressive acceleration rates. We attribute this to their limited expressivity and the failure to account for sampling information. To fill this gap, we propose to learn a lightweight model that incorporates both previous features and sampled tokens, and regresses the average velocity field of feature evolution. The model has moderate complexity that suffices to capture the subtle dynamics while keeping lightweight compared to the original base model. We apply our method, MIGM-Shortcut, to two representative MIGM architectures and tasks. In particular, on the state-of-the-art Lumina-DiMOO, it achieves over 4x acceleration of text-to-image generation while maintaining quality, significantly pushing the Pareto frontier of masked image generation. The code and model weights are available at https://github.com/Kaiwen-Zhu/MIGM-Shortcut.", "code_url": "https://github.com/Kaiwen-Zhu/MIGM-Shortcu", "AI": {"tldr": "\u63d0\u51fa MIGM-Shortcut \u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6548\u7387", "motivation": "\u63d0\u9ad8\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6548\u7387", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4e4b\u524d\u7279\u5f81\u548c\u91c7\u6837\u4ee4\u724c\uff0c\u5e76\u56de\u5f52\u7279\u5f81\u6f14\u5316\u7684\u5e73\u5747\u901f\u5ea6\u573a", "result": "\u5728 Lumina-DiMOO \u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7 4 \u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u63a9\u7801\u56fe\u50cf\u751f\u6210\u7684\u5e15\u7d2f\u6258\u524d\u6cbf", "conclusion": "MIGM-Shortcut \u6a21\u578b\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6548\u7387"}}
{"id": "2602.24182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24182", "abs": "https://arxiv.org/abs/2602.24182", "authors": ["Sikata Sengupta", "Guangyi Liu", "Omer Gottesman", "Joseph W Durham", "Michael Kearns", "Aaron Roth", "Michael Caldara"], "title": "Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers", "comment": null, "summary": "Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMORL\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u5bb9\u5668\u7684\u914d\u9001\u4e2d\u5fc3\u5408\u5e76\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u4f18\u5316\u57fa\u4e8e\u5bb9\u5668\u7684\u914d\u9001\u4e2d\u5fc3\u5408\u5e76\u6d41\u7a0b\uff0c\u9700\u8981\u6743\u8861\u5904\u7406\u901f\u5ea6\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u7a7a\u95f4\u5229\u7528\u7387\u7b49\u7ade\u4e89\u76ee\u6807\uff0c\u540c\u65f6\u9075\u5b88\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u8fd0\u8425\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u6700\u4f73\u53cd\u5e94\u548c\u975e\u540e\u6094\u52a8\u6001\u89e3\u51b3\u7ea6\u675fRL\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u539f\u7406\u6027\u7684\u6700\u5c0f-\u6700\u5927\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u73b0\u5b9e\u4ed3\u5e93\u6a21\u62df\u4e2d\u5bf9\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u6743\u8861\u4e86\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u89c2\u5bdf\u5230\u5b83\u5b66\u4e60\u5230\u4e00\u4e2a\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u7684\u5355\u4e2a\u7b56\u7565\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u89e3\u51b3\u8bef\u5dee\u6d88\u9664\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2a\u62c9\u683c\u6717\u65e5\u503c\u63a5\u8fd1\u6e38\u620f\u6700\u5c0f-\u6700\u5927\u503c\u7684\u5355\u6b21\u8fed\u4ee3\u3002", "conclusion": "MORL\u5728\u89e3\u51b3\u5927\u578b\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u590d\u6742\u3001\u9ad8\u5f71\u54cd\u51b3\u7b56\u95ee\u9898\u65b9\u9762\u5177\u6709\u524d\u666f\u3002"}}
{"id": "2602.24013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24013", "abs": "https://arxiv.org/abs/2602.24013", "authors": ["Gustav Schmidt", "Philipp Berens", "Sarah M\u00fcller"], "title": "Ordinal Diffusion Models for Color Fundus Images", "comment": null, "summary": "It has been suggested that generative image models such as diffusion models can improve performance on clinically relevant tasks by offering deep learning models supplementary training data. However, most conditional diffusion models treat disease stages as independent classes, ignoring the continuous nature of disease progression. This mismatch is problematic in medical imaging because continuous pathological processes are typically only observed through coarse, discrete but ordered labels as in ophthalmology for diabetic retinopathy (DR). We propose an ordinal latent diffusion model for generating color fundus images that explicitly incorporates the ordered structure of DR severity into the generation process. Instead of categorical conditioning, we used a scalar disease representation, enabling a smooth transition between adjacent stages. We evaluated our approach using visual realism metrics and classification-based clinical consistency analysis on the EyePACS dataset. Compared to a standard conditional diffusion model, our model reduced the Fr\u00e9chet inception distance for four of the five DR stages and increased the quadratic weighted $\u03ba$ from 0.79 to 0.87. Furthermore, interpolation experiments showed that the model captured a continuous spectrum of disease progression learned from ordered, coarse class labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5f69\u8272\u773c\u5e95\u56fe\u50cf\uff0c\u63d0\u9ad8\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u6539\u5584\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\u6027\u80fd\uff0c\u901a\u8fc7\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u8865\u5145\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u4e25\u91cd\u7a0b\u5ea6\u7684\u6709\u5e8f\u7ed3\u6784\u663e\u5f0f\u5730\u7eb3\u5165\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u4e0e\u6807\u51c6\u6761\u4ef6\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4e94\u4e2aDR\u9636\u6bb5\u4e2d\u7684\u56db\u4e2a\u9636\u6bb5\u4e0a\u51cf\u5c11\u4e86Fr\u00e9chet inception\u8ddd\u79bb\uff0c\u5e76\u5c06\u4e8c\u6b21\u52a0\u6743\u03ba\u503c\u4ece0.79\u63d0\u9ad8\u52300.87\u3002", "conclusion": "\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5f69\u8272\u773c\u5e95\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6355\u6349\u4ece\u6709\u5e8f\u3001\u7c97\u7565\u7684\u7c7b\u522b\u6807\u7b7e\u4e2d\u5b66\u4e60\u5230\u7684\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u8c31\u3002"}}
{"id": "2602.24201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24201", "abs": "https://arxiv.org/abs/2602.24201", "authors": ["Egor Antipov", "Alessandro Palma", "Lorenzo Consoli", "Stephan G\u00fcnnemann", "Andrea Dittadi", "Fabian J. Theis"], "title": "Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics", "comment": null, "summary": "Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.", "AI": {"tldr": "This paper proposes a new method for efficient density ratio estimation in probabilistic modeling and single-cell genomics data analysis.", "motivation": "Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling.", "method": "Leveraging condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories.", "result": "Competitive performance on simulated benchmarks for closed-form ratio estimation, and versatile tasks in single-cell genomics data analysis.", "conclusion": "The method provides an efficient and versatile approach for density ratio estimation in probabilistic modeling and single-cell genomics data analysis."}}
{"id": "2602.24014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24014", "abs": "https://arxiv.org/abs/2602.24014", "authors": ["Na Min An", "Yoonna Jang", "Yusuke Hirota", "Ryo Hachiuma", "Isabelle Augenstein", "Hyunjung Shim"], "title": "Interpretable Debiasing of Vision-Language Models for Social Fairness", "comment": "25 pages, 30 figures, 13 Tables Accepted to CVPR 2026", "summary": "The rapid advancement of Vision-Language models (VLMs) has raised growing concerns that their black-box reasoning processes could lead to unintended forms of social bias. Current debiasing approaches focus on mitigating surface-level bias signals through post-hoc learning or test-time algorithms, while leaving the internal dynamics of the model largely unexplored. In this work, we introduce an interpretable, model-agnostic bias mitigation framework, DeBiasLens, that localizes social attribute neurons in VLMs through sparse autoencoders (SAEs) applied to multimodal encoders. Building upon the disentanglement ability of SAEs, we train them on facial image or caption datasets without corresponding social attribute labels to uncover neurons highly responsive to specific demographics, including those that are underrepresented. By selectively deactivating the social neurons most strongly tied to bias for each group, we effectively mitigate socially biased behaviors of VLMs without degrading their semantic knowledge. Our research lays the groundwork for future auditing tools, prioritizing social fairness in emerging real-world AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u51cf\u8f7bVLMs\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8bed\u4e49\u77e5\u8bc6", "motivation": "\u5bf9VLMs\u9ed1\u76d2\u63a8\u7406\u8fc7\u7a0b\u7684\u6f5c\u5728\u793e\u4f1a\u504f\u89c1\u8868\u793a\u62c5\u5fe7", "method": "\u63d0\u51faDeBiasLens\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u5b9a\u4f4d\u793e\u4f1a\u5c5e\u6027\u795e\u7ecf\u5143", "result": "\u6709\u6548\u51cf\u8f7bVLMs\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u800c\u4e0d\u635f\u5bb3\u5176\u8bed\u4e49\u77e5\u8bc6", "conclusion": "\u4e3a\u672a\u6765\u5ba1\u8ba1\u5de5\u5177\u5960\u5b9a\u57fa\u7840\uff0c\u4f18\u5148\u8003\u8651AI\u7cfb\u7edf\u4e2d\u7684\u793e\u4f1a\u516c\u5e73\u6027"}}
{"id": "2602.24020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24020", "abs": "https://arxiv.org/abs/2602.24020", "authors": ["Xiang Feng", "Xiangbo Wang", "Tieshi Zhong", "Chengkai Wang", "Yiting Zhao", "Tianxiang Xu", "Zhenzhong Kuang", "Feiwei Qin", "Xuefei Yin", "Yanming Zhu"], "title": "SR3R: Rethinking Super-Resolution 3D Reconstruction With Feed-Forward Gaussian Splatting", "comment": "CVPR 2026", "summary": "3D super-resolution (3DSR) aims to reconstruct high-resolution (HR) 3D scenes from low-resolution (LR) multi-view images. Existing methods rely on dense LR inputs and per-scene optimization, which restricts the high-frequency priors for constructing HR 3D Gaussian Splatting (3DGS) to those inherited from pretrained 2D super-resolution (2DSR) models. This severely limits reconstruction fidelity, cross-scene generalization, and real-time usability. We propose to reformulate 3DSR as a direct feed-forward mapping from sparse LR views to HR 3DGS representations, enabling the model to autonomously learn 3D-specific high-frequency geometry and appearance from large-scale, multi-scene data. This fundamentally changes how 3DSR acquires high-frequency knowledge and enables robust generalization to unseen scenes. Specifically, we introduce SR3R, a feed-forward framework that directly predicts HR 3DGS representations from sparse LR views via the learned mapping network. To further enhance reconstruction fidelity, we introduce Gaussian offset learning and feature refinement, which stabilize reconstruction and sharpen high-frequency details. SR3R is plug-and-play and can be paired with any feed-forward 3DGS reconstruction backbone: the backbone provides an LR 3DGS scaffold, and SR3R upscales it to an HR 3DGS. Extensive experiments across three 3D benchmarks demonstrate that SR3R surpasses state-of-the-art (SOTA) 3DSR methods and achieves strong zero-shot generalization, even outperforming SOTA per-scene optimization methods on unseen scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843DSR\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u524d\u9988\u6620\u5c04\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bc6\u96c6\u7684LR\u8f93\u5165\u548c\u573a\u666f\u4f18\u5316\uff0c\u9650\u5236\u4e86\u4ece\u9884\u8bad\u7ec3\u76842D\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7ee7\u627f\u7684\u9ad8\u9891\u5148\u9a8c\uff0c\u4ece\u800c\u4e25\u91cd\u9650\u5236\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u573a\u666f\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u65f6\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u5c063DSR\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ece\u7a00\u758fLR\u89c6\u56fe\u5230HR 3DGS\u8868\u793a\u7684\u76f4\u63a5\u524d\u9988\u6620\u5c04\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u5927\u89c4\u6a21\u3001\u591a\u573a\u666f\u6570\u636e\u4e2d\u81ea\u4e3b\u5b66\u4e603D\u7279\u5b9a\u7684\u51e0\u4f55\u548c\u5916\u89c2\u7684\u9ad8\u9891\u4fe1\u606f\u3002\u5f15\u5165SR3R\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u7a00\u758fLR\u89c6\u56fe\u9884\u6d4bHR 3DGS\u8868\u793a\u3002\u8fd8\u5f15\u5165\u9ad8\u65af\u504f\u79fb\u5b66\u4e60\u548c\u7279\u5f81\u7ec6\u5316\u6765\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSR3R\u4f18\u4e8e\u73b0\u6709\u76843DSR\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u751a\u81f3\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e0a\u4f18\u4e8eSOTA\u573a\u666f\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e863DSR\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a3D\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.24209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e92\u8865\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u7b49\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u4fdd\u62a4\u9690\u79c1\u5e76\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bbe\u5907\u529f\u80fd\u7684\u5dee\u5f02\u3001\u6570\u636e\u683c\u5f0f\u548c\u901a\u4fe1\u7ea6\u675f\uff0cIoT\u6570\u636e\u5177\u6709\u5f02\u6784\u6027\uff0c\u8fd9\u7ed9\u7ef4\u62a4\u5168\u5c40\u6a21\u578b\u6027\u80fd\u548c\u9690\u79c1\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6765\u81ea\u4e24\u4e2a\u4e0d\u540c\u7269\u8054\u7f51\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\u6765\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u96c6\u7279\u5b9a\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u91c7\u7528\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5982SHAP\uff0c\u4ee5\u8bc6\u522b\u5f71\u54cd\u5c40\u90e8\u6a21\u578b\u51b3\u7b56\u7684\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7269\u8054\u7f51\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u4f7f\u7528\u4e92\u8865\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\u6765\u4f18\u5316\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\uff0c\u5e76\u5728\u53bb\u4e2d\u5fc3\u5316\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684\u5f02\u5e38\u68c0\u6d4b\u7ed3\u679c\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.24021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24021", "abs": "https://arxiv.org/abs/2602.24021", "authors": ["Zhaolin Cai", "Fan Li", "Huiyu Duan", "Lijun He", "Guangtao Zhai"], "title": "Steering and Rectifying Latent Representation Manifolds in Frozen Multi-modal LLMs for Video Anomaly Detection", "comment": "Accepted by ICLR 2026", "summary": "Video anomaly detection (VAD) aims to identify abnormal events in videos. Traditional VAD methods generally suffer from the high costs of labeled data and full training, thus some recent works have explored leveraging frozen multi-modal large language models (MLLMs) in a tuning-free manner to perform VAD. However, their performance is limited as they directly inherit pre-training biases and cannot adapt internal representations to specific video contexts, leading to difficulties in handling subtle or ambiguous anomalies. To address these limitations, we propose a novel intervention framework, termed SteerVAD, which advances MLLM-based VAD by shifting from passively reading to actively steering and rectifying internal representations. Our approach first leverages the gradient-free representational separability analysis (RSA) to identify top attention heads as latent anomaly experts (LAEs) which are most discriminative for VAD. Then a hierarchical meta-controller (HMC) generates dynamic rectification signals by jointly conditioning on global context and these LAE outputs. The signals execute targeted, anisotropic scaling directly upon the LAE representation manifolds, amplifying anomaly-relevant dimensions while suppressing inherent biases. Extensive experiments on mainstream benchmarks demonstrate our method achieves state-of-the-art performance among tuning-free approaches requiring only 1% of training data, establishing it as a powerful new direction for video anomaly detection. The code will be released upon the publication.", "AI": {"tldr": "SteerVAD\uff1a\u901a\u8fc7\u4e3b\u52a8\u5f15\u5bfc\u548c\u6821\u6b63\u5185\u90e8\u8868\u793a\u63d0\u5347\u57fa\u4e8eMLLM\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u548c\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u8fd1\u671f\u4e00\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e72\u9884\u6846\u67b6SteerVAD\u3002", "method": "SteerVAD\u901a\u8fc7\u4e3b\u52a8\u5f15\u5bfc\u548c\u6821\u6b63\u5185\u90e8\u8868\u793a\u6765\u63d0\u5347\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u6027\u80fd\u3002\u9996\u5148\u5229\u7528\u65e0\u68af\u5ea6\u8868\u793a\u53ef\u5206\u6027\u5206\u6790\uff08RSA\uff09\u8bc6\u522b\u51fa\u5bf9VAD\u6700\u6709\u5224\u522b\u529b\u7684\u6f5c\u5728\u5f02\u5e38\u4e13\u5bb6\uff08LAEs\uff09\uff0c\u7136\u540e\u901a\u8fc7\u5206\u5c42\u5143\u63a7\u5236\u5668\uff08HMC\uff09\u751f\u6210\u52a8\u6001\u6821\u6b63\u4fe1\u53f7\uff0c\u76f4\u63a5\u5bf9LAE\u8868\u793a\u6d41\u5f62\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u5404\u5411\u5f02\u6027\u7f29\u653e\uff0c\u589e\u5f3a\u5f02\u5e38\u76f8\u5173\u7ef4\u5ea6\u5e76\u6291\u5236\u56fa\u6709\u504f\u5dee\u3002", "result": "\u5728\u4e3b\u6d41\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSteerVAD\u5728\u65e0\u9700\u5fae\u8c03\u7684\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ec5\u97001%\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "SteerVAD\u4e3a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.24027", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.24027", "abs": "https://arxiv.org/abs/2602.24027", "authors": ["Xingyu Zhu", "Beier Zhu", "Junfeng Fang", "Shuo Wang", "Yin Zhang", "Xiang Wang", "Xiangnan He"], "title": "GuardAlign: Test-time Safety Alignment in Multimodal Large Language Models", "comment": "ICLR 2026", "summary": "Large vision-language models (LVLMs) have achieved remarkable progress in vision-language reasoning tasks, yet ensuring their safety remains a critical challenge. Recent input-side defenses detect unsafe images with CLIP and prepend safety prefixes to prompts, but they still suffer from inaccurate detection in complex scenes and unstable safety signals during decoding. To address these issues, we propose GuardAlign, a training-free defense framework that integrates two strategies. First, OT-enhanced safety detection leverages optimal transport to measure distribution distances between image patches and unsafe semantics, enabling accurate identification of malicious regions without additional computational cost. Second, cross-modal attentive calibration strengthens the influence of safety prefixes by adaptively reallocating attention across layers, ensuring that safety signals remain consistently activated throughout generation. Extensive evaluations on six representative MLLMs demonstrate that GuardAlign reduces unsafe response rates by up to 39% on SPA-VL, while preserving utility, achieving an improvement on VQAv2 from 78.51% to 79.21%.", "AI": {"tldr": "GuardAlign\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u9632\u5fa1\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8LVLMs\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6548\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuardAlign\u7684\u8bad\u7ec3\u514d\u8d39\u9632\u5fa1\u6846\u67b6\u3002", "method": "GuardAlign\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u7b56\u7565\uff1aOT\u589e\u5f3a\u5b89\u5168\u68c0\u6d4b\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6821\u51c6\u3002", "result": "\u5728\u516d\u4e2a\u4ee3\u8868\u6027\u7684MLLMs\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0cGuardAlign\u5728SPA-VL\u4e0a\u964d\u4f4e\u4e8639%\u7684\u4e0d\u5b89\u5168\u54cd\u5e94\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6548\u7528\uff0c\u5c06VQAv2\u7684\u8bc4\u5206\u4ece78.51%\u63d0\u9ad8\u523079.21%\u3002", "conclusion": "GuardAlign\u5728\u63d0\u9ad8LVLMs\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6548\u7528\u3002"}}
{"id": "2602.24231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24231", "abs": "https://arxiv.org/abs/2602.24231", "authors": ["Hongrui Xie", "Junyu Cao", "Kan Xu"], "title": "Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference", "comment": "30 pages, 3 figure, AISTATS 2026 accepted paper", "summary": "In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6743\u8861\uff0c\u4e3a\u591a\u76ee\u6807\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5173\u6ce8\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08CMAB\uff09\u4e2d\u9057\u61be\u6700\u5c0f\u5316\u4e0e\u7edf\u8ba1\u529f\u6548\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7Pareto\u6700\u4f18\u7684\u6982\u5ff5\u5f62\u5f0f\u5316\u6743\u8861\uff0c\u5efa\u7acbCMAB\u4e2dPareto\u6709\u6548\u5b66\u4e60\u7684\u7b49\u4ef7\u6761\u4ef6\u3002\u9488\u5bf9\u4e0d\u540c\u4fe1\u606f\u7ed3\u6784\uff08\u5168\u8001\u864e\u673a\u53cd\u9988\u548c\u534a\u8001\u864e\u673a\u53cd\u9988\uff09\u63d0\u51faMixCombKL\u548cMixCombUCB\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u7b97\u6cd5\u5728\u9057\u61be\u548c\u81c2\u95f4\u8ddd\u4f30\u8ba1\u8bef\u5dee\u4e0a\u5747\u6709\u6709\u9650\u65f6\u95f4\u4fdd\u8bc1\u3002\u53d1\u73b0\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u6536\u7d27\u53ef\u8fbe\u7684Pareto\u524d\u6cbf\u3002", "conclusion": "\u4e3a\u591a\u76ee\u6807\u51b3\u7b56\u4e2d\u7684\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2602.24041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24041", "abs": "https://arxiv.org/abs/2602.24041", "authors": ["Xingyu Zhu", "Kesen Zhao", "Liang Yi", "Shuo Wang", "Zhicai Wang", "Beier Zhu", "Hanwang Zhang"], "title": "Look Carefully: Adaptive Visual Reinforcements in Multimodal Large Language Models for Hallucination Mitigation", "comment": "ICLR 2026", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language reasoning, yet they remain vulnerable to hallucination, where generated content deviates from visual evidence. Existing mitigation strategies either require costly supervision during training or introduce additional latency at inference time. Recent vision enhancement methods attempt to address this issue by reinforcing visual tokens during decoding, but they typically inject all tokens indiscriminately, which causes interference from background regions and distracts the model from critical cues. To overcome this challenge, we propose Adaptive Visual Reinforcement (AIR), a training-free framework for MLLMs. AIR consists of two components. Prototype-based token reduction condenses the large pool of visual tokens into a compact subset to suppress redundancy. OT-guided patch reinforcement quantifies the alignment between hidden states and patch embeddings to selectively integrate the most consistent patches into feed-forward layers. As a result, AIR enhances the model's reliance on salient visual information and effectively mitigates hallucination. Extensive experiments across representative MLLMs demonstrate that AIR substantially reduces hallucination while preserving general capabilities, establishing it as an effective solution for building reliable MLLMs.", "AI": {"tldr": "Adaptive Visual Reinforcement (AIR) \u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86 MLLMs \u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3MLLMs\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e2d\u6613\u51fa\u73b0\u5e7b\u89c9\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u8bad\u7ec3\u76d1\u7763\uff0c\u8981\u4e48\u5728\u63a8\u7406\u65f6\u5f15\u5165\u989d\u5916\u7684\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive Visual Reinforcement (AIR)\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u539f\u578b token \u51cf\u5c11\u548cOT\u5f15\u5bfc\u7684 patch \u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIR\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u901a\u7528\u80fd\u529b\uff0c\u6210\u4e3a\u6784\u5efa\u53ef\u9760MLLMs\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Adaptive Visual Reinforcement (AIR) \u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11 MLLMs \u4e2d\u7684\u5e7b\u89c9\u3002"}}
{"id": "2602.24238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24238", "abs": "https://arxiv.org/abs/2602.24238", "authors": ["Javier Pulido", "Filipe Rodrigues"], "title": "Time Series Foundation Models as Strong Baselines in Transportation Forecasting: A Large-Scale Benchmark Analysis", "comment": "6 pages", "summary": "Accurate forecasting of transportation dynamics is essential for urban mobility and infrastructure planning. Although recent work has achieved strong performance with deep learning models, these methods typically require dataset-specific training, architecture design and hyper-parameter tuning. This paper evaluates whether general-purpose time-series foundation models can serve as forecasters for transportation tasks by benchmarking the zero-shot performance of the state-of-the-art model, Chronos-2, across ten real-world datasets covering highway traffic volume and flow, urban traffic speed, bike-sharing demand, and electric vehicle charging station data. Under a consistent evaluation protocol, we find that, even without any task-specific fine-tuning, Chronos-2 delivers state-of-the-art or competitive accuracy across most datasets, frequently outperforming classical statistical baselines and specialized deep learning architectures, particularly at longer horizons. Beyond point forecasting, we evaluate its native probabilistic outputs using prediction-interval coverage and sharpness, demonstrating that Chronos-2 also provides useful uncertainty quantification without dataset-specific training. In general, this study supports the adoption of time-series foundation models as a key baseline for transportation forecasting research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.24043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24043", "abs": "https://arxiv.org/abs/2602.24043", "authors": ["Yingxuan You", "Ren Li", "Corentin Dumery", "Cong Cao", "Hao Li", "Pascal Fua"], "title": "Spatio-Temporal Garment Reconstruction Using Diffusion Mapping via Pattern Coordinates", "comment": "arXiv admin note: text overlap with arXiv:2504.08353", "summary": "Reconstructing 3D clothed humans from monocular images and videos is a fundamental problem with applications in virtual try-on, avatar creation, and mixed reality. Despite significant progress in human body recovery, accurately reconstructing garment geometry, particularly for loose-fitting clothing, remains an open challenge. We propose a unified framework for high-fidelity 3D garment reconstruction from both single images and video sequences. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn expressive garment shape priors in 2D UV space. Leveraging these priors, we introduce a mapping model that establishes correspondences between image pixels, UV pattern coordinates, and 3D geometry, enabling accurate and detailed garment reconstruction from single images. We further extend this formulation to dynamic reconstruction by introducing a spatio-temporal diffusion scheme with test-time guidance to enforce long-range temporal consistency. We also develop analytic projection-based constraints that preserve image-aligned geometry in visible regions while enforcing coherent completion in occluded areas over time. Although trained exclusively on synthetically simulated cloth data, our method generalizes well to real-world imagery and consistently outperforms existing approaches on both tight- and loose-fitting garments. The reconstructed garments preserve fine geometric detail while exhibiting realistic dynamic motion, supporting downstream applications such as texture editing, garment retargeting, and animation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u91cd\u5efa3D\u7740\u88c5\u4eba\u7c7b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u5e76\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u3002", "motivation": "\u4ece\u5355\u5f20\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u91cd\u5efa3D\u7740\u88c5\u4eba\u7c7b\u662f\u865a\u62df\u8bd5\u7a7f\u3001\u5934\u50cf\u521b\u5efa\u548c\u6df7\u5408\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u7684\u57fa\u672c\u95ee\u9898\u3002\u5c3d\u7ba1\u5728\u4eba\u4f53\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u51c6\u786e\u91cd\u5efa\u670d\u88c5\u51e0\u4f55\u5f62\u72b6\uff0c\u7279\u522b\u662f\u5bbd\u677e\u670d\u88c5\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u9690\u5f0f\u7f1d\u5408\u56fe\u6848\uff08ISP\uff09\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u57282D UV\u7a7a\u95f4\u4e2d\u5b66\u4e60\u8868\u8fbe\u5f0f\u7684\u670d\u88c5\u5f62\u72b6\u5148\u9a8c\u3002\u5229\u7528\u8fd9\u4e9b\u5148\u9a8c\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u6620\u5c04\u6a21\u578b\uff0c\u5728\u56fe\u50cf\u50cf\u7d20\u3001UV\u56fe\u6848\u5750\u6807\u548c3D\u51e0\u4f55\u4e4b\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u51c6\u786e\u548c\u8be6\u7ec6\u5730\u91cd\u5efa\u670d\u88c5\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u7a7a\u6269\u6563\u65b9\u6848\u548c\u6d4b\u8bd5\u65f6\u5f15\u5bfc\uff0c\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u52a8\u6001\u91cd\u5efa\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u8fd8\u5f00\u53d1\u4e86\u57fa\u4e8e\u89e3\u6790\u6295\u5f71\u7684\u7ea6\u675f\uff0c\u4ee5\u4fdd\u6301\u53ef\u89c1\u533a\u57df\u7684\u56fe\u50cf\u5bf9\u9f50\u51e0\u4f55\u5f62\u72b6\uff0c\u540c\u65f6\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u5728\u906e\u6321\u533a\u57df\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u7684\u5b8c\u6210\u3002", "result": "\u867d\u7136\u4ec5\u5728\u5408\u6210\u6a21\u62df\u7684\u5e03\u6599\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f46\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u5e76\u4e14\u5728\u7d27\u8eab\u548c\u5bbd\u677e\u670d\u88c5\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u91cd\u5efa\u7684\u670d\u88c5\u4fdd\u7559\u4e86\u7cbe\u7ec6\u7684\u51e0\u4f55\u7ec6\u8282\uff0c\u5e76\u8868\u73b0\u51fa\u903c\u771f\u7684\u52a8\u6001\u8fd0\u52a8\uff0c\u652f\u6301\u4e0b\u6e38\u5e94\u7528\uff0c\u5982\u7eb9\u7406\u7f16\u8f91\u3001\u670d\u88c5\u91cd\u5b9a\u5411\u548c\u52a8\u753b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u4fdd\u771f3D\u670d\u88c5\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u865a\u62df\u8bd5\u7a7f\u3001\u5934\u50cf\u521b\u5efa\u548c\u6df7\u5408\u73b0\u5b9e\u7b49\u9886\u57df\u3002"}}
{"id": "2602.24245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24245", "abs": "https://arxiv.org/abs/2602.24245", "authors": ["Hainan Xu", "Vladimir Bataev", "Travis M. Bartley", "Jagadeesh Balam"], "title": "Chunk-wise Attention Transducers for Fast and Accurate Streaming Speech-to-Text", "comment": "Accepted at ICASSP 2026", "summary": "We propose Chunk-wise Attention Transducer (CHAT), a novel extension to RNN-T models that processes audio in fixed-size chunks while employing cross-attention within each chunk. This hybrid approach maintains RNN-T's streaming capability while introducing controlled flexibility for local alignment modeling. CHAT significantly reduces the temporal dimension that RNN-T must handle, yielding substantial efficiency improvements: up to 46.2% reduction in peak training memory, up to 1.36X faster training, and up to 1.69X faster inference. Alongside these efficiency gains, CHAT achieves consistent accuracy improvements over RNN-T across multiple languages and tasks -- up to 6.3% relative WER reduction for speech recognition and up to 18.0% BLEU improvement for speech translation. The method proves particularly effective for speech translation, where RNN-T's strict monotonic alignment hurts performance. Our results demonstrate that the CHAT model offers a practical solution for deploying more capable streaming speech models without sacrificing real-time constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHAT\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86RNN-T\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6d41\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8RNN-T\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6d41\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChunk-wise Attention Transducer (CHAT)\u7684\u65b0\u578b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u4e2d\u5904\u7406\u97f3\u9891\uff0c\u5e76\u5728\u6bcf\u4e2a\u5757\u4e2d\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5305\u62ec46.2%\u7684\u8bad\u7ec3\u5185\u5b58\u5cf0\u503c\u964d\u4f4e\u30011.36\u500d\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u548c1.69\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002\u540c\u65f6\uff0c\u5728\u591a\u4e2a\u8bed\u8a00\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u7684\u63d0\u5347\uff0c\u5305\u62ec6.3%\u7684\u76f8\u5bf9WER\u964d\u4f4e\u548c18.0%\u7684BLEU\u63d0\u5347\u3002", "conclusion": "CHAT\u6a21\u578b\u4e3a\u90e8\u7f72\u66f4\u5f3a\u5927\u7684\u6d41\u5f0f\u8bed\u97f3\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u5b9e\u65f6\u6027\u9650\u5236\u3002"}}
{"id": "2602.24059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24059", "abs": "https://arxiv.org/abs/2602.24059", "authors": ["Chenwei Jia", "Baoting Li", "Xuchong Zhang", "Mingzhuo Wei", "Bochen Lin", "Hongbin Sun"], "title": "Quant Experts: Token-aware Adaptive Error Reconstruction with Mixture of Experts for Large Vision-Language Models Quantization", "comment": "13 pages, 6 figures, including appendix, Accepted at CVPR 2026", "summary": "Post-Training Quantization (PTQ) has emerged as an effective technique for alleviating the substantial computational and memory overheads of Vision-Language Models (VLMs) by compressing both weights and activations without retraining the full model. Existing PTQ methods primarily rely on static identification and global compensation of sensitive or outlier channels, yet they often overlook the distributional differences of these important channels across inputs, leading to unsatisfactory quantization. In this work, we observe that the distributions and occurrence frequencies of important channels vary significantly both across modalities and among tokens, even within the same modality. Accordingly, we propose \\textbf{Quant Experts (QE)}, a token-aware adaptive error compensation with mixture-of-experts for VLMs quantization. QE divides the important channels into token-independent and token-dependent groups. For the former, a shared expert is designed for most tokens to compensate for global quantization error using a low-rank adapter. For the latter, routed experts including multiple routed low-rank adapters are elaborated to compensate for local quantization error related to specific tokens. Extensive experiments demonstrate that QE consistently enhances task accuracy across various quantization settings and model scales, ranging from 2B to 70B parameters, while maintaining performance comparable to full-precision models.", "AI": {"tldr": "\u63d0\u51fa\u4e86Quant Experts\uff08QE\uff09\uff0c\u4e00\u79cd\u9488\u5bf9VLMs\u7684token\u611f\u77e5\u81ea\u9002\u5e94\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u51cf\u8f7b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u63d0\u51fa\u4e86\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aQuant Experts\uff08QE\uff09\u7684token\u611f\u77e5\u81ea\u9002\u5e94\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\uff0c\u7528\u4e8eVLMs\u91cf\u5316\u3002QE\u5c06\u91cd\u8981\u901a\u9053\u5206\u4e3atoken\u72ec\u7acb\u548ctoken\u76f8\u5173\u4e24\u7ec4\u3002\u5bf9\u4e8e\u524d\u8005\uff0c\u4e3a\u5927\u591a\u6570token\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5171\u4eab\u4e13\u5bb6\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u8865\u507f\u5168\u5c40\u91cf\u5316\u8bef\u5dee\u3002\u5bf9\u4e8e\u540e\u8005\uff0c\u8bbe\u8ba1\u4e86\u5305\u62ec\u591a\u4e2a\u8def\u7531\u4f4e\u79e9\u9002\u914d\u5668\u7684\u8def\u7531\u4e13\u5bb6\uff0c\u4ee5\u8865\u507f\u4e0e\u7279\u5b9atoken\u76f8\u5173\u7684\u5c40\u90e8\u91cf\u5316\u8bef\u5dee\u3002", "result": "QE\u5728\u591a\u79cd\u91cf\u5316\u8bbe\u7f6e\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\uff0c\u4ece2B\u523070B\u53c2\u6570\uff0c\u6301\u7eed\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "QE\u662f\u4e00\u79cd\u6709\u6548\u7684VLMs\u91cf\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.24251", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24251", "abs": "https://arxiv.org/abs/2602.24251", "authors": ["Xiaolong Zhang", "Jianwei Zhang", "Selim Sevim", "Emek Demir", "Ece Eksi", "Xubo Song"], "title": "Histopathology Image Normalization via Latent Manifold Compaction", "comment": "11 pages", "summary": "Batch effects arising from technical variations in histopathology staining protocols, scanners, and acquisition pipelines pose a persistent challenge for computational pathology, hindering cross-batch generalization and limiting reliable deployment of models across clinical sites. In this work, we introduce Latent Manifold Compaction (LMC), an unsupervised representation learning framework that performs image harmonization by learning batch-invariant embeddings from a single source dataset through explicit compaction of stain-induced latent manifolds. This allows LMC to generalize to target domain data unseen during training. Evaluated on three challenging public and in-house benchmarks, LMC substantially reduces batch-induced separations across multiple datasets and consistently outperforms state-of-the-art normalization methods in downstream cross-batch classification and detection tasks, enabling superior generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6 LMC\uff0c\u7528\u4e8e\u89e3\u51b3\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u6279\u6b21\u6548\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5185\u5bb9\u4e2d\u63d0\u5230\uff0c\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\uff0c\u6765\u81ea\u67d3\u8272\u534f\u8bae\u3001\u626b\u63cf\u4eea\u548c\u91c7\u96c6\u7ba1\u9053\u7684\u6280\u672f\u5dee\u5f02\u5bfc\u81f4\u7684\u6279\u6b21\u6548\u5e94\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\uff0c\u963b\u788d\u4e86\u8de8\u6279\u6b21\u7684\u6cdb\u5316\uff0c\u5e76\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u5730\u70b9\u7684\u53ef\u9760\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Latent Manifold Compaction (LMC) \u7684\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u538b\u7f29\u7531\u67d3\u8272\u5f15\u8d77\u7684\u6f5c\u5728\u6d41\u5f62\u6765\u4ece\u5355\u4e2a\u6e90\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6279\u6b21\u4e0d\u53d8\u7684\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u56fe\u50cf\u5f52\u4e00\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u516c\u5171\u548c\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86 LMC\uff0c\u5b83\u663e\u8457\u51cf\u5c11\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u6279\u6b21\u5f15\u8d77\u7684\u5206\u79bb\uff0c\u5e76\u5728\u4e0b\u6e38\u7684\u8de8\u6279\u6b21\u5206\u7c7b\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u3002", "conclusion": "LMC \u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6279\u6b21\u6548\u5e94\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.24065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24065", "abs": "https://arxiv.org/abs/2602.24065", "authors": ["Zaiyan Yang", "Jieji Ren", "Xiangyi Wang", "zonglin li", "Xu Cao", "Heng Guo", "Zhanyu Ma", "Boxin Shi"], "title": "EvalMVX: A Unified Benchmarking for Neural 3D Reconstruction under Diverse Multiview Setups", "comment": null, "summary": "Recent advancements in neural surface reconstruction have significantly enhanced 3D reconstruction. However, current real world datasets mainly focus on benchmarking multiview stereo (MVS) based on RGB inputs. Multiview photometric stereo (MVPS) and multiview shape from polarization (MVSfP), though indispensable on high-fidelity surface reconstruction and sparse inputs, have not been quantitatively assessed together with MVS. To determine the working range of different MVX (MVS, MVSfP, and MVPS) techniques, we propose EvalMVX, a real-world dataset containing $25$ objects, each captured with a polarized camera under $20$ varying views and $17$ light conditions including OLAT and natural illumination, leading to $8,500$ images. Each object includes aligned ground-truth 3D mesh, facilitating quantitative benchmarking of MVX methods simultaneously. Based on our EvalMVX, we evaluate $13$ MVX methods published in recent years, record the best-performing methods, and identify open problems under diverse geometric details and reflectance types. We hope EvalMVX and the benchmarking results can inspire future research on multiview 3D reconstruction.", "AI": {"tldr": "\u63d0\u51faEvalMVX\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002", "motivation": "\u4e3a\u4e86\u786e\u5b9a\u4e0d\u540cMVX\uff08MVS\u3001MVSfP\u548cMVPS\uff09\u6280\u672f\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u8bc4\u4f30\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEvalMVX\uff0c\u4e00\u4e2a\u5305\u542b25\u4e2a\u7269\u4f53\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u7269\u4f53\u572820\u4e2a\u4e0d\u540c\u89c6\u89d2\u548c17\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u504f\u632f\u76f8\u673a\u6355\u83b7\uff0c\u5e76\u5305\u542b\u5bf9\u9f50\u7684\u5730\u9762\u771f\u5b9e3D\u7f51\u683c\uff0c\u4ee5\u4fc3\u8fdbMVX\u65b9\u6cd5\u7684\u5b9a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u4e8613\u79cdMVX\u65b9\u6cd5\uff0c\u8bb0\u5f55\u4e86\u8868\u73b0\u6700\u4f73\u7684\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u4e0d\u540c\u51e0\u4f55\u7ec6\u8282\u548c\u53cd\u5c04\u7c7b\u578b\u4e0b\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "EvalMVX\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u53ef\u4ee5\u542f\u53d1\u672a\u6765\u591a\u89c6\u56fe3D\u91cd\u5efa\u7814\u7a76\u3002"}}
{"id": "2602.24278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24278", "abs": "https://arxiv.org/abs/2602.24278", "authors": ["Shruti Joshi", "Th\u00e9o Saulus", "Wieland Brendel", "Philippe Brouillard", "Dhanya Sridhar", "Patrik Reizinger"], "title": "Who Guards the Guardians? The Challenges of Evaluating Identifiability of Learned Representations", "comment": null, "summary": "Identifiability in representation learning is commonly evaluated using standard metrics (e.g., MCC, DCI, R^2) on synthetic benchmarks with known ground-truth factors. These metrics are assumed to reflect recovery up to the equivalence class guaranteed by identifiability theory. We show that this assumption holds only under specific structural conditions: each metric implicitly encodes assumptions about both the data-generating process (DGP) and the encoder. When these assumptions are violated, metrics become misspecified and can produce systematic false positives and false negatives. Such failures occur both within classical identifiability regimes and in post-hoc settings where identifiability is most needed. We introduce a taxonomy separating DGP assumptions from encoder geometry, use it to characterise the validity domains of existing metrics, and release an evaluation suite for reproducible stress testing and comparison.", "AI": {"tldr": "\u73b0\u6709\u6307\u6807\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "motivation": "\u8bc4\u4f30\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u53ef\u8bc6\u522b\u6027\u901a\u5e38\u4f7f\u7528\u6807\u51c6\u6307\u6807\uff08\u4f8b\u5982\uff0cMCC\uff0cDCI\uff0cR^2\uff09\u5728\u5177\u6709\u5df2\u77e5\u771f\u5b9e\u56e0\u7d20\u7684\u5408\u6210\u57fa\u51c6\u4e0a\u8fdb\u884c\u3002\u8fd9\u4e9b\u6307\u6807\u88ab\u8ba4\u4e3a\u53cd\u6620\u4e86\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u4fdd\u8bc1\u7684\u7b49\u4ef7\u7c7b\u5185\u7684\u6062\u590d\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5c06DGP\u5047\u8bbe\u4e0e\u7f16\u7801\u5668\u51e0\u4f55\u5206\u79bb\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u63cf\u8ff0\u73b0\u6709\u6307\u6807\u7684\u9002\u7528\u57df\uff0c\u5e76\u53d1\u5e03\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u538b\u529b\u6d4b\u8bd5\u548c\u6bd4\u8f83\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u6307\u6807\u4ec5\u5728\u7279\u5b9a\u7ed3\u6784\u6761\u4ef6\u4e0b\u624d\u6210\u7acb\uff0c\u5426\u5219\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u7684\u9519\u8bef\u9633\u6027\u53cd\u5e94\u548c\u9519\u8bef\u9634\u6027\u53cd\u5e94\u3002", "conclusion": "\u9700\u8981\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6307\u6807\u80cc\u540e\u7684\u5047\u8bbe\uff0c\u4ee5\u907f\u514d\u9519\u8bef\u5730\u8bc4\u4f30\u53ef\u8bc6\u522b\u6027\u3002"}}
{"id": "2602.24133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24133", "abs": "https://arxiv.org/abs/2602.24133", "authors": ["Sifan Zhou", "Jiahao Nie", "Ziyu Zhao", "Yichao Cao", "Xiaobo Lu"], "title": "FocusTrack: One-Stage Focus-and-Suppress Framework for 3D Point Cloud Object Tracking", "comment": "Acceptted in ACM MM 2025", "summary": "In 3D point cloud object tracking, the motion-centric methods have emerged as a promising avenue due to its superior performance in modeling inter-frame motion. However, existing two-stage motion-based approaches suffer from fundamental limitations: (1) error accumulation due to decoupled optimization caused by explicit foreground segmentation prior to motion estimation, and (2) computational bottlenecks from sequential processing. To address these challenges, we propose FocusTrack, a novel one-stage paradigms tracking framework that unifies motion-semantics co-modeling through two core innovations: Inter-frame Motion Modeling (IMM) and Focus-and-Suppress Attention. The IMM module employs a temp-oral-difference siamese encoder to capture global motion patterns between adjacent frames. The Focus-and-Suppress attention that enhance the foreground semantics via motion-salient feature gating and suppress the background noise based on the temporal-aware motion context from IMM without explicit segmentation. Based on above two designs, FocusTrack enables end-to-end training with compact one-stage pipeline. Extensive experiments on prominent 3D tracking benchmarks, such as KITTI, nuScenes, and Waymo, demonstrate that the FocusTrack achieves new SOTA performance while running at a high speed with 105 FPS.", "AI": {"tldr": "FocusTrack\u901a\u8fc7IMM\u548c\u5173\u6ce8-\u6291\u5236\u6ce8\u610f\u529b\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u9636\u6bb53D\u70b9\u4e91\u5bf9\u8c61\u8ddf\u8e2a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u8fd0\u52a8\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u5982\u8bef\u5dee\u7d2f\u79ef\u548c\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u63d0\u51faFocusTrack\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u4e00\u9636\u6bb5\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u5e27\u95f4\u8fd0\u52a8\u5efa\u6a21\uff08IMM\uff09\u548c\u5173\u6ce8-\u6291\u5236\u6ce8\u610f\u529b\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cWaymo\u7b493D\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u65b0SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4ee5105 FPS\u7684\u901f\u5ea6\u8fd0\u884c\u3002", "conclusion": "FocusTrack\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2602.24136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24136", "abs": "https://arxiv.org/abs/2602.24136", "authors": ["Haoran Wang", "Guoxi Huang", "Fan Zhang", "David Bull", "Nantheera Anantrasirichai"], "title": "Prune Wisely, Reconstruct Sharply: Compact 3D Gaussian Splatting via Adaptive Pruning and Difference-of-Gaussian Primitives", "comment": "CVPR2026", "summary": "Recent significant advances in 3D scene representation have been driven by 3D Gaussian Splatting (3DGS), which has enabled real-time rendering with photorealistic quality. 3DGS often requires a large number of primitives to achieve high fidelity, leading to redundant representations and high resource consumption, thereby limiting its scalability for complex or large-scale scenes. Consequently, effective pruning strategies and more expressive primitives that can reduce redundancy while preserving visual quality are crucial for practical deployment. We propose an efficient, integrated reconstruction-aware pruning strategy that adaptively determines pruning timing and refining intervals based on reconstruction quality, thus reducing model size while enhancing rendering quality. Moreover, we introduce a 3D Difference-of-Gaussians primitive that jointly models both positive and negative densities in a single primitive, improving the expressiveness of Gaussians under compact configurations. Our method significantly improves model compactness, achieving up to 90\\% reduction in Gaussian-count while delivering visual quality that is similar to, or in some cases better than, that produced by state-of-the-art methods. Code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u5e76\u964d\u4f4e\u5197\u4f59\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad83D\u573a\u666f\u8868\u793a\u7684\u6548\u7387\uff0c\u964d\u4f4e\u5197\u4f59\u548c\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u91cd\u5efa\u611f\u77e5\u526a\u679d\u7b56\u7565\u548c3D\u5dee\u5f02\u9ad8\u65af\u539f\u59cb\u6570\u636e\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7d27\u51d1\u6027\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u76f8\u4f3c\u751a\u81f3\u66f4\u597d\u7684\u540c\u65f6\uff0c\u5c06\u9ad8\u65af\u6570\u91cf\u51cf\u5c11\u4e8690%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u4e8e3D\u573a\u666f\u8868\u793a\u5177\u6709\u91cd\u5927\u610f\u4e49\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\u3002"}}
{"id": "2602.24138", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24138", "abs": "https://arxiv.org/abs/2602.24138", "authors": ["Omar Mohamed", "Edoardo Fazzari", "Ayah Al-Naji", "Hamdan Alhadhrami", "Khalfan Hableel", "Saif Alkindi", "Cesare Stefanini"], "title": "Multimodal Optimal Transport for Unsupervised Temporal Segmentation in Surgical Robotics", "comment": null, "summary": "Recognizing surgical phases and steps from video is a fundamental problem in computer-assisted interventions. Recent approaches increasingly rely on large-scale pre-training on thousands of labeled surgical videos, followed by zero-shot transfer to specific procedures. While effective, this strategy incurs substantial computational and data collection costs. In this work, we question whether such heavy pre-training is truly necessary. We propose Text-Augmented Action Segmentation Optimal Transport (TASOT), an unsupervised method for surgical phase and step recognition that extends Action Segmentation Optimal Transport (ASOT) by incorporating textual information generated directly from the videos. TASOT formulates temporal action segmentation as a multimodal optimal transport problem, where the matching cost is defined as a weighted combination of visual and text-based costs. The visual term captures frame-level appearance similarity, while the text term provides complementary semantic cues, and both are jointly regularized through a temporally consistent unbalanced Gromov-Wasserstein formulation. This design enables effective alignment between video frames and surgical actions without surgical-specific pretraining or external web-scale supervision. We evaluate TASOT on multiple benchmark surgical datasets and observe consistent and substantial improvements over existing zero-shot methods, including StrasBypass70 (+23.7), BernBypass70 (+4.5), Cholec80 (+16.5), and AutoLaparo (+19.6). These results demonstrate that fine-grained surgical understanding can be achieved by exploiting information already present in standard visual and textual representations, without resorting to increasingly complex pre-training pipelines. The code will be available at https://github.com/omar8ahmed9/TASOT.", "code_url": "https://github.com/omar8ahmed9/TASOT", "code_stars": 0, "code_last_update": "2026-02-27", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u65b9\u6cd5TASOT\uff0c\u901a\u8fc7\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u52a8\u4f5c\u5206\u5272\uff0c\u6709\u6548\u964d\u4f4e\u624b\u672f\u89c6\u9891\u5206\u6790\u6210\u672c", "motivation": "\u964d\u4f4e\u624b\u672f\u89c6\u9891\u5206\u6790\u4e2d\u7684\u8ba1\u7b97\u548c\u6570\u636e\u6536\u96c6\u6210\u672c", "method": "\u63d0\u51faText-Augmented Action Segmentation Optimal Transport (TASOT)\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u4fe1\u606f\u878d\u5165ASOT\uff0c\u5c06\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u4f5c\u4e3a\u591a\u6a21\u6001\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u5904\u7406", "result": "\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u89c2\u5bdf\u5230\u6bd4\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u66f4\u4e00\u81f4\u7684\u5b9e\u8d28\u6027\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u5229\u7528\u6807\u51c6\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u4e2d\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u7684\u624b\u672f\u7406\u89e3\uff0c\u65e0\u9700\u590d\u6742\u7684\u9884\u8bad\u7ec3\u6d41\u7a0b"}}
{"id": "2602.24144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24144", "abs": "https://arxiv.org/abs/2602.24144", "authors": ["Muquan Li", "Hang Gou", "Yingyi Ma", "Rongzheng Wang", "Ke Qin", "Tao He"], "title": "Fixed Anchors Are Not Enough: Dynamic Retrieval and Persistent Homology for Dataset Distillation", "comment": "Accepted by CVPR 2026", "summary": "Decoupled dataset distillation (DD) compresses large corpora into a few synthetic images by matching a frozen teacher's statistics. However, current residual-matching pipelines rely on static real patches, creating a fit-complexity gap and a pull-to-anchor effect that reduce intra-class diversity and hurt generalization. To address these issues, we introduce RETA -- a Retrieval and Topology Alignment framework for decoupled DD. First, Dynamic Retrieval Connection (DRC) selects a real patch from a prebuilt pool by minimizing a fit-complexity score in teacher feature space; the chosen patch is injected via a residual connection to tighten feature fit while controlling injected complexity. Second, Persistent Topology Alignment (PTA) regularizes synthesis with persistent homology: we build a mutual k-NN feature graph, compute persistence images of components and loops, and penalize topology discrepancies between real and synthetic sets, mitigating pull-to-anchor effect. Across CIFAR-100, Tiny-ImageNet, ImageNet-1K, and multiple ImageNet subsets, RETA consistently outperforms various baselines under comparable time and memory, especially reaching 64.3% top-1 accuracy on ImageNet-1K with ResNet-18 at 50 images per class, +3.1% over the best prior.", "AI": {"tldr": "RETA\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u8fde\u63a5\u548c\u6301\u4e45\u62d3\u6251\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u89e3\u8026\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u6b8b\u5dee\u5339\u914d\u7ba1\u9053\u4f9d\u8d56\u4e8e\u9759\u6001\u771f\u5b9e\u8865\u4e01\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u62df\u5408\u590d\u6742\u5ea6\u5dee\u8ddd\u548c\u62c9\u5411\u951a\u70b9\u6548\u5e94\uff0c\u4ece\u800c\u964d\u4f4e\u7c7b\u5185\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRETA\u7684\u68c0\u7d22\u548c\u62d3\u6251\u5bf9\u9f50\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u6001\u68c0\u7d22\u8fde\u63a5\uff08DRC\uff09\u548c\u6301\u4e45\u62d3\u6251\u5bf9\u9f50\uff08PTA\uff09\u3002DRC\u901a\u8fc7\u5728\u6559\u5e08\u7279\u5f81\u7a7a\u95f4\u4e2d\u6700\u5c0f\u5316\u62df\u5408\u590d\u6742\u5ea6\u5206\u6570\u4ece\u9884\u5efa\u6c60\u4e2d\u9009\u62e9\u771f\u5b9e\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u6ce8\u5165\u6240\u9009\u8865\u4e01\u4ee5\u7d27\u5316\u7279\u5f81\u62df\u5408\u5e76\u63a7\u5236\u6ce8\u5165\u7684\u590d\u6742\u5ea6\u3002PTA\u4f7f\u7528\u6301\u4e45\u540c\u4f26\u6b63\u5219\u5316\u5408\u6210\uff0c\u6784\u5efa\u76f8\u4e92k-NN\u7279\u5f81\u56fe\uff0c\u8ba1\u7b97\u7ec4\u4ef6\u548c\u73af\u7684\u6301\u4e45\u56fe\u50cf\uff0c\u5e76\u60e9\u7f5a\u771f\u5b9e\u548c\u5408\u6210\u96c6\u4e4b\u95f4\u7684\u62d3\u6251\u5dee\u5f02\uff0c\u4ee5\u51cf\u8f7b\u62c9\u5411\u951a\u70b9\u6548\u5e94\u3002", "result": "\u5728CIFAR-100\u3001Tiny-ImageNet\u3001ImageNet-1K\u548c\u591a\u4e2aImageNet\u5b50\u96c6\u4e0a\uff0cRETA\u5728\u53ef\u6bd4\u7684\u65f6\u95f4\u548c\u5185\u5b58\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728ImageNet-1K\u4e0a\uff0c\u4f7f\u7528ResNet-18\u5728\u6bcf\u7c7b50\u4e2a\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e8664.3%\u7684top-1\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u4f73\u5148\u524d\u65b9\u6cd5\u9ad8\u51fa3.1%\u3002", "conclusion": "RETA\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u89e3\u8026\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u964d\u4f4e\u62df\u5408\u590d\u6742\u5ea6\u5dee\u8ddd\u548c\u62c9\u5411\u951a\u70b9\u6548\u5e94\u65b9\u9762\u3002"}}
{"id": "2602.24148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24148", "abs": "https://arxiv.org/abs/2602.24148", "authors": ["Keito Suzuki", "Kunyao Chen", "Lei Wang", "Bang Du", "Runfa Blark Li", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "HumanOrbit: 3D Human Reconstruction as 360\u00b0 Orbit Generation", "comment": "CVPR 2026 Findings", "summary": "We present a method for generating a full 360\u00b0 orbit video around a person from a single input image. Existing methods typically adapt image-based diffusion models for multi-view synthesis, but yield inconsistent results across views and with the original identity. In contrast, recent video diffusion models have demonstrated their ability in generating photorealistic results that align well with the given prompts. Inspired by these results, we propose HumanOrbit, a video diffusion model for multi-view human image generation. Our approach enables the model to synthesize continuous camera rotations around the subject, producing geometrically consistent novel views while preserving the appearance and identity of the person. Using the generated multi-view frames, we further propose a reconstruction pipeline that recovers a textured mesh of the subject. Experimental results validate the effectiveness of HumanOrbit for multi-view image generation and that the reconstructed 3D models exhibit superior completeness and fidelity compared to those from state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHumanOrbit\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210360\u5ea6\u73af\u7ed5\u4e00\u4e2a\u4eba\u7684\u591a\u89c6\u56fe\u89c6\u9891\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u751f\u6210360\u5ea6\u73af\u7ed5\u4e00\u4e2a\u4eba\u7684\u89c6\u9891\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u89c6\u56fe\u5408\u6210\uff0c\u4f46\u7ed3\u679c\u5728\u4e0d\u540c\u89c6\u56fe\u548c\u539f\u59cb\u8eab\u4efd\u4e4b\u95f4\u4e0d\u4e00\u81f4\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHumanOrbit\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u89c6\u56fe\u4eba\u50cf\u751f\u6210\u3002", "method": "HumanOrbit\u662f\u4e00\u79cd\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u5408\u6210\u8fde\u7eed\u7684\u76f8\u673a\u65cb\u8f6c\uff0c\u4ea7\u751f\u51e0\u4f55\u4e0a\u4e00\u81f4\u7684 novel \u89c6\u56fe\uff0c\u540c\u65f6\u4fdd\u7559\u4eba\u7684\u5916\u89c2\u548c\u8eab\u4efd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHumanOrbit\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u65b9\u9762\u6709\u6548\uff0c\u91cd\u5efa\u76843D\u6a21\u578b\u5728\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "HumanOrbit\u4e3a\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u8d28\u91cf\u5916\u89c2\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u76843D\u6a21\u578b\u3002"}}
{"id": "2602.24160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24160", "abs": "https://arxiv.org/abs/2602.24160", "authors": ["Alexander Vieth", "Boudewijn Lelieveldt", "Elmar Eisemann", "Anna Vilanova", "Thomas H\u00f6llt"], "title": "Manifold-Preserving Superpixel Hierarchies and Embeddings for the Exploration of High-Dimensional Images", "comment": "12 pages main paper, 8 pages supplemental material", "summary": "High-dimensional images, or images with a high-dimensional attribute vector per pixel, are commonly explored with coordinated views of a low-dimensional embedding of the attribute space and a conventional image representation. Nowadays, such images can easily contain several million pixels. For such large datasets, hierarchical embedding techniques are better suited to represent the high-dimensional attribute space than flat dimensionality reduction methods. However, available hierarchical dimensionality reduction methods construct the hierarchy purely based on the attribute information and ignore the spatial layout of pixels in the images. This impedes the exploration of regions of interest in the image space, since there is no congruence between a region of interest in image space and the associated attribute abstractions in the hierarchy. In this paper, we present a superpixel hierarchy for high-dimensional images that takes the high-dimensional attribute manifold into account during construction. Through this, our method enables consistent exploration of high-dimensional images in both image and attribute space. We show the effectiveness of this new image-guided hierarchy in the context of embedding exploration by comparing it with classical hierarchical embedding-based image exploration in two use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u63a2\u7d22\u9ad8\u7ef4\u56fe\u50cf\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u7684\u5c42\u6b21\u964d\u7ef4\u65b9\u6cd5\u5728\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\u65f6\u4ec5\u8003\u8651\u5c5e\u6027\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u7a7a\u95f4\u5e03\u5c40\u3002\u8fd9\u963b\u788d\u4e86\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5bf9\u611f\u5174\u8da3\u533a\u57df\u7684\u63a2\u7d22\uff0c\u56e0\u4e3a\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\u4e0e\u5176\u5728\u5c42\u6b21\u7ed3\u6784\u4e2d\u5173\u8054\u7684\u5c5e\u6027\u62bd\u8c61\u4e4b\u95f4\u6ca1\u6709\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5728\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\u65f6\u8003\u8651\u4e86\u9ad8\u7ef4\u5c5e\u6027\u6d41\u5f62\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u53ef\u4ee5\u5728\u56fe\u50cf\u548c\u5c5e\u6027\u7a7a\u95f4\u4e2d\u4e00\u81f4\u5730\u63a2\u7d22\u9ad8\u7ef4\u56fe\u50cf\uff0c\u5e76\u5728\u4e24\u4e2a\u7528\u4f8b\u4e2d\u4e0e\u7ecf\u5178\u57fa\u4e8e\u5c42\u6b21\u5d4c\u5165\u7684\u56fe\u50cf\u63a2\u7d22\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ee5\u5c55\u793a\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5728\u56fe\u50cf\u548c\u5c5e\u6027\u7a7a\u95f4\u4e2d\u63a2\u7d22\u9ad8\u7ef4\u56fe\u50cf\uff0c\u63d0\u9ad8\u4e86\u5bf9\u611f\u5174\u8da3\u533a\u57df\u7684\u63a2\u7d22\u80fd\u529b\u3002"}}
{"id": "2602.24161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24161", "abs": "https://arxiv.org/abs/2602.24161", "authors": ["Chao Xu", "Xiaochen Zhao", "Xiang Deng", "Jingxiang Sun", "Zhuo Su", "Donglin Di", "Yebin Liu"], "title": "GeoDiff4D: Geometry-Aware Diffusion for 4D Head Avatar Reconstruction", "comment": "17 pages", "summary": "Reconstructing photorealistic and animatable 4D head avatars from a single portrait image remains a fundamental challenge in computer vision. While diffusion models have enabled remarkable progress in image and video generation for avatar reconstruction, existing methods primarily rely on 2D priors and struggle to achieve consistent 3D geometry. We propose a novel framework that leverages geometry-aware diffusion to learn strong geometry priors for high-fidelity head avatar reconstruction. Our approach jointly synthesizes portrait images and corresponding surface normals, while a pose-free expression encoder captures implicit expression representations. Both synthesized images and expression latents are incorporated into 3D Gaussian-based avatars, enabling photorealistic rendering with accurate geometry. Extensive experiments demonstrate that our method substantially outperforms state-of-the-art approaches in visual quality, expression fidelity, and cross-identity generalization, while supporting real-time rendering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u6269\u6563\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u8096\u50cf\u56fe\u50cf\u4e2d\u91cd\u5efa\u903c\u771f\u4e14\u53ef\u52a8\u76844D\u5934\u50cf\u3002", "motivation": "\u91cd\u5efa\u4ece\u5355\u5f20\u8096\u50cf\u56fe\u50cf\u4e2d\u83b7\u53d6\u903c\u771f\u4e14\u53ef\u52a84D\u5934\u50cf\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u51e0\u4f55\u611f\u77e5\u6269\u6563\u5b66\u4e60\u5f3a\u5927\u51e0\u4f55\u5148\u9a8c\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5934\u50cf\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u8054\u5408\u751f\u6210\u8096\u50cf\u56fe\u50cf\u548c\u76f8\u5e94\u7684\u8868\u9762\u6cd5\u7ebf\uff0c\u540c\u65f6\u4f7f\u7528\u65e0\u59ff\u6001\u8868\u8fbe\u5f0f\u7f16\u7801\u5668\u6355\u83b7\u9690\u5f0f\u8868\u8fbe\u5f0f\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8868\u60c5\u4fdd\u771f\u5ea6\u548c\u8de8\u8eab\u4efd\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u4fdd\u771f\u5934\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2602.24181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24181", "abs": "https://arxiv.org/abs/2602.24181", "authors": ["Rishabh Kabra", "Maks Ovsjanikov", "Drew A. Hudson", "Ye Xia", "Skanda Koppula", "Andre Araujo", "Joao Carreira", "Niloy J. Mitra"], "title": "A Mixed Diet Makes DINO An Omnivorous Vision Encoder", "comment": "CVPR 2026", "summary": "Pre-trained vision encoders like DINOv2 have demonstrated exceptional performance on unimodal tasks. However, we observe that their feature representations are poorly aligned across different modalities. For instance, the feature embedding for an RGB image and its corresponding depth map of the same scene exhibit a cosine similarity that is nearly identical to that of two random, unrelated images. To address this, we propose the Omnivorous Vision Encoder, a novel framework that learns a modality-agnostic feature space. We train the encoder with a dual objective: first, to maximize the feature alignment between different modalities of the same scene; and second, a distillation objective that anchors the learned representations to the output of a fully frozen teacher such as DINOv2. The resulting student encoder becomes \"omnivorous\" by producing a consistent, powerful embedding for a given scene, regardless of the input modality (RGB, Depth, Segmentation, etc.). This approach enables robust cross-modal understanding while retaining the discriminative semantics of the original foundation model.", "AI": {"tldr": "Omnivorous Vision Encoder\u901a\u8fc7\u5b66\u4e60\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7279\u5f81\u8868\u793a\u5bf9\u9f50\u4e0d\u826f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5728\u5355\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7279\u5f81\u8868\u793a\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u5bf9\u9f50\u4e0d\u826f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmnivorous Vision Encoder\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5b66\u4e60\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u4f7f\u7528\u53cc\u91cd\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff1a\u9996\u5148\uff0c\u6700\u5927\u5316\u540c\u4e00\u573a\u666f\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u7279\u5f81\u5bf9\u9f50\uff1b\u5176\u6b21\uff0c\u84b8\u998f\u76ee\u6807\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u951a\u5b9a\u5230DINOv2\u7b49\u5b8c\u5168\u51bb\u7ed3\u7684\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u3002", "result": "\u5b66\u751f\u7f16\u7801\u5668\u80fd\u591f\u4ea7\u751f\u4e00\u81f4\u3001\u5f3a\u5927\u7684\u5d4c\u5165\uff0c\u65e0\u8bba\u8f93\u5165\u6a21\u6001\uff08RGB\u3001\u6df1\u5ea6\u3001\u5206\u5272\u7b49\uff09\u5982\u4f55\u3002\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u59cb\u57fa\u7840\u6a21\u578b\u7684\u5224\u522b\u8bed\u4e49\u3002", "conclusion": "Omnivorous Vision Encoder\u901a\u8fc7\u5b66\u4e60\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7279\u5f81\u8868\u793a\u5bf9\u9f50\u4e0d\u826f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3002"}}
{"id": "2602.24183", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24183", "abs": "https://arxiv.org/abs/2602.24183", "authors": ["Yixuan Liu", "Kanwal K. Bhatia", "Ahmed E. Fetit"], "title": "A multimodal slice discovery framework for systematic failure detection and explanation in medical image classification", "comment": null, "summary": "Despite advances in machine learning-based medical image classifiers, the safety and reliability of these systems remain major concerns in practical settings. Existing auditing approaches mainly rely on unimodal features or metadata-based subgroup analyses, which are limited in interpretability and often fail to capture hidden systematic failures. To address these limitations, we introduce the first automated auditing framework that extends slice discovery methods to multimodal representations specifically for medical applications. Comprehensive experiments were conducted under common failure scenarios using the MIMIC-CXR-JPG dataset, demonstrating the framework's strong capability in both failure discovery and explanation generation. Our results also show that multimodal information generally allows more comprehensive and effective auditing of classifiers, while unimodal variants beyond image-only inputs exhibit strong potential in scenarios where resources are constrained.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u591a\u6a21\u6001\u5ba1\u8ba1\u6846\u67b6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6545\u969c\u53d1\u73b0\u548c\u89e3\u91ca\u751f\u6210\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4ecd\u7136\u662f\u4e3b\u8981\u95ee\u9898\u3002\u73b0\u6709\u7684\u5ba1\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u7279\u5f81\u6216\u57fa\u4e8e\u5143\u6570\u636e\u7684\u5b50\u7ec4\u5206\u6790\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6709\u9650\uff0c\u5e76\u4e14\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u9690\u85cf\u7684\u7cfb\u7edf\u6545\u969c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u81ea\u52a8\u5ba1\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5207\u7247\u53d1\u73b0\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u6a21\u6001\u8868\u793a\uff0c\u4e13\u95e8\u7528\u4e8e\u533b\u5b66\u5e94\u7528\u3002", "result": "\u5728MIMIC-CXR-JPG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6545\u969c\u53d1\u73b0\u548c\u89e3\u91ca\u751f\u6210\u65b9\u9762\u90fd\u5177\u6709\u5f3a\u5927\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u591a\u6a21\u6001\u4fe1\u606f\u901a\u5e38\u5141\u8bb8\u5bf9\u5206\u7c7b\u5668\u8fdb\u884c\u66f4\u5168\u9762\u548c\u6709\u6548\u7684\u5ba1\u8ba1\uff0c\u800c\u8d85\u8d8a\u4ec5\u56fe\u50cf\u8f93\u5165\u7684\u5355\u6a21\u6001\u53d8\u4f53\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u5408\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u4fe1\u606f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5ba1\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.24208", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24208", "abs": "https://arxiv.org/abs/2602.24208", "authors": ["Yasaman Haghighi", "Alexandre Alahi"], "title": "SenCache: Accelerating Diffusion Model Inference via Sensitivity-Aware Caching", "comment": null, "summary": "Diffusion models achieve state-of-the-art video generation quality, but their inference remains expensive due to the large number of sequential denoising steps. This has motivated a growing line of research on accelerating diffusion inference. Among training-free acceleration methods, caching reduces computation by reusing previously computed model outputs across timesteps. Existing caching methods rely on heuristic criteria to choose cache/reuse timesteps and require extensive tuning. We address this limitation with a principled sensitivity-aware caching framework. Specifically, we formalize the caching error through an analysis of the model output sensitivity to perturbations in the denoising inputs, i.e., the noisy latent and the timestep, and show that this sensitivity is a key predictor of caching error. Based on this analysis, we propose Sensitivity-Aware Caching (SenCache), a dynamic caching policy that adaptively selects caching timesteps on a per-sample basis. Our framework provides a theoretical basis for adaptive caching, explains why prior empirical heuristics can be partially effective, and extends them to a dynamic, sample-specific approach. Experiments on Wan 2.1, CogVideoX, and LTX-Video show that SenCache achieves better visual quality than existing caching methods under similar computational budgets.", "AI": {"tldr": "SenCache\u901a\u8fc7\u654f\u611f\u5ea6\u611f\u77e5\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u63a2\u7d22\u52a0\u901f\u6269\u6563\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u654f\u611f\u5ea6\u611f\u77e5\u7684\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u8f93\u51fa\u5bf9\u53bb\u566a\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\u6765\u4f18\u5316\u7f13\u5b58\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSenCache\u5728\u7c7b\u4f3c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u6bd4\u73b0\u6709\u7684\u7f13\u5b58\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "SenCache\u901a\u8fc7\u654f\u611f\u5ea6\u611f\u77e5\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u3002"}}
{"id": "2602.24222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24222", "abs": "https://arxiv.org/abs/2602.24222", "authors": ["Albert Dominguez Mantes", "Gioele La Manno", "Martin Weigert"], "title": "MuViT: Multi-Resolution Vision Transformers for Learning Across Scales in Microscopy", "comment": "Accepted at CVPR 2026", "summary": "Modern microscopy routinely produces gigapixel images that contain structures across multiple spatial scales, from fine cellular morphology to broader tissue organization. Many analysis tasks require combining these scales, yet most vision models operate at a single resolution or derive multi-scale features from one view, limiting their ability to exploit the inherently multi-resolution nature of microscopy data. We introduce MuViT, a transformer architecture built to fuse true multi-resolution observations from the same underlying image. MuViT embeds all patches into a shared world-coordinate system and extends rotary positional embeddings to these coordinates, enabling attention to integrate wide-field context with high-resolution detail within a single encoder. Across synthetic benchmarks, kidney histopathology, and high-resolution mouse-brain microscopy, MuViT delivers consistent improvements over strong ViT and CNN baselines. Multi-resolution MAE pretraining further produces scale-consistent representations that enhance downstream tasks. These results demonstrate that explicit world-coordinate modelling provides a simple yet powerful mechanism for leveraging multi-resolution information in large-scale microscopy analysis.", "AI": {"tldr": "MuViT\u901a\u8fc7\u878d\u5408\u591a\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u5728\u663e\u5fae\u955c\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8bb8\u591a\u5206\u6790\u4efb\u52a1\u9700\u8981\u7ed3\u5408\u591a\u4e2a\u7a7a\u95f4\u5c3a\u5ea6\u7684\u7ed3\u6784\uff0c\u4f46\u5927\u591a\u6570\u89c6\u89c9\u6a21\u578b\u5728\u5355\u4e00\u5206\u8fa8\u7387\u4e0b\u8fd0\u884c\u6216\u4ece\u5355\u4e00\u89c6\u89d2\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5229\u7528\u663e\u5fae\u955c\u6570\u636e\u7684\u5185\u5728\u591a\u5206\u8fa8\u7387\u6027\u8d28\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faMuViT\uff0c\u8fd9\u662f\u4e00\u79cdTransformer\u67b6\u6784\uff0c\u65e8\u5728\u878d\u5408\u6765\u81ea\u540c\u4e00\u5e95\u5c42\u56fe\u50cf\u7684\u771f\u5b9e\u591a\u5206\u8fa8\u7387\u89c2\u5bdf\u7ed3\u679c\u3002MuViT\u5c06\u6240\u6709\u8865\u4e01\u5d4c\u5165\u5230\u5171\u4eab\u7684\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\uff0c\u5e76\u5c06\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u6269\u5c55\u5230\u8fd9\u4e9b\u5750\u6807\uff0c\u4f7f\u6ce8\u610f\u529b\u80fd\u591f\u5728\u5355\u4e2a\u7f16\u7801\u5668\u4e2d\u6574\u5408\u5e7f\u57df\u4e0a\u4e0b\u6587\u4e0e\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u3001\u80be\u810f\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u9ad8\u5206\u8fa8\u7387\u5c0f\u9f20\u8111\u663e\u5fae\u955c\u4e2d\uff0cMuViT\u5728ViT\u548cCNN\u57fa\u7ebf\u4e4b\u4e0a\u63d0\u4f9b\u4e86\u6301\u7eed\u6539\u8fdb\u3002\u591a\u5206\u8fa8\u7387MAE\u9884\u8bad\u7ec3\u8fdb\u4e00\u6b65\u4ea7\u751f\u4e86\u5c3a\u5ea6\u4e00\u81f4\u7684\u8868\u793a\uff0c\u589e\u5f3a\u4e86\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u663e\u5f0f\u4e16\u754c\u5750\u6807\u5efa\u6a21\u4e3a\u5229\u7528\u5927\u89c4\u6a21\u663e\u5fae\u955c\u5206\u6790\u4e2d\u7684\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u673a\u5236\u3002"}}
{"id": "2602.24233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24233", "abs": "https://arxiv.org/abs/2602.24233", "authors": ["Zhenyu Tang", "Chaoran Feng", "Yufan Deng", "Jie Wu", "Xiaojie Li", "Rui Wang", "Yunpeng Chen", "Daquan Zhou"], "title": "Enhancing Spatial Understanding in Image Generation via Reward Modeling", "comment": "Accepted at CVPR 2026. Github: https://github.com/DAGroup-PKU/SpatialT2I Project website: https://dagroup-pku.github.io/SpatialT2I/", "summary": "Recent progress in text-to-image generation has greatly advanced visual fidelity and creativity, but it has also imposed higher demands on prompt complexity-particularly in encoding intricate spatial relationships. In such cases, achieving satisfactory results often requires multiple sampling attempts. To address this challenge, we introduce a novel method that strengthens the spatial understanding of current image generation models. We first construct the SpatialReward-Dataset with over 80k preference pairs. Building on this dataset, we build SpatialScore, a reward model designed to evaluate the accuracy of spatial relationships in text-to-image generation, achieving performance that even surpasses leading proprietary models on spatial evaluation. We further demonstrate that this reward model effectively enables online reinforcement learning for the complex spatial generation. Extensive experiments across multiple benchmarks show that our specialized reward model yields significant and consistent gains in spatial understanding for image generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u3002", "motivation": "\u8fd1\u5e74\u6765\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u540c\u65f6\u4e5f\u5bf9\u63d0\u793a\u7684\u590d\u6742\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\uff0c\u5c24\u5176\u662f\u5728\u7f16\u7801\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u52a0\u5f3a\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc780k\u4e2a\u504f\u597d\u5bf9\u7684SpatialReward-Dataset\uff0c\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86SpatialScore\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7a7a\u95f4\u5173\u7cfb\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u5956\u52b1\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u63d0\u5347\u3002", "result": "SpatialScore\u5956\u52b1\u6a21\u578b\u5728\u7a7a\u95f4\u8bc4\u4f30\u65b9\u9762\u751a\u81f3\u8d85\u8fc7\u4e86\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u590d\u6742\u7a7a\u95f4\u751f\u6210\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4e3a\u590d\u6742\u7a7a\u95f4\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24240", "abs": "https://arxiv.org/abs/2602.24240", "authors": ["Chengyan Deng", "Zhangquan Chen", "Li Yu", "Kai Zhang", "Xue Zhou", "Wang Zhang"], "title": "Joint Geometric and Trajectory Consistency Learning for One-Step Real-World Super-Resolution", "comment": null, "summary": "Diffusion-based Real-World Image Super-Resolution (Real-ISR) achieves impressive perceptual quality but suffers from high computational costs due to iterative sampling. While recent distillation approaches leveraging large-scale Text-to-Image (T2I) priors have enabled one-step generation, they are typically hindered by prohibitive parameter counts and the inherent capability bounds imposed by teacher models. As a lightweight alternative, Consistency Models offer efficient inference but struggle with two critical limitations: the accumulation of consistency drift inherent to transitive training, and a phenomenon we term \"Geometric Decoupling\" - where the generative trajectory achieves pixel-wise alignment yet fails to preserve structural coherence. To address these challenges, we propose GTASR (Geometric Trajectory Alignment Super-Resolution), a simple yet effective consistency training paradigm for Real-ISR. Specifically, we introduce a Trajectory Alignment (TA) strategy to rectify the tangent vector field via full-path projection, and a Dual-Reference Structural Rectification (DRSR) mechanism to enforce strict structural constraints. Extensive experiments verify that GTASR delivers superior performance over representative baselines while maintaining minimal latency. The code and model will be released at https://github.com/Blazedengcy/GTASR.", "code_url": "https://github.com/Blazedengcy/GTASR", "code_stars": 2, "code_last_update": "2026-02-25", "AI": {"tldr": "GTASR\u662f\u4e00\u79cd\u9ad8\u6548\u7684Real-ISR\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u4e00\u81f4\u6027\u6f02\u79fb\u95ee\u9898\u3002", "method": "GTASR\uff08\u51e0\u4f55\u8f68\u8ff9\u5bf9\u9f50\u8d85\u5206\u8fa8\u7387\uff09\u662f\u4e00\u79cd\u4e00\u81f4\u6027\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u62ec\u8f68\u8ff9\u5bf9\u9f50\u7b56\u7565\u548c\u53cc\u91cd\u53c2\u8003\u7ed3\u6784\u6821\u6b63\u673a\u5236\u3002", "result": "GTASR\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5177\u6709\u8f83\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "GTASR\u5728Real-ISR\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u5ef6\u8fdf\u3002"}}
{"id": "2602.24275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24275", "abs": "https://arxiv.org/abs/2602.24275", "authors": ["Junxian Huang", "Ruichu Cai", "Hao Zhu", "Juntao Fang", "Boyan Xu", "Weilin Chen", "Zijian Li", "Shenghua Gao"], "title": "Hierarchical Action Learning for Weakly-Supervised Action Segmentation", "comment": null, "summary": "Humans perceive actions through key transitions that structure actions across multiple abstraction levels, whereas machines, relying on visual features, tend to over-segment. This highlights the difficulty of enabling hierarchical reasoning in video understanding. Interestingly, we observe that lower-level visual and high-level action latent variables evolve at different rates, with low-level visual variables changing rapidly, while high-level action variables evolve more slowly, making them easier to identify. Building on this insight, we propose the Hierarchical Action Learning (\\textbf{HAL}) model for weakly-supervised action segmentation. Our approach introduces a hierarchical causal data generation process, where high-level latent action governs the dynamics of low-level visual features. To model these varying timescales effectively, we introduce deterministic processes to align these latent variables over time. The \\textbf{HAL} model employs a hierarchical pyramid transformer to capture both visual features and latent variables, and a sparse transition constraint is applied to enforce the slower dynamics of high-level action variables. This mechanism enhances the identification of these latent variables over time. Under mild assumptions, we prove that these latent action variables are strictly identifiable. Experimental results on several benchmarks show that the \\textbf{HAL} model significantly outperforms existing methods for weakly-supervised action segmentation, confirming its practical effectiveness in real-world applications.", "AI": {"tldr": "HAL\u6a21\u578b\u901a\u8fc7\u5c42\u6b21\u63a8\u7406\u5b9e\u73b0\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5173\u952e\u8fc7\u6e21\u611f\u77e5\u52a8\u4f5c\uff0c\u800c\u673a\u5668\u5219\u503e\u5411\u4e8e\u8fc7\u5ea6\u5206\u5272\uff0c\u8fd9\u7a81\u51fa\u4e86\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u5b9e\u73b0\u5c42\u6b21\u63a8\u7406\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAL\u7684\u5c42\u6b21\u52a8\u4f5c\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u5c42\u6b21\u56e0\u679c\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u5c42\u6b21\u91d1\u5b57\u5854\u8f6c\u6362\u5668\u6765\u6355\u83b7\u89c6\u89c9\u7279\u5f81\u548c\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHAL\u6a21\u578b\u5728\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u6709\u6548\u6027\u3002", "conclusion": "HAL\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u5c42\u6b21\u52a8\u4f5c\u5b66\u4e60\uff0c\u4e3a\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24289", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24289", "abs": "https://arxiv.org/abs/2602.24289", "authors": ["Shengqu Cai", "Weili Nie", "Chao Liu", "Julius Berner", "Lvmin Zhang", "Nanye Ma", "Hansheng Chen", "Maneesh Agrawala", "Leonidas Guibas", "Gordon Wetzstein", "Arash Vahdat"], "title": "Mode Seeking meets Mean Seeking for Fast Long Video Generation", "comment": "Project website: https://primecai.github.io/mmm/", "summary": "Scaling video generation from seconds to minutes faces a critical bottleneck: while short-video data is abundant and high-fidelity, coherent long-form data is scarce and limited to narrow domains. To address this, we propose a training paradigm where Mode Seeking meets Mean Seeking, decoupling local fidelity from long-term coherence based on a unified representation via a Decoupled Diffusion Transformer. Our approach utilizes a global Flow Matching head trained via supervised learning on long videos to capture narrative structure, while simultaneously employing a local Distribution Matching head that aligns sliding windows to a frozen short-video teacher via a mode-seeking reverse-KL divergence. This strategy enables the synthesis of minute-scale videos that learns long-range coherence and motions from limited long videos via supervised flow matching, while inheriting local realism by aligning every sliding-window segment of the student to a frozen short-video teacher, resulting in a few-step fast long video generator. Evaluations show that our method effectively closes the fidelity-horizon gap by jointly improving local sharpness, motion and long-range consistency. Project website: https://primecai.github.io/mmm/.", "code_url": "https://primecai.github.io/mmm/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u8026\u6269\u6563\u53d8\u6362\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4ece\u79d2\u5230\u5206\u949f\u7684\u6269\u5c55\u95ee\u9898\uff0c\u5b9e\u73b0\u5feb\u901f\u751f\u6210\u957f\u89c6\u9891\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u4ece\u79d2\u5230\u5206\u949f\u7684\u6269\u5c55\u9762\u4e34\u74f6\u9888\uff0c\u56e0\u4e3a\u867d\u7136\u77ed\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4e14\u9ad8\u4fdd\u771f\uff0c\u4f46\u8fde\u8d2f\u7684\u957f\u89c6\u9891\u6570\u636e\u7a00\u7f3a\u4e14\u9650\u4e8e\u72ed\u7a84\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c06\u6a21\u5f0f\u5bfb\u6c42\u4e0e\u5e73\u5747\u5bfb\u6c42\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u89e3\u8026\u6269\u6563\u53d8\u6362\u5668\u5b9e\u73b0\u5c40\u90e8\u4fdd\u771f\u5ea6\u4e0e\u957f\u671f\u4e00\u81f4\u6027\u7684\u89e3\u8026\u3002\u5229\u7528\u5168\u5c40Flow Matching\u5934\u5728\u957f\u89c6\u9891\u4e0a\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u4ee5\u6355\u83b7\u53d9\u4e8b\u7ed3\u6784\uff0c\u540c\u65f6\u91c7\u7528\u5c40\u90e8Distribution Matching\u5934\u901a\u8fc7\u6a21\u5f0f\u5bfb\u6c42\u7684\u9006KL\u6563\u5ea6\u5c06\u6ed1\u52a8\u7a97\u53e3\u4e0e\u51bb\u7ed3\u7684\u77ed\u89c6\u9891\u6559\u5e08\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u6539\u5584\u5c40\u90e8\u6e05\u6670\u5ea6\u3001\u8fd0\u52a8\u548c\u957f\u8ddd\u79bb\u4e00\u81f4\u6027\uff0c\u6709\u6548\u5730\u7f29\u5c0f\u4e86\u4fdd\u771f\u5ea6-\u89c6\u57df\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4ece\u79d2\u5230\u5206\u949f\u7684\u6269\u5c55\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u751f\u6210\u957f\u89c6\u9891\u3002"}}
{"id": "2602.24290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24290", "abs": "https://arxiv.org/abs/2602.24290", "authors": ["Junhwa Hur", "Charles Herrmann", "Songyou Peng", "Philipp Henzler", "Zeyu Ma", "Todd Zickler", "Deqing Sun"], "title": "UFO-4D: Unposed Feedforward 4D Reconstruction from Two Images", "comment": "ICLR 2026, Project page: https://ufo-4d.github.io/", "summary": "Dense 4D reconstruction from unposed images remains a critical challenge, with current methods relying on slow test-time optimization or fragmented, task-specific feedforward models. We introduce UFO-4D, a unified feedforward framework to reconstruct a dense, explicit 4D representation from just a pair of unposed images. UFO-4D directly estimates dynamic 3D Gaussian Splats, enabling the joint and consistent estimation of 3D geometry, 3D motion, and camera pose in a feedforward manner. Our core insight is that differentiably rendering multiple signals from a single Dynamic 3D Gaussian representation offers major training advantages. This approach enables a self-supervised image synthesis loss while tightly coupling appearance, depth, and motion. Since all modalities share the same geometric primitives, supervising one inherently regularizes and improves the others. This synergy overcomes data scarcity, allowing UFO-4D to outperform prior work by up to 3 times in joint geometry, motion, and camera pose estimation. Our representation also enables high-fidelity 4D interpolation across novel views and time. Please visit our project page for visual results: https://ufo-4d.github.io/", "code_url": "https://ufo-4d.github.io", "AI": {"tldr": "UFO-4D\uff1a\u4e00\u79cd\u4ece\u672a\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u91cd\u5efa\u5bc6\u96c64D\u8868\u793a\u7684\u7edf\u4e00\u9988\u524d\u6846\u67b6\u3002", "motivation": "\u4ece\u672a\u5b9a\u4f4d\u7684\u56fe\u50cf\u4e2d\u91cd\u5efa\u5bc6\u96c6\u76844D\u7ed3\u6784\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7f13\u6162\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u6216\u788e\u7247\u5316\u7684\u3001\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u9988\u524d\u6a21\u578b\u3002", "method": "UFO-4D\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u9988\u524d\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u4e00\u5bf9\u672a\u5b9a\u4f4d\u7684\u56fe\u50cf\u4e2d\u91cd\u5efa\u5bc6\u96c6\u7684\u3001\u663e\u5f0f\u76844D\u8868\u793a\u3002\u5b83\u76f4\u63a5\u4f30\u8ba1\u52a8\u60013D\u9ad8\u65afSplat\uff0c\u4ece\u800c\u5b9e\u73b03D\u51e0\u4f55\u30013D\u8fd0\u52a8\u548c\u76f8\u673a\u59ff\u6001\u7684\u8054\u5408\u548c\u4e00\u81f4\u4f30\u8ba1\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u81ea\u76d1\u7763\u56fe\u50cf\u5408\u6210\u635f\u5931\uff0c\u540c\u65f6\u7d27\u5bc6\u8026\u5408\u5916\u89c2\u3001\u6df1\u5ea6\u548c\u8fd0\u52a8\u3002\u7531\u4e8e\u6240\u6709\u6a21\u6001\u5171\u4eab\u76f8\u540c\u7684\u51e0\u4f55\u539f\u8bed\uff0c\u56e0\u6b64\u76d1\u7763\u4e00\u4e2a\u672c\u8d28\u4e0a\u53ef\u4ee5\u6b63\u5219\u5316\u548c\u6539\u8fdb\u5176\u4ed6\u6a21\u6001\u3002\u8fd9\u79cd\u534f\u540c\u4f5c\u7528\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u4f7f\u5f97UFO-4D\u5728\u8054\u5408\u51e0\u4f55\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u6bd4\u4ee5\u5f80\u7684\u5de5\u4f5c\u63d0\u9ad8\u4e863\u500d\u3002\u6b64\u5916\uff0c\u8be5\u8868\u793a\u8fd8\u5141\u8bb8\u5728\u65b0\u7684\u89c6\u56fe\u548c\u65f6\u95f4\u4e0a\u8fdb\u884c\u9ad8\u4fdd\u771f\u76844D\u63d2\u503c\u3002", "conclusion": "UFO-4D\u901a\u8fc7\u76f4\u63a5\u4f30\u8ba1\u52a8\u60013D\u9ad8\u65afSplat\uff0c\u5b9e\u73b0\u4e86\u4ece\u4e00\u5bf9\u672a\u5b9a\u4f4d\u56fe\u50cf\u5230\u5bc6\u96c64D\u8868\u793a\u7684\u9ad8\u6548\u91cd\u5efa\uff0c\u5728\u51e0\u4f55\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}

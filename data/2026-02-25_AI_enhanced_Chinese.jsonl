{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728\u5e73\u8861\u5224\u522b\u548c\u6821\u51c6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u7fa4\u4f53\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u7f8e\u56fd\u9752\u5c11\u5e74\u8d85\u91cd\u548c\u80a5\u80d6\u7684\u591a\u5c42\u6b21\u9884\u6d4b\u56e0\u7d20\uff0c\u5e76\u6bd4\u8f83\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3001\u6821\u51c6\u548c\u5b50\u7fa4\u4f53\u516c\u5e73\u6027\u3002", "method": "\u5206\u67902021\u5e74\u7f8e\u56fd\u513f\u7ae5\u5065\u5eb7\u8c03\u67e5\u4e2d\u768418,792\u540d10-17\u5c81\u7684\u513f\u7ae5\u6570\u636e\uff0c\u5305\u62ec\u996e\u98df\u3001\u8eab\u4f53\u6d3b\u52a8\u3001\u7761\u7720\u3001\u7236\u6bcd\u538b\u529b\u3001\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\u3001\u4e0d\u826f\u7ecf\u5386\u548c\u793e\u533a\u7279\u5f81\u7b49\u9884\u6d4b\u56e0\u7d20\u3002\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u3001XGBoost\u3001LightGBM\u3001\u591a\u5c42\u611f\u77e5\u5668\u548cTabNet\u7b49\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u9884\u6d4b\u6027\u80fd\u4ece0.66\u52300.79\u4e0d\u7b49\u3002\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728\u5224\u522b\u548c\u6821\u51c6\u4e4b\u95f4\u8868\u73b0\u51fa\u6700\u7a33\u5b9a\u7684\u5e73\u8861\u3002\u589e\u5f3a\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u9002\u5ea6\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002\u6ca1\u6709\u6a21\u578b\u5728\u6240\u6709\u65b9\u9762\u90fd\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u5728\u7b97\u6cd5\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u6a21\u578b\u590d\u6742\u5ea6\u7684\u589e\u52a0\u5728\u903b\u8f91\u56de\u5f52\u4e4b\u4e0a\u5e26\u6765\u7684\u6536\u76ca\u6709\u9650\u3002\u9884\u6d4b\u56e0\u7d20\u59cb\u7ec8\u8de8\u8d8a\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u9886\u57df\u3002\u6301\u7eed\u7684\u5b50\u7fa4\u4f53\u5dee\u5f02\u8868\u660e\u9700\u8981\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u4ee5\u516c\u5e73\u4e3a\u91cd\u70b9\u7684\u76d1\u6d4b\uff0c\u800c\u4e0d\u662f\u589e\u52a0\u7b97\u6cd5\u590d\u6742\u5ea6\u3002"}}
{"id": "2602.20324", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20324", "abs": "https://arxiv.org/abs/2602.20324", "authors": ["Cathy Shyr", "Yan Hu", "Rory J. Tinker", "Thomas A. Cassini", "Kevin W. Byram", "Rizwan Hamid", "Daniel V. Fabbri", "Adam Wright", "Josh F. Peterson", "Lisa Bastarache", "Hua Xu"], "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models", "comment": null, "summary": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.", "AI": {"tldr": "DMCD\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u56e0\u679c\u53d1\u73b0\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMCD\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\u3002", "method": "DMCD\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6839\u636e\u53d8\u91cf\u5143\u6570\u636e\u751f\u6210\u7a00\u758f\u7684DAG\u8349\u56fe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u5bf9\u8349\u56fe\u8fdb\u884c\u5ba1\u8ba1\u548c\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\uff0cDMCD\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u3002", "conclusion": "DMCD\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u7684\u9ad8\u6548\u548c\u51c6\u786e\u3002"}}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "DMEMM\u901a\u8fc7\u73af\u5883\u673a\u5236\u8c03\u8282\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\u751f\u6210\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments.", "method": "We propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions.", "result": "Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "conclusion": "DMEMM provides a novel and effective solution for trajectory generation in offline reinforcement learning by modulating diffusion model training with environment mechanisms."}}
{"id": "2602.20394", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20394", "abs": "https://arxiv.org/abs/2602.20394", "authors": ["Shiba Biswal", "Marc Vuffray", "Andrey Y. Lokhov"], "title": "Selecting Optimal Variable Order in Autoregressive Ising Models", "comment": null, "summary": "Autoregressive models enable tractable sampling from learned probability distributions, but their performance critically depends on the variable ordering used in the factorization via complexities of the resulting conditional distributions. We propose to learn the Markov random field describing the underlying data, and use the inferred graphical model structure to construct optimized variable orderings. We illustrate our approach on two-dimensional image-like models where a structure-aware ordering leads to restricted conditioning sets, thereby reducing model complexity. Numerical experiments on Ising models with discrete data demonstrate that graph-informed orderings yield higher-fidelity generated samples compared to naive variable orderings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4fe1\u606f\u987a\u5e8f\u7684\u81ea\u56de\u5f52\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6734\u7d20\u65b9\u6cd5", "motivation": "\u63d0\u9ad8\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd", "method": "\u5b66\u4e60\u63cf\u8ff0\u6570\u636e\u7684\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u5e76\u5229\u7528\u63a8\u65ad\u7684\u56fe\u5f62\u6a21\u578b\u7ed3\u6784\u6784\u5efa\u4f18\u5316\u7684\u53d8\u91cf\u987a\u5e8f", "result": "\u5728Ising\u6a21\u578b\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6734\u7d20\u53d8\u91cf\u987a\u5e8f\u76f8\u6bd4\uff0c\u56fe\u4fe1\u606f\u987a\u5e8f\u751f\u6210\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u6837\u672c", "conclusion": "\u7ed3\u6784\u611f\u77e5\u7684\u53d8\u91cf\u987a\u5e8f\u53ef\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u63d0\u9ad8\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd"}}
{"id": "2602.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20162", "abs": "https://arxiv.org/abs/2602.20162", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Phillip Miao", "Zilun Zhang", "Haozhan Shen", "Ruizhe Zhu", "Jianwei Yin"], "title": "Talking to Yourself: Defying Forgetting in Large Language Models", "comment": null, "summary": "Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.\n  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20193", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20193", "abs": "https://arxiv.org/abs/2602.20193", "authors": ["Shenyang Chen", "Liuwan Zhu"], "title": "When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks", "comment": null, "summary": "Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.", "AI": {"tldr": "Challenges standard T2I backdoor evaluations, reveals deep structural risks, and emphasizes geometric audits importance.", "motivation": "Standard evaluations of backdoor attacks on T2I models primarily measure trigger activation and visual fidelity.", "method": "Jacobian-based analysis, introduction of SEMAD (Semantic Alignment and Drift) framework", "result": "Persistent, trigger-free semantic corruption, structural degradation, deep structural risks of encoder poisoning", "conclusion": "Highlight the necessity of geometric audits beyond simple attack success rates."}}
{"id": "2602.20424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20424", "abs": "https://arxiv.org/abs/2602.20424", "authors": ["Ved Sirdeshmukh", "Marc Wetter"], "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say", "comment": null, "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20555", "categories": ["stat.ML", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20555", "abs": "https://arxiv.org/abs/2602.20555", "authors": ["Yanming Lai", "Defeng Sun"], "title": "Standard Transformers Achieve the Minimax Rate in Nonparametric Regression with $C^{s,\u03bb}$ Targets", "comment": "58 pages, 1 figure", "summary": "The tremendous success of Transformer models in fields such as large language models and computer vision necessitates a rigorous theoretical investigation. To the best of our knowledge, this paper is the first work proving that standard Transformers can approximate H\u00f6lder functions $ C^{s,\u03bb}\\left([0,1]^{d\\times n}\\right) $$ (s\\in\\mathbb{N}_{\\geq0},0<\u03bb\\leq1) $ under the $L^t$ distance ($t \\in [1, \\infty]$) with arbitrary precision. Building upon this approximation result, we demonstrate that standard Transformers achieve the minimax optimal rate in nonparametric regression for H\u00f6lder target functions. It is worth mentioning that, by introducing two metrics: the size tuple and the dimension vector, we provide a fine-grained characterization of Transformer structures, which facilitates future research on the generalization and optimization errors of Transformers with different structures. As intermediate results, we also derive the upper bounds for the Lipschitz constant of standard Transformers and their memorization capacity, which may be of independent interest. These findings provide theoretical justification for the powerful capabilities of Transformer models.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u6807\u51c6Transformer\u5728\u903c\u8fd1H\u00f6lder\u51fd\u6570\u548c\u4f18\u5316\u975e\u53c2\u6570\u56de\u5f52\u65b9\u9762\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9Transformer\u7ed3\u6784\u7684\u65b0\u7406\u89e3\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u7684\u5de8\u5927\u6210\u529f\uff0c\u9700\u8981\u4e25\u683c\u7684\u7406\u8bba\u7814\u7a76\u3002", "method": "\u8bc1\u660e\u4e86\u6807\u51c6Transformer\u53ef\u4ee5\u5728L^t\u8ddd\u79bb\uff08t\u5c5e\u4e8e[1,\u221e]\uff09\u4e0b\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1H\u00f6lder\u51fd\u6570C^{s,\u03bb}([0,1]^{d\u00d7n})\uff08s\u5c5e\u4e8eN_{\u22650}\uff0c0<\u03bb\u22641\uff09\u3002", "result": "\u6807\u51c6Transformer\u5728\u975e\u53c2\u6570\u56de\u5f52\u4e2d\u5bf9\u4e8eH\u00f6lder\u76ee\u6807\u51fd\u6570\u8fbe\u5230\u4e86\u6700\u5c0f-\u6700\u5927\u6700\u4f18\u901f\u7387\u3002\u63d0\u51fa\u4e86\u4e24\u4e2a\u5ea6\u91cf\uff1a\u5927\u5c0f\u5143\u7ec4\u548c\u7ef4\u5ea6\u5411\u91cf\uff0c\u5bf9Transformer\u7ed3\u6784\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u63cf\u8ff0\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u4e0d\u540c\u7ed3\u6784Transformer\u7684\u6cdb\u5316\u548c\u4f18\u5316\u8bef\u5dee\u3002\u63a8\u5bfc\u4e86\u6807\u51c6Transformer\u7684Lipschitz\u5e38\u6570\u7684\u4e0a\u754c\u53ca\u5176\u8bb0\u5fc6\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3aTransformer\u6a21\u578b\u5f3a\u5927\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.20191", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.", "AI": {"tldr": "MoBiQuant\u662f\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u4f4d\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u6807\u8bb0\u654f\u611f\u6027\u8c03\u6574\u5f39\u6027LLM\u63a8\u7406\u7684\u6743\u91cd\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7684\u7cbe\u5ea6\u5207\u6362\uff0c\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u4e91\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6539\u53d8\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f39\u6027\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5176\u4e2dLLM\u53ef\u4ee5\u6839\u636e\u53ef\u7528\u7684\u8ba1\u7b97\u8d44\u6e90\u4ee5\u4e0d\u540c\u7684\u91cf\u5316\u7cbe\u5ea6\u8fdb\u884c\u63a8\u7406\u3002\u7136\u800c\uff0c\u89c2\u5bdf\u5230\u91cf\u5316\u6821\u51c6\u53c2\u6570\u901a\u5e38\u4e0e\u7279\u5b9a\u7684\u7cbe\u5ea6\u76f8\u5173\u8054\uff0c\u8fd9\u5728\u5f39\u6027\u7cbe\u5ea6\u6821\u51c6\u548c\u8fd0\u884c\u65f6\u7cbe\u5ea6\u5207\u6362\u8fc7\u7a0b\u4e2d\u5e26\u6765\u4e86\u6311\u6218\u3002\u53d7\u6b64\u89c2\u5bdf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u4f4d\u91cf\u5316\u6846\u67b6MoBiQuant\uff0c\u8be5\u6846\u67b6\u6839\u636e\u6807\u8bb0\u654f\u611f\u6027\u8c03\u6574\u5f39\u6027LLM\u63a8\u7406\u7684\u6743\u91cd\u7cbe\u5ea6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bb8\u591a\u9012\u5f52\u6b8b\u5dee\u91cf\u5316\uff0c\u53ef\u4ee5\u8fed\u4ee3\u5730\u91cd\u5efa\u66f4\u9ad8\u7cbe\u5ea6\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u6807\u8bb0\u611f\u77e5\u8def\u7531\u5668\uff0c\u53ef\u4ee5\u52a8\u6001\u9009\u62e9\u6b8b\u5dee\u4f4d\u7247\u6570\u3002MoBiQuant\u53ef\u4ee5\u5b9e\u73b0\u5e73\u6ed1\u7684\u7cbe\u5ea6\u5207\u6362\uff0c\u540c\u65f6\u63d0\u9ad8\u6807\u8bb0\u5f02\u5e38\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoBiQuant\u7684\u65b0\u9896\u6df7\u5408\u4f4d\u91cf\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6839\u636e\u6807\u8bb0\u654f\u611f\u6027\u8c03\u6574\u5f39\u6027LLM\u63a8\u7406\u7684\u6743\u91cd\u7cbe\u5ea6\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u8bb8\u591a\u9012\u5f52\u6b8b\u5dee\u91cf\u5316\uff0c\u53ef\u4ee5\u8fed\u4ee3\u5730\u91cd\u5efa\u66f4\u9ad8\u7cbe\u5ea6\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u6807\u8bb0\u611f\u77e5\u8def\u7531\u5668\uff0c\u53ef\u4ee5\u52a8\u6001\u9009\u62e9\u6b8b\u5dee\u4f4d\u7247\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoBiQuant\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5f39\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u91cd\u590d\u6821\u51c6\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u7279\u5b9a\u4f4d\u6821\u51c6\u7684PTQ\u5728LLaMA3-8B\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "MoBiQuant\u80fd\u591f\u5b9e\u73b0\u5e73\u6ed1\u7684\u7cbe\u5ea6\u5207\u6362\uff0c\u540c\u65f6\u63d0\u9ad8\u6807\u8bb0\u5f02\u5e38\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5f39\u6027LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20205", "abs": "https://arxiv.org/abs/2602.20205", "authors": ["Xiwen Chen", "Wenhui Zhu", "Gen Li", "Xuanzhao Dong", "Yujian Xiong", "Hao Wang", "Peijie Qiu", "Qingquan Song", "Zhipeng Wang", "Shao Tang", "Yalin Wang", "Abolfazl Razi"], "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport", "comment": "Accepted by CVPR2026 (Findings). arXiv admin note: text overlap with arXiv:2503.02175 by other authors", "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.", "code_url": "https://github.com/xiwenc1/OTPrune", "code_stars": 0, "code_last_update": "2026-02-22", "AI": {"tldr": "OTPrune\uff1a\u57fa\u4e8e\u6700\u4f18\u8fd0\u8f93\u7684\u89c6\u89c9token\u526a\u679d\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u8868\u793a\u7684\u6f5c\u5728\u5206\u5e03\u7ed3\u6784\u3002", "method": "OTPrune\u901a\u8fc7\u6700\u5c0f\u5316\u5168\u548c\u526a\u679dtoken\u5206\u5e03\u4e4b\u95f4\u76842-Wasserstein\u8ddd\u79bb\u8fdb\u884c\u526a\u679d\uff0c\u4ee5\u4f18\u5316\u8fd0\u8f93\u3002", "result": "OTPrune\u51cf\u5c11\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5c40\u90e8\u591a\u6837\u6027\u548c\u5168\u5c40\u4ee3\u8868\u6027\u3002", "conclusion": "OTPrune\u5728\u6027\u80fd\u6548\u7387\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.20585", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20585", "abs": "https://arxiv.org/abs/2602.20585", "authors": ["Mo\u00efse Blanchard", "Abhishek Shetty", "Alexander Rakhlin"], "title": "Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness", "comment": null, "summary": "Understanding minimal assumptions that enable learning and generalization is perhaps the central question of learning theory. Several celebrated results in statistical learning theory, such as the VC theorem and Littlestone's characterization of online learnability, establish conditions on the hypothesis class that allow for learning under independent data and adversarial data, respectively. Building upon recent work bridging these extremes, we study sequential decision making under distributional adversaries that can adaptively choose data-generating distributions from a fixed family $U$ and ask when such problems are learnable with sample complexity that behaves like the favorable independent case. We provide a near complete characterization of families $U$ that admit learnability in terms of a notion known as generalized smoothness i.e. a distribution family admits VC-dimension-dependent regret bounds for every finite-VC hypothesis class if and only if it is generalized smooth. Further, we give universal algorithms that achieve low regret under any generalized smooth adversary without explicit knowledge of $U$. Finally, when $U$ is known, we provide refined bounds in terms of a combinatorial parameter, the fragmentation number, that captures how many disjoint regions can carry nontrivial mass under $U$. These results provide a nearly complete understanding of learnability under distributional adversaries. In addition, building upon the surprising connection between online learning and differential privacy, we show that the generalized smoothness also characterizes private learnability under distributional constraints.", "AI": {"tldr": "The research provides a near complete understanding of learnability under distributional adversaries and shows the connection between generalized smoothness and private learnability under distributional constraints.", "motivation": "Understanding minimal assumptions for learning and generalization in learning theory.", "method": "Analyzing sequential decision-making under distributional adversaries, providing a near complete characterization of families U that admit learnability in terms of generalized smoothness. Providing universal algorithms with low regret. Providing refined bounds when U is known.", "result": "Near complete understanding of learnability under distributional adversaries, showing that generalized smoothness characterizes private learnability under distributional constraints.", "conclusion": "The research provides significant insights into learning theory and has implications for private learning under distributional constraints."}}
{"id": "2602.20194", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20194", "abs": "https://arxiv.org/abs/2602.20194", "authors": ["Takato Yasuno"], "title": "FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment", "comment": "10 pages, 4 figures, 2 tables", "summary": "Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\\to$Minor, Good$\\to$Severe, and Minor$\\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u6865\u6881\u9000\u5316\u7684CTMC\u98ce\u9669\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u5171\u4eab\u548c\u6a21\u578b\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u6cbb\u7406\u7ea6\u675f\u4f7f\u5f97\u6865\u6881\u5b9a\u671f\u68c0\u67e5\u8bb0\u5f55\u96be\u4ee5\u5728\u7ec4\u7ec7\u95f4\u5171\u4eab\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u6846\u67b6\u6765\u4f30\u8ba1\u6865\u6881\u9000\u5316\u7684\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08CTMC\uff09\u98ce\u9669\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u6237\u5728\u672c\u5730\u8bad\u7ec3\u57fa\u4e8e\u6865\u6881\u5e74\u9f84\u3001\u6d77\u5cb8\u7ebf\u8ddd\u79bb\u548c\u6865\u9762\u9762\u79ef\u7684\u534f\u53d8\u91cf\u8fdb\u884c\u9000\u5316\u7684\u5bf9\u6570\u7ebf\u6027\u98ce\u9669\u6a21\u578b\u3002\u901a\u8fc7\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5bf9CTMC\u5bf9\u6570\u4f3c\u7136\u8fdb\u884c\u5c40\u90e8\u4f18\u5316\uff0c\u5e76\u5728\u6bcf\u8f6e\u901a\u4fe1\u4e2d\u4ec5\u4e0a\u4f20\u4e00\u4e2a12\u7ef4\u4f2a\u68af\u5ea6\u5411\u91cf\u5230\u4e2d\u592e\u670d\u52a1\u5668\u3002\u670d\u52a1\u5668\u4f7f\u7528\u5177\u6709\u52a8\u91cf\u548c\u68af\u5ea6\u88c1\u526a\u7684\u6837\u672c\u52a0\u6743\u7684\u8054\u90a6\u5e73\u5747\uff08FedAvg\uff09\u805a\u5408\u7528\u6237\u66f4\u65b0\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u5f02\u6784\u7528\u6237\u8868\u73b0\u51fa\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136\u7684\u4e00\u81f4\u6536\u655b\uff0c\u968f\u7740\u7528\u6237\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u805a\u5408\u68af\u5ea6\u8303\u6570\u9010\u6e10\u51cf\u5c0f\u3002\u6b64\u5916\uff0c\u8054\u90a6\u66f4\u65b0\u673a\u5236\u63d0\u4f9b\u4e86\u81ea\u7136\u7684\u53c2\u4e0e\u6fc0\u52b1\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u5171\u4eab\u6280\u672f\u6807\u51c6\u5e73\u53f0\u4e0a\u6ce8\u518c\u5176\u672c\u5730\u68c0\u67e5\u6570\u636e\u96c6\uff0c\u5e76\u5b9a\u671f\u83b7\u5f97\u5168\u5c40\u57fa\u51c6\u53c2\u6570\u7684\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u8bc1\u636e\u7684\u751f\u547d\u5468\u671f\u89c4\u5212\uff0c\u800c\u65e0\u9700\u653e\u5f03\u6570\u636e\u4e3b\u6743\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6865\u6881\u5b9a\u671f\u68c0\u67e5\u8bb0\u5f55\u7684\u6570\u636e\u5171\u4eab\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6570\u636e\u4e3b\u6743\u4fdd\u62a4\u3002"}}
{"id": "2602.20291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6570\u636e\u53ef\u89c6\u5316\u53cd\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\uff0c\u63d0\u9ad8\u53ef\u89c6\u5316\u8d28\u91cf\u548c\u7528\u6237\u7d20\u517b\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u5728\u79d1\u5b66\u4f20\u64ad\u3001\u65b0\u95fb\u4e1a\u548c\u65e5\u5e38\u51b3\u7b56\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5b58\u5728\u9519\u8bef\uff0c\u8fd9\u4e9b\u9519\u8bef\u53ef\u80fd\u4f1a\u626d\u66f2\u89e3\u91ca\u6216\u8bef\u5bfc\u53d7\u4f17\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u89c6\u89c9\u5316\u68c0\u67e5\u5668\u53ef\u4ee5\u6807\u8bb0\u8fdd\u89c4\u884c\u4e3a\uff0c\u4f46\u5b83\u4eec\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\uff0c\u5e76\u4e14\u4e0d\u63d0\u51fa\u6709\u610f\u4e49\u7684\u8bbe\u8ba1\u66f4\u6539\u3002\u76f4\u63a5\u67e5\u8be2\u901a\u7528LLMs\u5173\u4e8e\u53ef\u89c6\u5316\u8d28\u91cf\u662f\u4e0d\u53ef\u9760\u7684\uff1a\u7f3a\u4e4f\u9075\u5faa\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\u7684\u8bad\u7ec3\uff0c\u5b83\u4eec\u901a\u5e38\u4f1a\u63d0\u4f9b\u4e0d\u4e00\u81f4\u6216\u9519\u8bef\u7684\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u56fe\u8868\u53cd\u6e32\u67d3\u3001\u81ea\u52a8\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8bbe\u8ba1\u53cd\u9988\u3002\u7cfb\u7edf\u4ece\u56fe\u50cf\u4e2d\u91cd\u5efa\u56fe\u8868\u7ed3\u6784\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u53ef\u89c6\u5316\u7814\u7a76\u786e\u7acb\u7684\u539f\u5219\u7684\u5177\u4f53\u4fee\u6539\u5efa\u8bae\u3002", "result": "\u5728Chart2Code\u57fa\u51c6\u6d4b\u8bd5\u76841000\u4e2a\u56fe\u8868\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7cfb\u7edf\u751f\u6210\u4e8610452\u4e2a\u8bbe\u8ba1\u5efa\u8bae\uff0c\u8fd9\u4e9b\u5efa\u8bae\u805a\u7c7b\u621010\u4e2a\u6709\u610f\u4e49\u7684\u7c7b\u522b\uff08\u4f8b\u5982\uff0c\u8f74\u683c\u5f0f\u5316\u3001\u989c\u8272\u53ef\u8bbf\u95ee\u6027\u3001\u56fe\u4f8b\u4e00\u81f4\u6027\uff09\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u539f\u5219\u7684\u89c6\u89c9\u8bbe\u8ba1\u53cd\u9988\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u667a\u80fd\u548c\u6613\u4e8e\u8bbf\u95ee\u7684\u7f16\u5199\u5de5\u5177\u6253\u5f00\u4e86\u5927\u95e8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u53ef\u89c6\u5316\u53cd\u9988\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7LLM\u6280\u672f\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u53ef\u89c6\u5316\u8d28\u91cf\u548c\u53ef\u89c6\u5316\u7d20\u517b\u3002"}}
{"id": "2602.20459", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20459", "abs": "https://arxiv.org/abs/2602.20459", "authors": ["Anirudh Ajith", "Amanpreet Singh", "Jay DeYoung", "Nadav Kunievsky", "Austin C. Kozlowski", "Oyvind Tafjord", "James Evans", "Daniel S. Weld", "Tom Hope", "Doug Downey"], "title": "PreScience: A Benchmark for Forecasting Scientific Contributions", "comment": "10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm", "summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.", "AI": {"tldr": "PreScience\u57fa\u51c6\u901a\u8fc7\u5206\u89e3\u7814\u7a76\u8fc7\u7a0b\u9884\u6d4b\u79d1\u5b66\u8fdb\u6b65\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u9884\u6d4b\u79d1\u5b66\u8fdb\u6b65\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc6\u522b\u5408\u4f5c\u8005\u548c\u6709\u5f71\u54cd\u529b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u9884\u6d4b\u54ea\u4e9b\u95ee\u9898\u548c\u65b9\u6cd5\u5c06\u6210\u4e3a\u672a\u6765\u4e2d\u5fc3\u3002", "method": "\u63d0\u51faPreScience\u57fa\u51c6\uff0c\u5c06\u7814\u7a76\u8fc7\u7a0b\u5206\u89e3\u4e3a\u56db\u4e2a\u76f8\u4e92\u4f9d\u5b58\u7684\u751f\u6210\u4efb\u52a1\uff1a\u5408\u4f5c\u8005\u9884\u6d4b\u3001\u5148\u524d\u5de5\u4f5c\u9009\u62e9\u3001\u8d21\u732e\u751f\u6210\u548c\u5f71\u54cd\u9884\u6d4b\u3002", "result": "\u53d1\u73b0\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5b58\u5728\u5927\u91cf\u63d0\u5347\u7a7a\u95f4\uff0c\u4f8b\u5982\u5728\u8d21\u732e\u751f\u6210\u4e2d\uff0c\u524d\u6cbfLLM\u4e0e\u771f\u5b9e\u60c5\u51b5\uff08GPT-5\uff0c\u5e73\u5747\u5f97\u5206\u4e3a5.6\u5206\uff09\u53ea\u6709\u9002\u5ea6\u7684\u76f8\u4f3c\u6027\u3002\u5f53\u7ec4\u621012\u4e2a\u6708\u7684\u7aef\u5230\u7aef\u79d1\u5b66\u751f\u4ea7\u6a21\u62df\u65f6\uff0c\u751f\u6210\u7684\u5408\u6210\u8bed\u6599\u5e93\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u90fd\u7cfb\u7edf\u5730\u4f4e\u4e8e\u540c\u4e00\u65f6\u671f\u7684\u4eba\u7c7b\u4f5c\u8005\u7814\u7a76\u3002", "conclusion": "PreScience\u57fa\u51c6\u4e3a\u79d1\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2602.20611", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20611", "abs": "https://arxiv.org/abs/2602.20611", "authors": ["Daniel Zhou", "Sudipto Banerjee"], "title": "Amortized Bayesian inference for actigraph time sheet data from mobile devices", "comment": "40 pages, 7 figures", "summary": "Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u7684\u6d3b\u52a8\u8ba1\u6570\u636e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b0\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u79fb\u52a8\u6570\u636e\u6280\u672f\u5728\u5065\u5eb7\u53d8\u91cf\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5206\u6790\u9ad8\u5206\u8fa8\u7387\u6d3b\u52a8\u8ba1\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c\u5206\u5c42\u52a8\u6001\u7ebf\u6027\u6a21\u578b\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u8fdb\u884c\u6570\u636e\u5206\u6790\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5b9e\u73b0\u5bf9\u6d3b\u52a8\u8ba1\u65f6\u95f4\u8868\u7684\u6982\u7387\u63d2\u8865\uff0c\u5e76\u5b66\u4e60\u89e3\u91ca\u53d8\u91cf\u5bf9\u53d7\u8bd5\u8005\u52a0\u901f\u5ea6\uff08MAG\uff09\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u6d3b\u52a8\u8ba1\u6570\u636e\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5065\u5eb7\u7814\u7a76\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.20294", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20294", "abs": "https://arxiv.org/abs/2602.20294", "authors": ["Yu Li", "Pranav Narayanan Venkit", "Yada Pruksachatkun", "Chien-Sheng Wu"], "title": "InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation", "comment": null, "summary": "Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbf\u8c08\u7684\u5927\u89c4\u6a21\u4e2a\u6027\u6a21\u62df\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u6a21\u62df\u771f\u5b9e\u7684\u4e2a\u6027\uff0c\u9700\u8981\u5c06\u751f\u6210\u8fc7\u7a0b\u5efa\u7acb\u5728\u771f\u5b9e\u7684\u4e2a\u4eba\u6570\u636e\u57fa\u7840\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbf\u8c08\u7684\u5927\u89c4\u6a21\u4e2a\u6027\u6a21\u62df\u8bc4\u4f30\u6846\u67b6\u3002\u4ece1,000\u4f4d\u516c\u5171\u4eba\u7269\u4e2d\u63d0\u53d6\u4e8623,000\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u8bbf\u8c08\u8bb0\u5f55\uff0c\u5171671,000\u4e2a\u95ee\u7b54\u5bf9\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u5185\u5bb9\u76f8\u4f3c\u5ea6\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u4e2a\u6027\u5339\u914d\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4fdd\u7559\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u8bbf\u8c08\u6570\u636e\u7684\u65b9\u6cd5\u5728\u5185\u5bb9\u76f8\u4f3c\u5ea6\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u4e2a\u6027\u5339\u914d\u548c\u4e8b\u5b9e\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4f20\u8bb0\u8d44\u6599\u6216\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bc4\u4f30\u6846\u67b6\u53ef\u4ee5\u57fa\u4e8e\u5e94\u7528\u9700\u6c42\u8fdb\u884c\u539f\u5219\u6027\u7684\u65b9\u6cd5\u9009\u62e9\uff0c\u4e3a\u4e2a\u6027\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.20312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "code_url": "https://github.com/frozzzen3/N4MC", "code_stars": 0, "code_last_update": "2026-02-21", "AI": {"tldr": "N4MC is a 4D neural compression framework that efficiently compresses time-varying mesh sequences and outperforms existing methods", "motivation": "Compressing time-varying mesh sequences efficiently", "method": "Learning motion compensation using a 4D tensor representation and a transformer-based interpolation model", "result": "Outperforms state-of-the-art in rate-distortion performance, enabling real-time decoding of 4D mesh sequences", "conclusion": "N4MC is a novel and effective 4D neural compression framework for time-varying mesh sequences"}}
{"id": "2602.20652", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20652", "abs": "https://arxiv.org/abs/2602.20652", "authors": ["Brandon R. Feng", "Brian J. Reich", "Daniel Beaglehole", "Xihaier Luo", "David Keetae Park", "Shinjae Yoo", "Zhechao Huang", "Xueyu Mao", "Olcay Boz", "Jungeum Kim"], "title": "DANCE: Doubly Adaptive Neighborhood Conformal Estimation", "comment": null, "summary": "The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.", "AI": {"tldr": "\u63d0\u51faDANCE\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e24\u79cd\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u6709\u6548\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "motivation": "\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u6570\u636e\u8868\u793a\u7c7b\u578b\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u9700\u8981\u51c6\u786e\u91cf\u5316\u8fd9\u4e9b\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDANCE\u7684\u53cc\u5c40\u90e8\u81ea\u9002\u5e94\u6700\u8fd1\u90bb\u57fa\u4e8e\u7684\u4e00\u81f4\u6027\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e24\u79cd\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u76f4\u63a5\u4f7f\u7528\u6570\u636e\u7684\u5d4c\u5165\u8868\u793a\u3002DANCE\u9996\u5148\u5bf9\u5d4c\u5165\u5c42\u62df\u5408\u4e00\u4e2a\u4efb\u52a1\u81ea\u9002\u5e94\u6838\u56de\u5f52\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u6838\u7a7a\u95f4\u4ea7\u751f\u6700\u7ec8\u7684\u9884\u6d4b\u96c6\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u672c\u5730\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u548c\u96f6\u6837\u672c\u4e00\u81f4\u6027\u57fa\u7ebf\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86DANCE\u5728\u5404\u4e2a\u6570\u636e\u96c6\u4e0a\u5177\u6709\u4f18\u8d8a\u7684\u96c6\u5927\u5c0f\u6548\u7387\u548c\u9c81\u68d2\u6027\u7684\u7ed3\u5408\u3002", "conclusion": "DANCE\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u4e00\u81f4\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.20300", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u67e5\u8be2\u7279\u5f81\u4e0eLLM\u5e7b\u89c9\u98ce\u9669\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u63a2\u8ba8\u67e5\u8be2\u5f62\u5f0f\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa22\u7ef4\u67e5\u8be2\u7279\u5f81\u5411\u91cf\uff0c\u5206\u6790369,837\u4e2a\u771f\u5b9e\u67e5\u8be2\uff0c\u63ed\u793a\u67e5\u8be2\u7279\u5f81\u4e0e\u5e7b\u89c9\u98ce\u9669\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u67e5\u8be2\u7279\u5f81\uff08\u5982\u6df1\u5c42\u5d4c\u5957\u4ece\u53e5\u548c\u4e0b\u5b9a\u4e49\u4e0d\u5145\u5206\uff09\u4e0e\u66f4\u9ad8\u7684\u5e7b\u89c9\u503e\u5411\u76f8\u5173\uff0c\u800c\u660e\u786e\u7684\u610f\u56fe\u63a5\u5730\u548c\u53ef\u56de\u7b54\u6027\u4e0e\u66f4\u4f4e\u7684\u5e7b\u89c9\u7387\u76f8\u5173\u3002", "conclusion": "\u67e5\u8be2\u7279\u5f81\u4e0e\u5e7b\u89c9\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff0c\u4e3a\u5f15\u5bfc\u67e5\u8be2\u91cd\u5199\u548c\u672a\u6765\u5e72\u9884\u7814\u7a76\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2602.20222", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20222", "abs": "https://arxiv.org/abs/2602.20222", "authors": ["Victor Morel", "Cristiana Santos", "Pontus Carlsson", "Joel Ahlinder", "Romaric Duvignau"], "title": "The TCF doesn't really A(A)ID -- Automatic Privacy Analysis and Legal Compliance of TCF-based Android Applications", "comment": "Accepted for publication at PETS'26", "summary": "The Transparency and Consent Framework (TCF), developed by the Interactive Advertising Bureau (IAB) Europe, provides a de facto standard for requesting, recording, and managing user consent from European end-users. This framework has previously been found to infringe European data protection law and has subsequently been regularly updated. Previous research on the TCF focused exclusively on web contexts, with no attention given to its implementation in mobile applications. No work has systematically studied the privacy implications of the TCF on Android apps. To address this gap, we investigate the prevalence of the TCF in popular Android apps from the Google Play Store, and assess whether these apps respect users' consent banner choices. By scraping and downloading 4482 of the most popular Google Play Store apps on an emulated Android device, we automatically determine which apps use the TCF, automatically interact with consent banners, and analyze the apps' traffic in two different stages, passive (post choices) and active (during banner interaction and post choices).\n  We found that 576 (12.85%) of the 4482 downloadable apps in our dataset implemented the TCF, and we identified potential privacy violations within this subset. In 15 (2.6%) of these apps, users' choices are stored only when consent is granted. Users who refuse consent are shown the consent banner again each time they launch the app. Network traffic analysis conducted during the passive stage reveals that 66.2% of the analyzed TCF-based apps share personal data, through the Android Advertising ID (AAID), in the absence of a lawful basis for processing. 55.3% of apps analyzed during the active stage share AAID before users interact with the apps' consent banners, violating the prior consent requirement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20328", "categories": ["cs.CV", "eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdr\u00f3n-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "\u63d0\u51fa\u4e86GSNR\uff0c\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u56fe\u50cf\u53cd\u6f14\u95ee\u9898\u56e0\u611f\u77e5\u77e9\u9635\u7684\u975e\u5e73\u51e1\u96f6\u7a7a\u95f4\u800c\u5177\u6709\u75c5\u6001\u6027\uff0c\u5bfc\u81f4\u4e0e\u6d4b\u91cf\u4e00\u81f4\u7684\u89e3\u6709\u65e0\u9650\u591a\u4e2a\u3002\u5e38\u89c1\u7684\u56fe\u50cf\u5148\u9a8c\u4fc3\u8fdb\u4e86\u56fe\u50cf\u6d41\u5f62\u4e0a\u7684\u89e3\uff0c\u5982\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u6027\u6216\u5f97\u5206\u51fd\u6570\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u5148\u9a8c\u6ca1\u6709\u7ea6\u675f\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u5b83\u4eec\u53ef\u80fd\u4f1a\u5bf9\u91cd\u5efa\u9020\u6210\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u65e8\u5728\u5c06\u6709\u610f\u4e49\u7684\u96f6\u7a7a\u95f4\u4fe1\u606f\u7eb3\u5165\u91cd\u5efa\u6846\u67b6\u3002\u53d7\u56fe\u4e0a\u5e73\u6ed1\u56fe\u50cf\u8868\u793a\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u793a\uff08GSNR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4ec5\u5bf9\u4e0d\u53ef\u89c1\u5206\u91cf\u65bd\u52a0\u7ed3\u6784\u7684\u673a\u5236\u3002\u7279\u522b\u662f\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u6211\u4eec\u5728\u96f6\u7a7a\u95f4\u4fe1\u53f7\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u96f6\u9650\u5236\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u8be5\u7b97\u5b50\u7f16\u7801\u4e86\u76f8\u90bb\u50cf\u7d20\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ece $p$-\u5e73\u6ed1\u56fe\u8c31\u6a21\u5f0f\uff08\u6700\u4f4e\u56fe\u9891\u7387\uff09\u5230\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635\u3002\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\uff1ai\uff09\u901a\u8fc7\u96f6\u7a7a\u95f4\u56fe\u6b63\u5219\u5316\u5668\u63d0\u9ad8\u6536\u655b\u6027\uff0cii\uff09\u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\uff0c\u5373\u591a\u5c11\u96f6\u7a7a\u95f4\u65b9\u5dee\u88ab $p$ \u6a21\u5f0f\u6355\u83b7\uff0c\u4ee5\u53caiii\uff09\u9ad8\u5ea6\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5373\u8fd9\u4e9b\u6a21\u5f0f\u53ef\u4ee5\u4ece\u6d4b\u91cf\u4e2d\u63a8\u65ad\u5f97\u591a\u597d\u3002GSNR\u88ab\u7eb3\u5165\u5df2\u77e5\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u4f8b\u5982PnP\u3001DIP\u548c\u6269\u6563\u6c42\u89e3\u5668\uff0c\u5728\u56db\u79cd\u573a\u666f\u4e2d\uff1a\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u53bb\u9a6c\u8d5b\u514b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u4e0e\u57fa\u7ebf\u516c\u5f0f\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9ad8\u8fbe4.3 dB\uff0c\u4e0e\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u5728PSNR\u65b9\u9762\u63d0\u9ad8\u4e86\u9ad8\u8fbe1 dB\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGSNR\u7684\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u793a\u673a\u5236\uff0c\u8be5\u673a\u5236\u4ec5\u5bf9\u4e0d\u53ef\u89c1\u5206\u91cf\u65bd\u52a0\u7ed3\u6784\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u96f6\u9650\u5236\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u8be5\u7b97\u5b50\u7f16\u7801\u4e86\u76f8\u90bb\u50cf\u7d20\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ece $p$-\u5e73\u6ed1\u56fe\u8c31\u6a21\u5f0f\u5230\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u53bb\u9a6c\u8d5b\u514b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7b49\u573a\u666f\u4e2d\u4e0e\u57fa\u7ebf\u516c\u5f0f\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9ad8\u8fbe4.3 dB\uff0c\u4e0e\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u5728PSNR\u65b9\u9762\u63d0\u9ad8\u4e86\u9ad8\u8fbe1 dB\u3002", "conclusion": "GSNR\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2602.21039", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21039", "abs": "https://arxiv.org/abs/2602.21039", "authors": ["Rafael Hanashiro", "Abhishek Shetty", "Patrick Jaillet"], "title": "Is Multi-Distribution Learning as Easy as PAC Learning: Sharp Rates with Bounded Label Noise", "comment": null, "summary": "Towards understanding the statistical complexity of learning from heterogeneous sources, we study the problem of multi-distribution learning. Given $k$ data sources, the goal is to output a classifier for each source by exploiting shared structure to reduce sample complexity. We focus on the bounded label noise setting to determine whether the fast $1/\u03b5$ rates achievable in single-task learning extend to this regime with minimal dependence on $k$. Surprisingly, we show that this is not the case. We demonstrate that learning across $k$ distributions inherently incurs slow rates scaling with $k/\u03b5^2$, even under constant noise levels, unless each distribution is learned separately. A key technical contribution is a structured hypothesis-testing framework that captures the statistical cost of certifying near-optimality under bounded noise-a cost we show is unavoidable in the multi-distribution setting.\n  Finally, we prove that when competing with the stronger benchmark of each distribution's optimal Bayes error, the sample complexity incurs a \\textit{multiplicative} penalty in $k$. This establishes a \\textit{statistical} separation between random classification noise and Massart noise, highlighting a fundamental barrier unique to learning from multiple sources.", "AI": {"tldr": "\u591a\u5206\u5e03\u5b66\u4e60\u5b58\u5728\u56fa\u6709\u96be\u5ea6\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0ek\u6210\u6b63\u6bd4\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u7edf\u8ba1\u65b9\u6cd5", "motivation": "\u7406\u89e3\u4ece\u5f02\u6784\u6765\u6e90\u5b66\u4e60\u7edf\u8ba1\u590d\u6742\u5ea6", "method": "\u7814\u7a76\u591a\u5206\u5e03\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u5171\u4eab\u7ed3\u6784\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6", "result": "\u8bc1\u660e\u5728\u591a\u4e2a\u5206\u5e03\u5b66\u4e60\u60c5\u51b5\u4e0b\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0ek\u6210\u6b63\u6bd4\uff0c\u5b58\u5728\u7edf\u8ba1\u5206\u79bb", "conclusion": "\u591a\u5206\u5e03\u5b66\u4e60\u5b58\u5728\u56fa\u6709\u96be\u5ea6\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u7edf\u8ba1\u65b9\u6cd5"}}
{"id": "2602.20332", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "QueryBandits \u662f\u4e00\u79cd\u6709\u6548\u7684 LLM \u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "LLMs \u5728\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u540c\u65f6\uff0c\u51fa\u73b0\u4e86\u66f4\u9891\u7e41\u7684\u5e7b\u89c9\u73b0\u8c61\u3002\u5927\u591a\u6570\u7f13\u89e3\u5de5\u4f5c\u96c6\u4e2d\u5728\u5f00\u6e90\u6a21\u578b\u7684\u540e\u5904\u7406\u68c0\u6d4b\u548c\u53c2\u6570\u7f16\u8f91\u4e0a\u3002\u7136\u800c\uff0c\u9488\u5bf9\u95ed\u6e90\u6a21\u578b\u5e7b\u89c9\u7684\u7814\u7a76\u5374\u5f88\u5c11\uff0c\u800c\u95ed\u6e90\u6a21\u578b\u5728\u673a\u6784\u90e8\u7f72\u4e2d\u5360\u7edd\u5927\u591a\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a QueryBandits \u7684\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u8d4c\u535a\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u7ecf\u9a8c\u9a8c\u8bc1\u548c\u6821\u51c6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u81ea\u9002\u5e94\u5730\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u3002", "result": "\u5728 16 \u4e2a\u95ee\u7b54\u573a\u666f\u4e2d\uff0cQueryBandit\uff08Thompson Sampling\uff09\u7684\u80dc\u7387\u8fbe\u5230 87.5%\uff0c\u8d85\u8fc7\u65e0\u91cd\u5199\u57fa\u7ebf\uff0c\u5206\u522b\u6bd4\u96f6\u6837\u672c\u9759\u6001\u7b56\u7565\uff08\u5982\u91ca\u4e49\u6216\u6269\u5c55\uff09\u9ad8 42.6% \u548c 60.3%\u3002\u6b64\u5916\uff0c\u6240\u6709\u4e0a\u4e0b\u6587\u8d4c\u535a\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u8d4c\u535a\uff0c\u7279\u5f81\u65b9\u5dee\u8d8a\u5927\uff0c\u81c2\u9009\u62e9\u65b9\u5dee\u4e5f\u8d8a\u5927\u3002\u8fd9\u8bc1\u5b9e\u4e86\u6ca1\u6709\u5355\u4e00\u7684\u91cd\u5199\u7b56\u7565\u5bf9\u6240\u6709\u67e5\u8be2\u90fd\u6700\u4f18\u3002\u8fd8\u53d1\u73b0\u67d0\u4e9b\u9759\u6001\u7b56\u7565\u7684\u7d2f\u79ef\u540e\u6094\u66f4\u9ad8\uff0c\u8868\u660e\u50f5\u5316\u7684\u67e5\u8be2\u91cd\u5199\u7b56\u7565\u4f1a\u52a0\u5267\u5e7b\u89c9\u3002\u901a\u8fc7 QueryBandits \u5728\u8bed\u4e49\u7279\u5f81\u4e0a\u5b66\u4e60\u5728\u7ebf\u7b56\u7565\uff0c\u53ef\u4ee5\u901a\u8fc7\u524d\u5411\u4f20\u9012\u673a\u5236\u7eaf\u7cb9\u5730\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u9002\u5e94\u3002", "conclusion": "QueryBandits \u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11 LLMs \u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u9002\u5e94\u3002"}}
{"id": "2602.20207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20207", "abs": "https://arxiv.org/abs/2602.20207", "authors": ["Shrestha Datta", "Hongfu Liu", "Anshuman Chhabra"], "title": "Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis", "comment": null, "summary": "Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u68af\u5ea6\u5206\u6790\uff08LGA\uff09\u627e\u5230\u56fa\u5b9a\u9ec4\u91d1\u5c42\uff0c\u63d0\u9ad8\u7f16\u8f91\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8fdb\u884c\u77e5\u8bc6\u7f16\u8f91\uff0c\u65e8\u5728\u66f4\u65b0\u6a21\u578b\u5bf9\u7279\u5b9a\u67e5\u8be2\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u5728\u6240\u6709\u5176\u4ed6\u8f93\u5165\u4e0a\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c42\u68af\u5ea6\u5206\u6790\uff08LGA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u9ad8\u6548\u5730\u4f30\u8ba1\u9ec4\u91d1\u5c42\uff0c\u907f\u514d\u5728\u591a\u6b21\u7f16\u8f91\u8fd0\u884c\u4e2d\u8fdb\u884c\u5927\u91cf\u8bd5\u9519\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540cLLM\u7c7b\u578b\u548c\u591a\u79cd\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5047\u8bbe\u5b58\u5728\u56fa\u5b9a\u9ec4\u91d1\u5c42\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u8fd1\u4f3c\u7684\u6700\u4f73\u7f16\u8f91\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5047\u8bbe\u3002"}}
{"id": "2602.21130", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21130", "abs": "https://arxiv.org/abs/2602.21130", "authors": ["Natalia da Silva", "Dianne Cook", "Eun-Kyung Lee"], "title": "An Enhanced Projection Pursuit Tree Classifier with Visual Methods for Assessing Algorithmic Improvements", "comment": null, "summary": "This paper presents enhancements to the projection pursuit tree classifier and visual diagnostic methods for assessing their impact in high dimensions. The original algorithm uses linear combinations of variables in a tree structure where depth is constrained to be less than the number of classes -- a limitation that proves too rigid for complex classification problems. Our extensions improve performance in multi-class settings with unequal variance-covariance structures and nonlinear class separations by allowing more splits and more flexible class groupings in the projection pursuit computation. Proposing algorithmic improvements is straightforward; demonstrating their actual utility is not. We therefore develop two visual diagnostic approaches to verify that the enhancements perform as intended. Using high-dimensional visualization techniques, we examine model fits on benchmark datasets to assess whether the algorithm behaves as theorized. An interactive web application enables users to explore the behavior of both the original and enhanced classifiers under controlled scenarios. The enhancements are implemented in the R package PPtreeExt.", "AI": {"tldr": "\u6539\u8fdb\u4e86\u6295\u5f71\u8ffd\u6c42\u6811\u5206\u7c7b\u5668\uff0c\u63d0\u9ad8\u4e86\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6295\u5f71\u8ffd\u6c42\u6811\u5206\u7c7b\u5668\u548c\u53ef\u89c6\u5316\u8bca\u65ad\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u6295\u5f71\u8ffd\u6c42\u6811\u5206\u7c7b\u5668\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e24\u79cd\u53ef\u89c6\u5316\u8bca\u65ad\u65b9\u6cd5\u6765\u9a8c\u8bc1\u6539\u8fdb\u7684\u6548\u679c\u3002", "result": "\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5728\u591a\u7c7b\u8bbe\u7f6e\u3001\u4e0d\u7b49\u65b9\u5dee\u534f\u65b9\u5dee\u7ed3\u6784\u548c\u975e\u7ebf\u6027\u7c7b\u522b\u5206\u79bb\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7b97\u6cd5\u548c\u53ef\u89c6\u5316\u8bca\u65ad\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6295\u5f71\u8ffd\u6c42\u6811\u5206\u7c7b\u5668\u5728\u9ad8\u7ef4\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.20336", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20336", "abs": "https://arxiv.org/abs/2602.20336", "authors": ["Radoslaw Roszczyk", "Pawel Tecza", "Maciej Stodolski", "Krzysztof Siwek"], "title": "Natural Language Processing Models for Robust Document Categorization", "comment": "13 pages, 1 fiure, 5 tables", "summary": "This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.\n  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.\n  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21160", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.21160", "abs": "https://arxiv.org/abs/2602.21160", "authors": ["Mame Diarra Toure", "David A. Stephens"], "title": "Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions", "comment": "8 pages, 17 figures", "summary": "In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=\u03c3_k^{2}/(2\u03bc_k)$, with $\u03bc_k{=}\\mathbb{E}[p_k]$ and $\u03c3_k^2{=}\\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/\u03bc_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\\sum_k C_k \\approx \\mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\\% over MI and 56.2\\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20210", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20571", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20571", "abs": "https://arxiv.org/abs/2602.20571", "authors": ["Ayush Sawarni", "Jiyuan Tan", "Vasilis Syrgkanis"], "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation", "comment": null, "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.", "AI": {"tldr": "CausalReasoningBenchmark\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\u6027\u80fd\u7684\u57fa\u51c6\uff0c\u5b83\u63ed\u793a\u4e86\u7814\u7a76\u8bbe\u8ba1\u7ec6\u8282\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bb8\u591a\u81ea\u52a8\u56e0\u679c\u63a8\u7406\u7684\u57fa\u51c6\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u662f\u57fa\u4e8e\u5355\u4e2a\u6570\u503c\u8f93\u51fa\uff0c\u5982\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u6df7\u6dc6\u4e86\u56e0\u679c\u5206\u6790\u4e2d\u7684\u4e24\u4e2a\u4e0d\u540c\u6b65\u9aa4\uff1a\u8bc6\u522b-\u5728\u7ed9\u5b9a\u5047\u8bbe\u4e0b\u5236\u5b9a\u6709\u6548\u7684\u79d1\u7814\u8bbe\u8ba1-\u4f30\u8ba1-\u5728\u6709\u9650\u6570\u636e\u4e0a\u5b9e\u73b0\u8be5\u8bbe\u8ba1\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CausalReasoningBenchmark\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b173\u4e2a\u67e5\u8be2\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e86138\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6765\u81ea85\u7bc7\u540c\u884c\u8bc4\u5ba1\u7684\u7814\u7a76\u8bba\u6587\u548c\u56db\u672c\u5e7f\u6cdb\u4f7f\u7528\u7684\u56e0\u679c\u63a8\u7406\u6559\u79d1\u4e66\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u67e5\u8be2\uff0c\u7cfb\u7edf\u5fc5\u987b\u751f\u6210\uff08i\uff09\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8bc6\u522b\u89c4\u8303\uff0c\u5176\u4e2d\u5305\u542b\u7b56\u7565\u3001\u5904\u7406\u3001\u7ed3\u679c\u548c\u63a7\u5236\u53d8\u91cf\u4ee5\u53ca\u6240\u6709\u8bbe\u8ba1\u7279\u5b9a\u5143\u7d20\uff0c\u4ee5\u53ca\uff08ii\uff09\u4e00\u4e2a\u70b9\u4f30\u8ba1\u548c\u6807\u51c6\u8bef\u5dee\u3002\u901a\u8fc7\u5206\u522b\u8bc4\u5206\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u53ef\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bca\u65ad\uff1a\u5b83\u533a\u5206\u4e86\u56e0\u679c\u63a8\u7406\u5931\u8d25\u548c\u6570\u503c\u6267\u884c\u9519\u8bef\u3002", "result": "\u57fa\u51c6\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u8be5\u6a21\u578b\u572884%\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u5730\u8bc6\u522b\u4e86\u9ad8\u7ea7\u7b56\u7565\uff0c\u4f46\u5b8c\u6574\u7684\u8bc6\u522b\u89c4\u8303\u6b63\u786e\u7387\u964d\u81f3\u53ea\u670930%\uff0c\u8868\u660e\u74f6\u9888\u5728\u4e8e\u7814\u7a76\u8bbe\u8ba1\u7684\u7ec6\u5fae\u7ec6\u8282\uff0c\u800c\u4e0d\u662f\u8ba1\u7b97\u3002CausalReasoningBenchmark\u5728Hugging Face\u4e0a\u516c\u5f00\u63d0\u4f9b\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u7a33\u5065\u7684\u81ea\u52a8\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "CausalReasoningBenchmark\u6709\u52a9\u4e8e\u63d0\u9ad8\u81ea\u52a8\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\u7684\u7a33\u5065\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u7814\u7a76\u8bbe\u8ba1\u7ec6\u8282\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.20595", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20595", "abs": "https://arxiv.org/abs/2602.20595", "authors": ["Longxiang Wang", "Xiang Zheng", "Xuhao Zhang", "Yao Zhang", "Ye Wu", "Cong Wang"], "title": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services", "comment": null, "summary": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.", "AI": {"tldr": "OptiLeak\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5fae\u8c03\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86LLM\u670d\u52a1\u6846\u67b6\u4e2d\u7684\u63d0\u793a\u6cc4\u9732\u98ce\u9669\u3002", "motivation": "\u591a\u79df\u6237LLM\u670d\u52a1\u6846\u67b6\u5e7f\u6cdb\u91c7\u7528\u5171\u4eab\u952e\u503c\u7f13\u5b58\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u8fd9\u4e5f\u521b\u9020\u4e86\u4fa7\u4fe1\u9053\u6f0f\u6d1e\uff0c\u4f7f\u63d0\u793a\u6cc4\u9732\u653b\u51fb\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOptiLeak\u7684\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u6765\u6700\u5927\u5316\u63d0\u793a\u91cd\u5efa\u6548\u7387\u3002", "result": "\u5728\u533b\u7597\u548c\u91d1\u878d\u9886\u57df\u7684\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptiLeak\u5c06\u5e73\u5747\u6bcftoken\u7684\u8bf7\u6c42\u6b21\u6570\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u51cf\u5c11\u4e86\u9ad8\u8fbe12.48\u500d\u3002", "conclusion": "\u57fa\u4e8e\u7f13\u5b58\u7684\u63d0\u793a\u6cc4\u9732\u6bd4\u4e4b\u524d\u62a5\u9053\u7684\u66f4\u5177\u5a01\u80c1\uff0c\u5f3a\u8c03\u4e86\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7f13\u5b58\u9694\u79bb\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.20354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "code_url": "https://github.com/TheProParadox/3dspa_code", "code_stars": 1, "code_last_update": "2026-02-19", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a3DSPA\u7684\u81ea\u52a8\u8bc4\u4f30\u89c6\u9891\u771f\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548cDINO\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u771f\u5b9e\u6027\u8bc4\u4f30\u3002", "motivation": "AI\u89c6\u9891\u751f\u6210\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u624b\u52a8\u8fc7\u7a0b\uff0c\u9700\u8981\u4eba\u7c7b\u6ce8\u91ca\u6216\u5b9a\u5236\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u8303\u56f4\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u89c6\u9891\u771f\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u540d\u4e3a3DSPA\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e863D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548cDINO\u8bed\u4e49\u7279\u5f81\uff0c\u4ee5\u7edf\u4e00\u8868\u793a\u89c6\u9891\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3DSPA\u53ef\u4ee5\u53ef\u9760\u5730\u8bc6\u522b\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u7684\u89c6\u9891\uff0c\u5bf9\u8fd0\u52a8\u4f2a\u5f71\u66f4\u654f\u611f\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u89c6\u9891\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "\u901a\u8fc7\u4e30\u5bcc\u57fa\u4e8e\u8f68\u8ff9\u7684\u8868\u793a\uff0c\u7ed3\u54083D\u8bed\u4e49\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2602.20433", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20433", "abs": "https://arxiv.org/abs/2602.20433", "authors": ["Atharva Kulkarni", "Jacob Mitchell Springer", "Arjun Subramonian", "Swabha Swayamdipta"], "title": "Disentangling Geometry, Performance, and Training in Language Models", "comment": null, "summary": "Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.", "AI": {"tldr": "\u7814\u7a76Transformer\u6743\u91cd\u51e0\u4f55\u5c5e\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u73b0\u6709\u6307\u6807\u4e3b\u8981\u53cd\u6620\u8bad\u7ec3\u9009\u62e9\uff0c\u800c\u975e\u6027\u80fd\u3002", "motivation": "\u7814\u7a76Transformer\u6743\u91cd\uff08\u5c24\u5176\u662funembedding\u77e9\u9635\uff09\u7684\u51e0\u4f55\u5c5e\u6027\u5728\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5c5e\u6027\u5bf9\u4e0b\u6e38\u6027\u80fd\u4f30\u8ba1\u7684\u6548\u7528\u3002", "method": "\u901a\u8fc7\u5728\u53d7\u63a7\u53d8\u5f02\u4e0b\u8bad\u7ec3\u4e00\u7cfb\u5217108\u4e2aOLMo\u98ce\u683c\u7684\u8bed\u6599\u6a21\u578b\uff0c\u7cfb\u7edf\u7814\u7a76\u6a21\u578b\u6027\u80fd\u4e0eunembedding\u77e9\u9635\u51e0\u4f55\u5c5e\u6027\uff08\u5c24\u5176\u662f\u5176\u6709\u6548\u79e9\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u6700\u4f73\u6027\u80fd\u6a21\u578b\u901a\u5e38\u5177\u6709\u9ad8\u6709\u6548\u79e9\uff0c\u4f46\u8fd9\u79cd\u8d8b\u52bf\u5e76\u975e\u5728\u6240\u6709\u4efb\u52a1\u548c\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u90fd\u5b58\u5728\u3002\u4f4e\u6709\u6548\u79e9\u4e0d\u4f1a\u5bfc\u81f4\u5c0f\u578b\u6a21\u578b\u540e\u671f\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u662f\u4e0e\u5176\u5171\u5b58\uff1b\u53d1\u73b0\u4f4e\u79e9\u6a21\u578b\u4e0d\u4f1a\u51fa\u73b0\u9971\u548c\u3002\u6709\u6548\u79e9\u53d7\u9884\u8bad\u7ec3\u8d85\u53c2\u6570\uff08\u5982\u6279\u5927\u5c0f\u548c\u6743\u91cd\u8870\u51cf\uff09\u7684\u5f3a\u70c8\u5f71\u54cd\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u53cd\u8fc7\u6765\u53c8\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u6269\u5c55\u5206\u6790\u5230\u5176\u4ed6\u51e0\u4f55\u6307\u6807\u548c\u6700\u7ec8\u5c42\u8868\u793a\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6307\u6807\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u4e00\u81f4\u7684\uff0c\u4f46\u6ca1\u6709\u4efb\u4f55\u4e00\u4e2a\u53ef\u4ee5\u53ef\u9760\u5730\u9884\u6d4b\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u7684\u51e0\u4f55\u5c5e\u6027\uff08\u7531\u73b0\u6709\u6307\u6807\u6355\u6349\uff09\u4e3b\u8981\u53cd\u6620\u8bad\u7ec3\u9009\u62e9\uff0c\u800c\u4e0d\u662f\u6027\u80fd\u3002"}}
{"id": "2602.20657", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20657", "abs": "https://arxiv.org/abs/2602.20657", "authors": ["Shahzad Ahmad", "Stefan Rass", "Zahra Seyedi"], "title": "Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing", "comment": "19 pages", "summary": "We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ea6\u79d1\u5229\u592b\u5bc6\u7801\u7cfb\u7edf\u7684\u540e\u91cf\u5b50\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\uff0c\u5177\u6709\u900f\u660e\u6027\u548c\u5f3a\u5927\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u5bf9\u73b0\u6709\u5bc6\u7801\u7cfb\u7edf\u6784\u6210\u7684\u5a01\u80c1\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ea6\u79d1\u5229\u592b\u5bc6\u7801\u7cfb\u7edf\u7684\u6df7\u6c8c\u54c8\u5e0c\u51fd\u6570\u7684\u65b0\u578b\u540e\u91cf\u5b50\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7b7e\u540d\u65b9\u6848\uff0c\u5229\u7528Goppa\u7801\u7684\u56fa\u6709\u9677\u95e8\uff0c\u901a\u8fc7Patterson\u89e3\u7801\u5b9e\u73b0\u63a7\u5236\u78b0\u649e\u67e5\u627e\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u5316\u6280\u672f\u5b9e\u73b0\u900f\u660e\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u900f\u660e\u3001\u57fa\u4e8e\u7f16\u7801\u7684\u540e\u91cf\u5b50\u53ef\u51c0\u5316\u7b7e\u540d\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u540e\u91cf\u5b50\u52a0\u5bc6\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.20403", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20403", "abs": "https://arxiv.org/abs/2602.20403", "authors": ["Guixian Chen", "Salar Fattahi", "Soroosh Shafiee"], "title": "Wasserstein Distributionally Robust Online Learning", "comment": null, "summary": "We study distributionally robust online learning, where a risk-averse learner updates decisions sequentially to guard against worst-case distributions drawn from a Wasserstein ambiguity set centered at past observations. While this paradigm is well understood in the offline setting through Wasserstein Distributionally Robust Optimization (DRO), its online extension poses significant challenges in both convergence and computation. In this paper, we address these challenges. First, we formulate the problem as an online saddle-point stochastic game between a decision maker and an adversary selecting worst-case distributions, and propose a general framework that converges to a robust Nash equilibrium coinciding with the solution of the corresponding offline Wasserstein DRO problem. Second, we address the main computational bottleneck, which is the repeated solution of worst-case expectation problems. For the important class of piecewise concave loss functions, we propose a tailored algorithm that exploits problem geometry to achieve substantial speedups over state-of-the-art solvers such as Gurobi. The key insight is a novel connection between the worst-case expectation problem, an inherently infinite-dimensional optimization problem, and a classical and tractable budget allocation problem, which is of independent interest.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u95ee\u9898\u51e0\u4f55\u5f62\u72b6\u6765\u52a0\u901f\u6700\u574f\u60c5\u51b5\u671f\u671b\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u9c81\u68d2\u5728\u7ebf\u5b66\u4e60\uff0c\u4ee5\u4fdd\u62a4\u5b66\u4e60\u8005\u514d\u53d7\u4ece\u4ee5\u8fc7\u53bb\u89c2\u6d4b\u4e3a\u4e2d\u5fc3\u7684\u6c34\u6676\u8ddd\u79bb\u6a21\u7cca\u96c6\u62bd\u53d6\u7684\u6700\u574f\u60c5\u51b5\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u51b3\u7b56\u8005\u4e0e\u9009\u62e9\u6700\u574f\u60c5\u51b5\u5206\u5e03\u7684\u5bf9\u624b\u4e4b\u95f4\u7684\u5728\u7ebf\u978d\u70b9\u968f\u673a\u535a\u5f08\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6536\u655b\u5230\u4e0e\u5bf9\u5e94\u79bb\u7ebfWasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08DRO\uff09\u95ee\u9898\u89e3\u76f8\u4e00\u81f4\u7684\u53cc\u91cd\u7eb3\u4ec0\u5747\u8861\u3002\u9488\u5bf9\u5206\u6bb5\u51f9\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u95ee\u9898\u51e0\u4f55\u5f62\u72b6\u6765\u52a0\u901f\u8d85\u8fc7Gurobi\u7b49\u6700\u5148\u8fdb\u6c42\u89e3\u5668\u7684\u5b9a\u5236\u7b97\u6cd5\u3002\u5173\u952e\u89c1\u89e3\u662f\u5173\u4e8e\u6700\u574f\u60c5\u51b5\u671f\u671b\u95ee\u9898\uff0c\u4e00\u4e2a\u672c\u8d28\u4e0a\u65e0\u9650\u7ef4\u4f18\u5316\u95ee\u9898\uff0c\u4e0e\u4e00\u4e2a\u7ecf\u5178\u4e14\u53ef\u5904\u7406\u7684\u9884\u7b97\u5206\u914d\u95ee\u9898\u4e4b\u95f4\u7684\u65b0\u9896\u8054\u7cfb\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u95ee\u9898\u51e0\u4f55\u5f62\u72b6\u6765\u52a0\u901f\u6700\u574f\u60c5\u51b5\u671f\u671b\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u5e03\u9c81\u68d2\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u6709\u671b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.20224", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20224", "abs": "https://arxiv.org/abs/2602.20224", "authors": ["Lana E. Yeganova", "Won G. Kim", "Shubo Tian", "Natalie Xie", "Donald C. Comeau", "W. John Wilbur", "Zhiyong Lu"], "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models", "comment": null, "summary": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20409", "abs": "https://arxiv.org/abs/2602.20409", "authors": ["Mainak Singha", "Sarthak Mehrotra", "Paolo Casari", "Subhasis Chaudhuri", "Elisa Ricci", "Biplab Banerjee"], "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation", "comment": "Accepted in CVPR 2026", "summary": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.", "code_url": "https://github.com/SarthakM320/CLIPoint3D", "code_stars": 0, "code_last_update": "2026-02-22", "AI": {"tldr": "CLIPoint3D is a new framework for 3D\u70b9\u4e91\u57df\u9002\u5e94\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds.", "method": "CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Projects 3D samples into multiple depth maps, exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme. Applies parameter-efficient fine-tuning to CLIP's encoders and designs an entropy-guided view sampling strategy. Uses optimal transport-based alignment loss and uncertainty-aware prototype alignment loss.", "result": "Achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines on PointDA-10 and GraspNetPC-10 benchmarks.", "conclusion": "CLIPoint3D is an effective framework for few-shot unsupervised 3D point cloud domain adaptation."}}
{"id": "2602.20457", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20457", "abs": "https://arxiv.org/abs/2602.20457", "authors": ["Zimeng Li", "Mudit Gaur", "Vaneet Aggarwal"], "title": "Oracle-Robust Online Alignment for Large Language Models", "comment": null, "summary": "We study online alignment of large language models under misspecified preference feedback, where the observed preference oracle deviates from an ideal but unknown ground-truth oracle. The online LLM alignment problem is a bi-level reinforcement problem due to the coupling between data collection and policy updates. Recently, the problem has been reduced to tractable single-level objective in the SAIL (Self-Improving Efficient Online Alignment) framework. In this paper, we introduce a pointwise oracle uncertainty set in this problem and formulate an oracle-robust online alignment objective as a worst-case optimization problem. For log-linear policies, we show that this robust objective admits an exact closed-form decomposition into the original loss function plus an explicit sensitivity penalty. We develop projected stochastic composite updates for the resulting weakly convex objective and prove $\\widetilde{O}(\\varepsilon^{-2})$ oracle complexity for reaching approximate stationarity.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20580", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20580", "abs": "https://arxiv.org/abs/2602.20580", "authors": ["Nishant Subramani", "Kshitish Ghate", "Mona Diab"], "title": "Personal Information Parroting in Language Models", "comment": "EACL Findings 2026", "summary": "Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u8bb0\u4f4f\u5927\u91cf\u4e2a\u4eba\u4fe1\u606f\uff0c\u5efa\u8bae\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u4e25\u683c\u8fc7\u6ee4\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u8bb0\u4f4f\u5927\u91cf\u4e2a\u4eba\u4fe1\u606f\uff0c\u589e\u52a0\u4e86\u9690\u79c1\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u548c\u89c4\u5219\uff08R&R\uff09\u68c0\u6d4b\u5668\u5957\u4ef6\u6765\u68c0\u6d4b\u7535\u5b50\u90ae\u4ef6\u5730\u5740\u3001\u7535\u8bdd\u53f7\u7801\u548cIP\u5730\u5740\u3002", "result": "\u5728\u624b\u52a8\u7f16\u7e82\u7684483\u4e2a\u4e2a\u4eba\u4fe1\u606f\u5b9e\u4f8b\u96c6\u4e2d\uff0c\u53d1\u73b013.6%\u88abPythia-6.9b\u6a21\u578b\u539f\u5c01\u4e0d\u52a8\u5730\u91cd\u590d\u3002\u6a21\u578b\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u65f6\u95f4\u4e0e\u8bb0\u5fc6\u80fd\u529b\u6b63\u76f8\u5173\u3002", "conclusion": "\u5f3a\u70c8\u5efa\u8bae\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u4e25\u683c\u8fc7\u6ee4\u548c\u533f\u540d\u5316\uff0c\u4ee5\u6700\u5c0f\u5316\u4e2a\u4eba\u4fe1\u606f\u91cd\u590d\u3002"}}
{"id": "2602.20360", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20360", "abs": "https://arxiv.org/abs/2602.20360", "authors": ["Runlong Liao", "Jian Yu", "Baiyu Su", "Chi Zhang", "Lizhang Chen", "Qiang Liu"], "title": "Momentum Guidance: Plug-and-Play Guidance for Flow Models", "comment": null, "summary": "Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.", "AI": {"tldr": "\u52a8\u91cf\u5f15\u5bfc\uff08MG\uff09\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\uff0c\u4f46\u9884\u8bad\u7ec3\u6a21\u578b\u5f88\u5c11\u4ee5\u6761\u4ef6\u5f62\u5f0f\u4f7f\u7528\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002", "method": "\u5f15\u5165\u52a8\u91cf\u5f15\u5bfc\uff08MG\uff09\uff0c\u5229\u7528\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u8f68\u8ff9\u672c\u8eab\u3002MG\u4f7f\u7528\u8fc7\u53bb\u901f\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6765\u5916\u63a8\u5f53\u524d\u901f\u5ea6\uff0c\u5e76\u4fdd\u6301\u6bcf\u6b65\u8bc4\u4f30\u4e00\u6b21\u7684\u6807\u51c6\u6210\u672c\u3002", "result": "MG\u5728ImageNet-256\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747FID\u6539\u8fdb36.68%\uff08\u65e0CFG\uff09\u548c25.52%\uff08\u6709CFG\uff09\uff0c\u572864\u4e2a\u91c7\u6837\u6b65\u9aa4\u65f6\u8fbe\u5230FID 1.597\u3002\u5728\u5927\u89c4\u6a21\u6a21\u578b\u5982Stable Diffusion 3\u548cFLUX.1-dev\u4e0a\u7684\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5728\u6807\u51c6\u6307\u6807\u4e0a\u7684\u4e00\u81f4\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "MG\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.20859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20859", "abs": "https://arxiv.org/abs/2602.20859", "authors": ["Zirui He", "Huopu Zhang", "Yanguang Liu", "Sirui Wu", "Mengnan Du"], "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction", "comment": "11 pages, 4 figures, 5 tables", "summary": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.", "AI": {"tldr": "FinAnchor\u901a\u8fc7\u951a\u5b9a\u5f02\u6784\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u91d1\u878d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u91d1\u878d\u9884\u6d4b\u4ece\u957f\u6587\u6863\u4e2d\u6d89\u53ca\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u53ef\u64cd\u4f5c\u4fe1\u53f7\u901a\u5e38\u7a00\u758f\u4e14\u88ab\u566a\u58f0\u6240\u63a9\u76d6\uff0c\u4e14\u6700\u4f18\u7684LLM\u7528\u4e8e\u751f\u6210\u5d4c\u5165\u968f\u4efb\u52a1\u548c\u65f6\u95f4\u800c\u53d8\u5316\u3002", "method": "\u63d0\u51faFinAnchor\uff08\u91d1\u878d\u951a\u5b9a\u8868\u793a\uff09\u6846\u67b6\uff0c\u96c6\u6210\u591a\u4e2aLLM\u7684\u5d4c\u5165\uff0c\u65e0\u9700\u5fae\u8c03\u5e95\u5c42\u6a21\u578b\u3002\u901a\u8fc7\u9009\u62e9\u951a\u5d4c\u5165\u7a7a\u95f4\u5e76\u5b66\u4e60\u7ebf\u6027\u6620\u5c04\u6765\u5bf9\u9f50\u5176\u4ed6\u6a21\u578b\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u91d1\u878dNLP\u4efb\u52a1\u4e2d\uff0cFinAnchor\u4f18\u4e8e\u5f3a\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u6807\u51c6\u96c6\u6210\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u951a\u5b9a\u5f02\u6784\u8868\u793a\u5bf9\u7a33\u5065\u91d1\u878d\u9884\u6d4b\u7684\u6709\u6548\u6027\u3002", "conclusion": "FinAnchor\u6846\u67b6\u901a\u8fc7\u951a\u5b9a\u5f02\u6784\u8868\u793a\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u91d1\u878d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.20543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20543", "abs": "https://arxiv.org/abs/2602.20543", "authors": ["Subhra Jyoti Mandal", "Lara Rachidi", "Puneet Jain", "Matthieu Duvinage", "Sander W. Timmer"], "title": "Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing", "comment": null, "summary": "Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20892", "abs": "https://arxiv.org/abs/2602.20892", "authors": ["Seyed Himan Ghaderi", "Saeed Sarbazi Azad", "Mohammad Mehdi Jaziriyan", "Ahmad Akbari"], "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter", "comment": "This is the original submitted (preprint) version of a paper published in Language Resources and Evaluation. The final published version is available at Springer via DOI: https://doi.org/10.1007/s10579-025-09886-5", "summary": "Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.", "AI": {"tldr": "\u63d0\u51faExa\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u60c5\u611f\u5206\u6790\u7cfb\u7edf\uff0c\u5b8fF\u5206\u6570\u4e3a79.87\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u6ce2\u65af\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e8612,000\u6761\u6ce2\u65af\u8bed\u63a8\u6587\uff0c\u75315\u540d\u6ce2\u65af\u8bed\u6bcd\u8bed\u8005\u6807\u6ce8\uff0c\u5206\u4e3a\u79ef\u6781\u3001\u4e2d\u7acb\u548c\u6d88\u6781\u4e09\u7c7b\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Pars Bert\u548cRoberta\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u73b0\u4e8679.87\u7684\u5b8fF\u5206\u6570\uff0c\u8868\u660e\u6a21\u578b\u548c\u6570\u636e\u5bf9\u4e8e\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u5177\u6709\u8db3\u591f\u7684\u4ef7\u503c\u3002", "conclusion": "\u63d0\u51fa\u4e86Exa\u60c5\u611f\u5206\u6790\u6ce2\u65af\u8bed\u6570\u636e\u96c6\uff0c\u5bf9\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.20396", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.20396", "abs": "https://arxiv.org/abs/2602.20396", "authors": ["J\u00f6rg Martin", "Stefan Haufe"], "title": "cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context", "comment": null, "summary": "Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.", "AI": {"tldr": "\u63d0\u51facc-Shapley\u65b9\u6cd5\uff0c\u89e3\u51b3\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u4e2d\u7684\u8bef\u5bfc\u6027\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u5b58\u5728\u8bef\u5bfc\u6027\u5173\u8054\u95ee\u9898\u3002", "method": "\u63d0\u51facc-Shapley\u65b9\u6cd5\uff0c\u7ed3\u5408\u56e0\u679c\u7ed3\u6784\u8fdb\u884c\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u3002", "result": "cc-Shapley\u65b9\u6cd5\u6709\u6548\u6d88\u9664\u89c2\u5bdf\u6027\u5206\u6790\u4e2d\u7684\u8bef\u5bfc\u6027\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51facc-Shapley\uff0c\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u6d88\u9664\u7531\u89c2\u5bdf\u6027\u5206\u6790\u5e26\u6765\u7684\u8bef\u5bfc\u6027\u5173\u8054\u3002"}}
{"id": "2602.20548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20548", "abs": "https://arxiv.org/abs/2602.20548", "authors": ["Shuai Wang", "Malu Zhang", "Yulin Jiang", "Dehao Zhang", "Ammar Belatreche", "Yu Liang", "Yimeng Shan", "Zijian Zhou", "Yang Yang", "Haizhou Li"], "title": "Robust Spiking Neural Networks Against Adversarial Attacks", "comment": "Published as a conference paper at ICLR 2026", "summary": "Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.", "AI": {"tldr": "\u63d0\u51faTGO\u65b9\u6cd5\u63d0\u9ad8SNNs\u9c81\u68d2\u6027", "motivation": "Spiking Neural Networks (SNNs) \u5728\u80fd\u91cf\u6548\u7387\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdThreshold Guarding Optimization (TGO) \u65b9\u6cd5\uff0c\u5305\u62ec\u589e\u52a0\u635f\u5931\u51fd\u6570\u7ea6\u675f\u548c\u5f15\u5165\u566a\u58f0\u8109\u51b2\u795e\u7ecf\u5143\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76f4\u63a5\u8bad\u7ec3\u7684SNNs\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TGO\u65b9\u6cd5\u4e3a\u63d0\u9ad8SNNs\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.20810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20810", "abs": "https://arxiv.org/abs/2602.20810", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "POMDPPlanners: Open-Source Package for POMDP Planning", "comment": null, "summary": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.", "AI": {"tldr": "POMDPPlanners\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30POMDP\u89c4\u5212\u7b97\u6cd5\u7684Python\u5305\uff0c\u65e8\u5728\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u51b3\u7b56\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u7814\u7a76\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u590d\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5de5\u5177\u5305\u4e0d\u8db3\u7684\u98ce\u9669\u654f\u611f\u8bbe\u7f6e\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aPOMDPPlanners\u7684Python\u5305\uff0c\u7528\u4e8e\u8bc4\u4f30POMDP\u89c4\u5212\u7b97\u6cd5\u3002\u8be5\u5305\u96c6\u6210\u4e86\u6700\u5148\u8fdb\u7684\u89c4\u5212\u7b97\u6cd5\u3001\u4e00\u7cfb\u5217\u57fa\u51c6\u73af\u5883\u4ee5\u53ca\u5b89\u5168\u5173\u952e\u53d8\u4f53\u3002\u5b83\u8fd8\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3001\u6301\u4e45\u7f13\u5b58\u548c\u5e76\u884c\u6a21\u62df\u914d\u7f6e\u3002", "result": "POMDPPlanners\u53ef\u4ee5\u51cf\u5c11\u5927\u91cf\u6a21\u62df\u7814\u7a76\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u4fc3\u8fdb\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u9886\u57df\u7684\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u590d\u7684\u7814\u7a76\u3002", "conclusion": "POMDPPlanners\u4e3a\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u9886\u57df\u7684\u98ce\u9669\u654f\u611f\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7814\u7a76\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2602.20945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20945", "abs": "https://arxiv.org/abs/2602.20945", "authors": ["Taiqiang Wu", "Zenan Zu", "Bo Zhou", "Ngai Wong"], "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization", "comment": "Tech Report, Insights on Efficient Reasoning via Reward Shaping", "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u63a8\u7406\u7684\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u5173\u952e\u53d1\u73b0\uff0c\u4e3a\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9ad8\u6548\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u9ad8\u6548\u63a8\u7406\u7684\u673a\u5236\uff0c\u5305\u62ec\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u3001\u5956\u52b1\u5851\u9020\u548c\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002\u6211\u4eec\u8fd8\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u53d1\u73b0\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u534f\u8bae\uff0c\u5bf9\u8bad\u7ec3\u63d0\u793a\u3001\u8f93\u51fa\u3001\u5956\u52b1\u5851\u9020\u548c\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u4e9b\u5173\u952e\u7684\u53d1\u73b0\uff0c\u5982\u957f\u5ea6\u9002\u5e94\u548c\u63a8\u7406\u7ec6\u5316\uff0c\u4ee5\u53ca\u957f\u5ea6\u504f\u5dee\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u53d1\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u9075\u5faa\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u957f\u5ea6\u9002\u5e94\u548c\u63a8\u7406\u7ec6\u5316\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u5728\u76f8\u5bf9\u8f83\u5bb9\u6613\u7684\u63d0\u793a\u4e0a\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u786e\u4fdd\u6b63\u5956\u52b1\u4fe1\u53f7\u7684\u5bc6\u5ea6\uff0c\u4ece\u800c\u907f\u514d\u957f\u5ea6\u5d29\u6e83\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u5230\u7684\u957f\u5ea6\u504f\u5dee\u53ef\u4ee5\u8de8\u9886\u57df\u6cdb\u5316\u3002"}}
{"id": "2602.20399", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20399", "abs": "https://arxiv.org/abs/2602.20399", "authors": ["Haixu Wu", "Minghao Guo", "Zongyi Li", "Zhiyang Dou", "Mingsheng Long", "Kaiming He", "Wojciech Matusik"], "title": "GeoPT: Scaling Physics Simulation via Lifted Geometric Pre-Training", "comment": null, "summary": "Neural simulators promise efficient surrogates for physics simulation, but scaling them is bottlenecked by the prohibitive cost of generating high-fidelity training data. Pre-training on abundant off-the-shelf geometries offers a natural alternative, yet faces a fundamental gap: supervision on static geometry alone ignores dynamics and can lead to negative transfer on physics tasks. We present GeoPT, a unified pre-trained model for general physics simulation based on lifted geometric pre-training. The core idea is to augment geometry with synthetic dynamics, enabling dynamics-aware self-supervision without physics labels. Pre-trained on over one million samples, GeoPT consistently improves industrial-fidelity benchmarks spanning fluid mechanics for cars, aircraft, and ships, and solid mechanics in crash simulation, reducing labeled data requirements by 20-60% and accelerating convergence by 2$\\times$. These results show that lifting with synthetic dynamics bridges the geometry-physics gap, unlocking a scalable path for neural simulation and potentially beyond. Code is available at https://github.com/Physics-Scaling/GeoPT.", "code_url": "https://github.com/Physics-Scaling/GeoPT", "code_stars": 0, "code_last_update": "2026-02-25", "AI": {"tldr": "GeoPT\u901a\u8fc7\u5408\u6210\u52a8\u529b\u5b66\u63d0\u5347\u51e0\u4f55\u5f62\u72b6\uff0c\u63d0\u9ad8\u7269\u7406\u6a21\u62df\u6548\u7387\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "motivation": "Neural simulators\u5728\u7269\u7406\u6a21\u62df\u4e2d\u5177\u6709\u9ad8\u6548\u6027\uff0c\u4f46\u6269\u5c55\u6027\u53d7\u9650\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u8bad\u7ec3\u6570\u636e\u7684\u9ad8\u6602\u6210\u672c\u3002", "method": "\u63d0\u51faGeoPT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u63d0\u5347\u51e0\u4f55\u9884\u8bad\u7ec3\u7684\u901a\u7528\u7269\u7406\u6a21\u62df\u7edf\u4e00\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5408\u6210\u52a8\u529b\u5b66\u589e\u5f3a\u51e0\u4f55\u5f62\u72b6\uff0c\u5b9e\u73b0\u65e0\u9700\u7269\u7406\u6807\u7b7e\u7684\u52a8\u6001\u611f\u77e5\u81ea\u76d1\u7763\u3002", "result": "GeoPT\u5728\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u6837\u672c\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u6d41\u4f53\u529b\u5b66\u3001\u56fa\u4f53\u529b\u5b66\u7b49\u9886\u57df\u63d0\u9ad8\u5de5\u4e1a\u4fdd\u771f\u5ea6\u57fa\u51c6\uff0c\u51cf\u5c1120-60%\u7684\u6807\u8bb0\u6570\u636e\u9700\u6c42\uff0c\u52a0\u901f\u6536\u655b2\u500d\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u52a8\u529b\u5b66\u63d0\u5347\u51e0\u4f55\u5f62\u72b6\uff0c\u5f25\u5408\u51e0\u4f55-\u7269\u7406\u5dee\u8ddd\uff0c\u4e3a\u795e\u7ecf\u6a21\u62df\u5f00\u8f9f\u53ef\u6269\u5c55\u7684\u9053\u8def\u3002"}}
{"id": "2602.20550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20550", "abs": "https://arxiv.org/abs/2602.20550", "authors": ["Chengshuai Yang"], "title": "The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation", "comment": null, "summary": "Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u50cf\u6b63\u5411\u6a21\u578b\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u666e\u9002\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u6210\u50cf\u6b63\u5411\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u50cf\u6b63\u5411\u6a21\u578b\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c7b\u578b\u5316\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u6210\u50cf\u6b63\u5411\u6a21\u578b\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u666e\u9002\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u6709\u6210\u50cf\u6b63\u5411\u6a21\u578b\u90fd\u53ef\u4ee5\u752811\u4e2a\u57fa\u672c\u64cd\u4f5c\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u4e86\u7b97\u6cd5\u5b9e\u73b0\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aPWM\u6846\u67b6\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u4e3a\u6210\u50cf\u6b63\u5411\u6a21\u578b\u7684\u8868\u793a\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.20812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20812", "abs": "https://arxiv.org/abs/2602.20812", "authors": ["Jia-Rui Lin", "Yun-Hong Cai", "Xiang-Rui Ni", "Shaojie Zhou", "Peng Pan"], "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset", "comment": null, "summary": "As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eBIM\u7684\u9886\u57df\u7279\u5b9aLLM\uff0c\u63d0\u9ad8\u4e86LLM\u5728BIM\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "BIM\u57fa\u4e8e\u8bbe\u8ba1\u7684\u53d1\u5c55\u9700\u8981LLM\u7684\u63a8\u52a8\uff0c\u4f46\u7f3a\u4e4f\u7279\u5b9a\u6570\u636e\u96c6\u548cLLM\u8bc4\u4f30\u57fa\u51c6\u9650\u5236\u4e86LLM\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdBIM\u57fa\u4e8e\u8bbe\u8ba1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u62ec\u76f8\u5e94\u7684\u5b9a\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u6027\u80fd\uff1b\u5f00\u53d1\u4e86\u4e00\u79cd\u4eceBIM\u751f\u6210\u6587\u672c\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u76f8\u5e94\u7684BIM\u884d\u751f\u6570\u636e\u96c6\uff0c\u7528\u4e8eLLM\u8bc4\u4f30\u548c\u5fae\u8c03\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u9002\u5e94BIM\u57fa\u4e8e\u8bbe\u8ba1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u63d0\u9ad8\u4e86LLM\u5728BIM\u57fa\u4e8e\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0cQwen-BIM\u5728G-Eval\u8bc4\u5206\u4e0a\u6bd4\u57fa\u7ebfLLM\u6a21\u578b\u5e73\u5747\u63d0\u9ad8\u4e8621.0%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5168\u9762\u7684\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8eBIM\u7684\u9886\u57df\u7279\u5b9aLLM\uff0c\u4e3a\u5404\u79cd\u9886\u57df\u7684BIM\u76f8\u5173LLM\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2602.20966", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20966", "abs": "https://arxiv.org/abs/2602.20966", "authors": ["Paola Merlo", "Chunyang Jiang", "Giuseppe Samo", "Vivi Nastase"], "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models", "comment": "Under review, 46 pages, 5 tables, 28 figures", "summary": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?\n  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.\n  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ed1\u9e1f\u8bed\u8a00\u77e9\u9635\u4efb\u52a1\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5206\u6790\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u68c0\u6d4b\u8bed\u8a00\u5bf9\u8c61\u53ca\u5176\u5c5e\u6027\u3001\u68c0\u6d4b\u548c\u4f7f\u7528\u53e5\u5b50\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u6a21\u5f0f\u3001\u4ee5\u53ca\u8bed\u8a00\u6216\u63a8\u7406\u9519\u8bef\u503e\u5411\u53ca\u5176\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u9ed1\u9e1f\u8bed\u8a00\u77e9\u9635\uff08BLM\uff09\u4efb\u52a1\uff0c\u5305\u62ec\u6570\u636e\u96c6\u7684\u6784\u5efa\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5bf9\u5206\u5757\u548c\u7cfb\u7edf\u6027\u8fdb\u884c\u7684\u9488\u5bf9\u6027\u5b9e\u9a8c\u3002", "result": "BLMs \u5728\u591a\u4e2a\u8bed\u8a00\u4e2d\u53ef\u4ee5\u4ee5\u826f\u597d\u7684\u6027\u80fd\u89e3\u51b3\uff0c\u7b80\u5355\u57fa\u7ebf\u6a21\u578b\u6216\u66f4\u5b9a\u5236\u7684\u6a21\u578b\u5747\u9002\u7528\u3002\u8fd9\u4e9b\u8868\u793a\u5305\u542b\u4e0e\u89e3\u51b3\u8bed\u8a00\u4efb\u52a1\u76f8\u5173\u7684\u8bed\u6cd5\u5bf9\u8c61\u548c\u5c5e\u6027\uff0c\u901a\u8fc7\u68c0\u6d4b\u53e5\u5b50\u95f4\u7684\u7cfb\u7edf\u6027\u6a21\u5f0f\u5f97\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "BLMs \u6709\u52a9\u4e8e\u652f\u6301\u5bf9\u8bed\u8a00\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7279\u6027\u7684\u591a\u65b9\u9762\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u3002"}}
{"id": "2602.20400", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20400", "abs": "https://arxiv.org/abs/2602.20400", "authors": ["Callum Canavan", "Aditya Shrivastava", "Allison Qi", "Jonathan Michala", "Fabien Roger"], "title": "Three Concrete Challenges and Two Hopes for the Safety of Unsupervised Elicitation", "comment": "19 pages, 9 figures", "summary": "To steer language models towards truthful outputs on tasks which are beyond human capability, previous work has suggested training models on easy tasks to steer them on harder ones (easy-to-hard generalization), or using unsupervised training algorithms to steer models with no external labels at all (unsupervised elicitation). Although techniques from both paradigms have been shown to improve model accuracy on a wide variety of tasks, we argue that the datasets used for these evaluations could cause overoptimistic evaluation results. Unlike many real-world datasets, they often (1) have no features with more salience than truthfulness, (2) have balanced training sets, and (3) contain only data points to which the model can give a well-defined answer. We construct datasets that lack each of these properties to stress-test a range of standard unsupervised elicitation and easy-to-hard generalization techniques. We find that no technique reliably performs well on any of these challenges. We also study ensembling and combining easy-to-hard and unsupervised techniques, and find they only partially mitigate performance degradation due to these challenges. We believe that overcoming these challenges should be a priority for future work on unsupervised elicitation.", "AI": {"tldr": "\u6784\u5efa\u6570\u636e\u96c6\u6d4b\u8bd5\u65e0\u76d1\u7763\u8bf1\u5bfc\u6280\u672f\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6280\u672f\u65e0\u6cd5\u53ef\u9760\u5730\u5e94\u5bf9\u6311\u6218\uff1b\u96c6\u6210\u548c\u7ed3\u5408\u6280\u672f\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u4e3a\u4e86\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u80fd\u529b\u4e4b\u5916\u7684\u4efb\u52a1\u4e0a\u4ea7\u751f\u771f\u5b9e\u7684\u8f93\u51fa\uff0c\u5148\u524d\u7684\u7814\u7a76\u5efa\u8bae\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8bad\u7ec3\u6a21\u578b\u4ee5\u5f15\u5bfc\u5b83\u4eec\u5728\u66f4\u96be\u7684\u4efb\u52a1\u4e0a\uff08\u7b80\u5355\u5230\u56f0\u96be\u6cdb\u5316\uff09\uff0c\u6216\u8005\u4f7f\u7528\u65e0\u76d1\u7763\u8bad\u7ec3\u7b97\u6cd5\u6765\u5f15\u5bfc\u6ca1\u6709\u5916\u90e8\u6807\u7b7e\u7684\u6a21\u578b\uff08\u65e0\u76d1\u7763\u8bf1\u5bfc\uff09\u3002\u5c3d\u7ba1\u8fd9\u4e24\u79cd\u8303\u5f0f\u4e2d\u7684\u6280\u672f\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u5e7f\u6cdb\u63d0\u9ad8\u5404\u79cd\u4efb\u52a1\u7684\u6a21\u578b\u7cbe\u5ea6\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u7528\u4e8e\u8fd9\u4e9b\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u53ef\u80fd\u5bfc\u81f4\u8fc7\u4e8e\u4e50\u89c2\u7684\u8bc4\u4f30\u7ed3\u679c\u3002\u4e0e\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u5b83\u4eec\u901a\u5e38\uff081\uff09\u6ca1\u6709\u6bd4\u771f\u5b9e\u6027\u66f4\u663e\u8457\u7684\u7279\u5f81\uff0c\uff082\uff09\u5177\u6709\u5e73\u8861\u7684\u8bad\u7ec3\u96c6\uff0c\u5e76\u4e14\uff083\uff09\u53ea\u5305\u542b\u6a21\u578b\u53ef\u4ee5\u7ed9\u51fa\u660e\u786e\u5b9a\u7b54\u7684\u6570\u636e\u70b9\u3002\u6211\u4eec\u6784\u5efa\u4e86\u7f3a\u4e4f\u8fd9\u4e9b\u7279\u6027\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u5bf9\u4e00\u7cfb\u5217\u6807\u51c6\u7684\u65e0\u76d1\u7763\u8bf1\u5bfc\u548c\u7b80\u5355\u5230\u56f0\u96be\u6cdb\u5316\u6280\u672f\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002\u6211\u4eec\u53d1\u73b0\u6ca1\u6709\u4efb\u4f55\u6280\u672f\u5728\u4efb\u4f55\u8fd9\u4e9b\u6311\u6218\u4e0a\u90fd\u80fd\u53ef\u9760\u5730\u8868\u73b0\u826f\u597d\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u96c6\u6210\u548c\u7ed3\u5408\u7b80\u5355\u5230\u56f0\u96be\u548c\u65e0\u76d1\u7763\u6280\u672f\uff0c\u53d1\u73b0\u5b83\u4eec\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u5e94\u8be5\u662f\u672a\u6765\u65e0\u76d1\u7763\u8bf1\u5bfc\u5de5\u4f5c\u7684\u91cd\u70b9\u3002", "method": "\u6784\u5efa\u7f3a\u4e4f\u7279\u5b9a\u7279\u6027\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u5bf9\u4e00\u7cfb\u5217\u6807\u51c6\u7684\u65e0\u76d1\u7763\u8bf1\u5bfc\u548c\u7b80\u5355\u5230\u56f0\u96be\u6cdb\u5316\u6280\u672f\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002\u7814\u7a76\u4e86\u96c6\u6210\u548c\u7ed3\u5408\u7b80\u5355\u5230\u56f0\u96be\u548c\u65e0\u76d1\u7763\u6280\u672f\u3002", "result": "\u6ca1\u6709\u6280\u672f\u5728\u4efb\u4f55\u6311\u6218\u4e0a\u90fd\u80fd\u53ef\u9760\u5730\u8868\u73b0\u826f\u597d\uff1b\u96c6\u6210\u548c\u7ed3\u5408\u6280\u672f\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u5e94\u8be5\u662f\u672a\u6765\u65e0\u76d1\u7763\u8bf1\u5bfc\u5de5\u4f5c\u7684\u91cd\u70b9\u3002"}}
{"id": "2602.20813", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20813", "abs": "https://arxiv.org/abs/2602.20813", "authors": ["Nora Petrova", "John Burden"], "title": "Pressure Reveals Character: Behavioural Alignment Evaluation at Depth", "comment": "Preprint", "summary": "Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.", "AI": {"tldr": "\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u548c\u6392\u884c\u699c\u6765\u652f\u6301\u6301\u7eed\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u9700\u8981\u6d4b\u8bd5\u5b83\u4eec\u5728\u73b0\u5b9e\u538b\u529b\u4e0b\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u58f0\u79f0\u4f1a\u505a\u4ec0\u4e48\u3002\u968f\u7740\u5bf9\u9f50\u5931\u8d25\u8d8a\u6765\u8d8a\u5bfc\u81f4\u73b0\u5b9e\u4e16\u754c\u7684\u5371\u5bb3\uff0c\u4ecd\u7136\u7f3a\u4e4f\u5177\u6709\u73b0\u5b9e\u591a\u8f6e\u573a\u666f\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5bf9\u9f50\u57fa\u51c6\uff0c\u5305\u542b904\u4e2a\u573a\u666f\uff0c\u6db5\u76d6\u516d\u4e2a\u7c7b\u522b\uff1a\u8bda\u5b9e\u3001\u5b89\u5168\u3001\u975e\u64cd\u7eb5\u3001\u9c81\u68d2\u6027\u3001\u53ef\u7ea0\u6b63\u6027\u548c\u7b56\u5212\u3002\u8fd9\u4e9b\u573a\u666f\u88ab\u4eba\u7c7b\u8bc4\u5ba1\u9a8c\u8bc1\u4e3a\u73b0\u5b9e\u3002\u6211\u4eec\u8bc4\u4f30\u4e8624\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u4f7f\u7528LLM\u8bc4\u5ba1\u5458\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8fdb\u884c\u9a8c\u8bc1\uff0c\u53d1\u73b0\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\u4e5f\u6709\u5dee\u8ddd\uff0c\u800c\u5927\u591a\u6570\u6a21\u578b\u5728\u5404\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5f31\u70b9\u3002\u56e0\u7d20\u5206\u6790\u663e\u793a\uff0c\u5bf9\u9f50\u884c\u4e3a\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7ed3\u6784\uff08\u7c7b\u4f3c\u4e8e\u8ba4\u77e5\u7814\u7a76\u4e2d\u7684g\u56e0\u7d20\uff09\uff0c\u5728\u67d0\u4e2a\u7c7b\u522b\u5f97\u5206\u9ad8\u7684\u6a21\u578b\u5f80\u5f80\u5728\u5176\u4ed6\u7c7b\u522b\u4e2d\u4e5f\u5f97\u5206\u9ad8\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\u4e5f\u6709\u5dee\u8ddd\uff0c\u800c\u5927\u591a\u6570\u6a21\u578b\u5728\u5404\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5f31\u70b9\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\u5bf9\u9f50\u884c\u4e3a\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7ed3\u6784\uff0c\u5728\u67d0\u4e2a\u7c7b\u522b\u5f97\u5206\u9ad8\u7684\u6a21\u578b\u5f80\u5f80\u5728\u5176\u4ed6\u7c7b\u522b\u4e2d\u4e5f\u5f97\u5206\u9ad8\u3002", "conclusion": "\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u5168\u9762\u3001\u5177\u6709\u73b0\u5b9e\u591a\u8f6e\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u3002\u6211\u4eec\u7684\u57fa\u51c6\u548c\u4ea4\u4e92\u5f0f\u6392\u884c\u699c\u65e8\u5728\u652f\u6301\u6301\u7eed\u8bc4\u4f30\uff0c\u5e76\u8ba1\u5212\u5728\u672a\u6765\u6269\u5c55\u573a\u666f\u548c\u6dfb\u52a0\u65b0\u6a21\u578b\u3002"}}
{"id": "2602.20556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20556", "abs": "https://arxiv.org/abs/2602.20556", "authors": ["Hanhui Li", "Xuan Huang", "Wanquan Liu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang", "Chenqiang Gao"], "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos", "comment": null, "summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.", "code_url": "https://github.com/XuanHuang0/WildGHand", "code_stars": 0, "code_last_update": "2026-02-02", "AI": {"tldr": "WildGHand\u5728\u771f\u5b9e\u573a\u666f\u624b\u90e8\u91cd\u5efa\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9000\u5316\u95ee\u9898\u3002", "method": "\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\u548c\u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5982PSNR\u63d0\u9ad8\u4e8615.8%\uff0cLPIPS\u964d\u4f4e\u4e8623.1%\u3002", "conclusion": "WildGHand\u5728\u771f\u5b9e\u573a\u666f\u7684\u624b\u90e8\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2602.20878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20878", "abs": "https://arxiv.org/abs/2602.20878", "authors": ["Dhita Putri Pratama", "Soyeon Caren Han", "Yihao Ding"], "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs", "comment": null, "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165VLCGs\u548cViLCaR\u57fa\u51c6\uff0c\u53d1\u73b0LVLM\u7684\u56e0\u679c\u63a8\u7406\u5c40\u9650\u6027\u6e90\u4e8e\u7ed3\u6784\u6307\u5bfc\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u865a\u5047\u7684\u76f8\u5173\u6027\u800c\u4e0d\u662f\u771f\u6b63\u7684\u56e0\u679c\u63a8\u7406\u3002", "method": "\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u56e0\u679c\u56fe\uff08VLCGs\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u67e5\u8be2\u6761\u4ef6\u7684\u8868\u793a\uff0c\u660e\u786e\u7f16\u7801\u56e0\u679c\u76f8\u5173\u7684\u5bf9\u8c61\u3001\u5c5e\u6027\u3001\u5173\u7cfb\u548c\u573a\u666f\u57fa\u7840\u5047\u8bbe\u3002\u57fa\u4e8e\u8fd9\u79cd\u8868\u793a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ViLCaR\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u56e0\u679c\u5f52\u56e0\u3001\u56e0\u679c\u63a8\u7406\u548c\u95ee\u7b54\u4efb\u52a1\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u4ee5\u53ca\u4e0e\u56fe\u5bf9\u9f50\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u76f8\u5173\u6027\u8bc6\u522b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u6700\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6ce8\u5165\u7ed3\u6784\u5316\u76f8\u5173\u6027\u4fe1\u606f\u4e0e\u96f6\u6837\u672c\u548c\u6807\u51c6\u60c5\u5883\u5b66\u4e60\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f52\u56e0\u548c\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u5f53\u524dLVLM\u56e0\u679c\u63a8\u7406\u7684\u5c40\u9650\u6027\u4e3b\u8981\u6e90\u4e8e\u7ed3\u6784\u6307\u5bfc\u4e0d\u8db3\uff0c\u800c\u4e0d\u662f\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2602.20976", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20976", "abs": "https://arxiv.org/abs/2602.20976", "authors": ["Xuan Luo", "Yubin Chen", "Zhiyu Hou", "Linpu Yu", "Geng Tu", "Jing Li", "Ruifeng Xu"], "title": "Evaluating Proactive Risk Awareness of Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLMs\u4e3b\u52a8\u98ce\u9669\u610f\u8bc6\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u751f\u6001\u8d23\u4efb\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u4e3b\u52a8\u4fdd\u969c\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65e5\u5e38\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u8d23\u4efb\u4e0d\u4ec5\u9650\u4e8e\u5bf9\u6709\u5bb3\u610f\u56fe\u7684\u54cd\u5e94\uff0c\u8fd8\u5e94\u6269\u5c55\u5230\u9884\u6d4b\u53ef\u80fd\u7684\u4e0d\u5f53\u4f46\u5177\u6709\u540e\u679c\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u98ce\u9669\u610f\u8bc6\u8bc4\u4f30\u6846\u67b6\uff0c\u6784\u5efa\u4e86Butterfly\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u54cd\u5e94\u957f\u5ea6\u3001\u8bed\u8a00\u548c\u6a21\u6001\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u957f\u5ea6\u9650\u5236\u7684\u54cd\u5e94\u3001\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u548c\uff08\u591a\u6a21\u6001\uff09\u7269\u79cd\u4fdd\u62a4\u4e2d\u7684\u6301\u7eed\u76f2\u70b9\u4e0b\uff0c\u4e3b\u52a8\u610f\u8bc6\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5f53\u524d\u5b89\u5168\u5bf9\u9f50\u4e0e\u5b9e\u9645\u751f\u6001\u8d23\u4efb\u8981\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728LLMs\u90e8\u7f72\u4e2d\u5b9e\u65bd\u4e3b\u52a8\u4fdd\u969c\u63aa\u65bd\u7684\u9700\u6c42\u3002"}}
{"id": "2602.20404", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20404", "abs": "https://arxiv.org/abs/2602.20404", "authors": ["Xihe Gu", "Urbashi Mitra", "Tara Javidi"], "title": "$\u03ba$-Explorer: A Unified Framework for Active Model Estimation in MDPs", "comment": null, "summary": "In tabular Markov decision processes (MDPs) with perfect state observability, each trajectory provides active samples from the transition distributions conditioned on state-action pairs. Consequently, accurate model estimation depends on how the exploration policy allocates visitation frequencies in accordance with the intrinsic complexity of each transition distribution. Building on recent work on coverage-based exploration, we introduce a parameterized family of decomposable and concave objective functions $U_\u03ba$ that explicitly incorporate both intrinsic estimation complexity and extrinsic visitation frequency. Moreover, the curvature $\u03ba$ provides a unified treatment of various global objectives, such as the average-case and worst-case estimation error objectives. Using the closed-form characterization of the gradient of $U_\u03ba$, we propose $\u03ba$-Explorer, an active exploration algorithm that performs Frank-Wolfe-style optimization over state-action occupancy measures. The diminishing-returns structure of $U_\u03ba$ naturally prioritizes underexplored and high-variance transitions, while preserving smoothness properties that enable efficient optimization. We establish tight regret guarantees for $\u03ba$-Explorer and further introduce a fully online and computationally efficient surrogate algorithm for practical use. Experiments on benchmark MDPs demonstrate that $\u03ba$-Explorer provides superior performance compared to existing exploration strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a2\u7d22\u7b97\u6cd5$\u03ba$-Explorer\uff0c\u901a\u8fc7\u4f18\u5316\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u5ea6\u91cf\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u5b8c\u7f8e\u72b6\u6001\u53ef\u89c2\u6d4b\u7684\u8868\u683c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e2d\u7684\u6a21\u578b\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u7814\u7a76\u4e86\u63a2\u7d22\u7b56\u7565\u5982\u4f55\u6839\u636e\u6bcf\u4e2a\u8f6c\u79fb\u5206\u5e03\u7684\u5185\u5728\u590d\u6742\u6027\u5206\u914d\u8bbf\u95ee\u9891\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u53ef\u5206\u89e3\u548c\u51f9\u76ee\u6807\u51fd\u6570\u65cf$U_\u03ba$\uff0c\u8be5\u51fd\u6570\u65cf\u660e\u786e\u5730\u7ed3\u5408\u4e86\u5185\u5728\u4f30\u8ba1\u590d\u6742\u6027\u548c\u5916\u90e8\u8bbf\u95ee\u9891\u7387\u3002\u4f7f\u7528\u95ed\u5f0f\u68af\u5ea6\u63cf\u8ff0\u7b26\uff0c\u63d0\u51fa\u4e86$\u03ba$-Explorer\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u5ea6\u91cf\u4e0a\u8fdb\u884cFrank-Wolfe\u98ce\u683c\u7684\u4f18\u5316\u3002", "result": "$\u03ba$-Explorer\u7b97\u6cd5\u5728\u57fa\u51c6MDP\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u63a2\u7d22\u7b56\u7565\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a2\u7d22\u7b97\u6cd5$\u03ba$-Explorer\uff0c\u901a\u8fc7\u4f18\u5316\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u5ea6\u91cf\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.20569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20569", "abs": "https://arxiv.org/abs/2602.20569", "authors": ["Jiaqi Wu", "Yuchen Zhou", "Muduo Xu", "Zisheng Liang", "Simiao Ren", "Jiayu Xue", "Meige Yang", "Siying Chen", "Jingheng Huan"], "title": "AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents", "comment": "17 pages, 10 figures", "summary": "We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.", "AI": {"tldr": "AIForge-Doc \u662f\u9996\u4e2a\u9488\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u91d1\u878d\u548c\u8868\u683c\u6587\u6863\u4fee\u590d\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u5668\u5bf9 AI \u4f2a\u9020\u7684\u6587\u6863\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u6587\u4ef6\u4f2a\u9020\u6570\u636e\u96c6\u4f9d\u8d56\u4e8e\u4f20\u7edf\u6570\u5b57\u7f16\u8f91\u5de5\u5177\uff0c\u65e0\u6cd5\u68c0\u6d4b AI \u4f2a\u9020\u7684\u6587\u4ef6\u6b3a\u8bc8\u5a01\u80c1\u3002", "method": "\u4f7f\u7528 Gemini 2.5 Flash Image \u548c Ideogram v2 Edit \u4e24\u4e2a AI \u4fee\u590d API \u7cfb\u7edf\u6027\u5730\u4f2a\u9020\u5b9e\u9645\u6536\u636e\u548c\u8868\u5355\u56fe\u50cf\u4e2d\u7684\u6570\u5b57\u5b57\u6bb5\uff0c\u5e76\u4f7f\u7528\u50cf\u7d20\u7cbe\u786e\u7684\u7be1\u6539\u533a\u57df\u63a9\u7801\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u4e09\u4e2a\u4ee3\u8868\u6027\u68c0\u6d4b\u5668\uff08TruFor\u3001DocTamper \u548c GPT-4o\uff09\u7684\u68c0\u6d4b\u7ed3\u679c\u5747\u5927\u5e45\u4e0b\u964d\uff0c\u8bc1\u5b9e\u4e86 AI \u4f2a\u9020\u503c\u5bf9\u81ea\u52a8\u68c0\u6d4b\u5668\u548c VLMs \u6765\u8bf4\u662f\u65e0\u6cd5\u533a\u5206\u7684\u3002", "conclusion": "AIForge-Doc \u4ee3\u8868\u4e86\u5bf9\u6587\u4ef6\u6cd5\u533b\u5b66\u7684\u4e00\u4e2a\u5168\u65b0\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2602.20575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20575", "abs": "https://arxiv.org/abs/2602.20575", "authors": ["Haojie Feng", "Peizhi Zhang", "Mengjie Tian", "Xinrui Zhang", "Zhuoren Li", "Junpeng Huang", "Xiurong Wang", "Junfan Zhu", "Jianzhou Wang", "Dongxiao Yin", "Lu Xiong"], "title": "An interactive enhanced driving dataset for autonomous driving", "comment": null, "summary": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86IEDD\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2dVLA\u6a21\u578b\u7684\u5f00\u53d1\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4ef7\u503c\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5411\u5b8c\u5168\u81ea\u52a8\u5316\u53d1\u5c55\u6240\u9700\u7684\u9c81\u68d2\u4ea4\u4e92\u80fd\u529b\uff0c\u4ee5\u53ca\u73b0\u6709\u6570\u636e\u4e2d\u4ea4\u4e92\u573a\u666f\u7684\u7a00\u758f\u6027\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Interactive Enhanced Driving Dataset (IEDD)\uff0c\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u6d41\u7a0b\u4ece\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u4e2d\u6316\u6398\u767e\u4e07\u7ea7\u522b\u7684\u4ea4\u4e92\u7247\u6bb5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5ea6\u91cf\u6307\u6807\u6765\u91cf\u5316\u4ea4\u4e92\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u751f\u6210\u4e0e\u7ed3\u6784\u5316\u8bed\u8a00\u4e25\u683c\u5bf9\u9f50\u7684\u5408\u6210\u9e1f\u77b0\u89c6\u56fe\uff08BEV\uff09\u89c6\u9891\uff0c\u6784\u5efa\u4e86IEDD-VQA\u6570\u636e\u96c6\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u5341\u79cd\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u57fa\u51c6\u7ed3\u679c\uff0c\u4ee5\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u5728\u8bc4\u4f30\u548c\u5fae\u8c03\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u91cd\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.20926", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20926", "abs": "https://arxiv.org/abs/2602.20926", "authors": ["Yuqi Huang", "Ning Liao", "Kai Yang", "Anning Hu", "Shengchao Hu", "Xiaoxing Wang", "Junchi Yan"], "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG", "comment": null, "summary": "Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\\times$ speedup over leading Graph-based RAG baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GraphRAG\u6846\u67b6\uff0c\u901a\u8fc7HyperNode Expansion\u548cLogical Path-Guided Evidence Localization\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86RAG\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "LLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u53d7\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u5f80\u5f80\u5b58\u5728\u56fa\u6709\u7684\u77e5\u8bc6\u8fb9\u754c\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faHyperNode Expansion\u548cLogical Path-Guided Evidence Localization\u7b56\u7565\uff0c\u7528\u4e8eGraphRAG\uff08HELP\uff09\u6846\u67b6\uff0c\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHELP\u5728\u591a\u4e2a\u7b80\u5355\u548c\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u6bd4\u9886\u5148\u7684\u57fa\u4e8e\u56fe\u7684RAG\u57fa\u7ebf\u5feb28.8\u500d\u3002", "conclusion": "HELP\u901a\u8fc7\u907f\u514d\u6602\u8d35\u7684\u968f\u673a\u6e38\u8d70\u548c\u8bed\u4e49\u626d\u66f2\uff0c\u5728\u4fdd\u6301\u77e5\u8bc6\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u68c0\u7d22\u5ef6\u8fdf\u3002"}}
{"id": "2602.21103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21103", "abs": "https://arxiv.org/abs/2602.21103", "authors": ["Sanket Badhe", "Deep Shah"], "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning", "comment": null, "summary": "Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\\% to 90.0\\% and 67\\% to 83\\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.", "AI": {"tldr": "PLD\u901a\u8fc7\u63d0\u53d6\u63a8\u7406\u6a21\u5f0f\u5e76\u7ec4\u7ec7\u6210\u6307\u4ee4\u5217\u8868\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u7684\u884c\u4e1a\u548c\u573a\u666f\u3002", "motivation": "\u9ad8\u7ea7\u63a8\u7406\u901a\u5e38\u9700\u8981\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u51c6\u786e\u4f46\u5ef6\u8fdf\u9ad8\uff0c\u63a8\u7406\u6210\u672c\u9ad8\u3002\u5fae\u8c03\u5c0f\u6a21\u578b\u867d\u7136\u662f\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u663e\u8457\u7684\u8d44\u6e90\u548c\u64cd\u4f5c\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u63d0\u793a\u7ea7\u84b8\u998f\uff08PLD\uff09\u3002", "method": "\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u663e\u5f0f\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u7684\u6307\u4ee4\u5217\u8868\uff0c\u4f9b\u5b66\u751f\u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u4f7f\u7528\u3002", "result": "\u5728StereoSet\u548cContract-NLI\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Gemma-3 4B\u8fdb\u884c\u8bc4\u4f30\uff0cPLD\u5c06\u5b8f\u89c2F1\u5206\u6570\u4ece57%\u63d0\u9ad8\u523090.0%\uff0c\u4ece67%\u63d0\u9ad8\u523083%\u3002", "conclusion": "PLD\u4f7f\u7d27\u51d1\u578b\u6a21\u578b\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u5ef6\u8fdf\u5f00\u9500\u5339\u914d\u524d\u6cbf\u6027\u80fd\uff0c\u8fd9\u4e9b\u8868\u8fbe\u6027\u6307\u4ee4\u4f7f\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\uff0c\u5141\u8bb8\u5bf9\u903b\u8f91\u8fdb\u884c\u5b8c\u5168\u7684\u4eba\u7c7b\u9a8c\u8bc1\uff0c\u56e0\u6b64\u8be5\u65b9\u6cd5\u975e\u5e38\u9002\u5408\u6cd5\u5f8b\u3001\u91d1\u878d\u548c\u5185\u5bb9\u5ba1\u6838\u7b49\u76d1\u7ba1\u884c\u4e1a\uff0c\u4ee5\u53ca\u9ad8\u5bb9\u91cf\u4f7f\u7528\u573a\u666f\u548c\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2602.20577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20577", "abs": "https://arxiv.org/abs/2602.20577", "authors": ["Jiaru Zhang", "Manav Gagvani", "Can Cui", "Juntong Peng", "Ruqi Zhang", "Ziran Wang"], "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion", "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.", "AI": {"tldr": "MVLAD-AD\uff1a\u7ed3\u5408\u63a9\u7801\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9ad8\u6548\u89c4\u5212\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "LLMs\u548cVLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u3001\u52a8\u4f5c\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faMVLAD-AD\u6846\u67b6\uff0c\u7ed3\u5408\u63a9\u7801\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u548c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728nuScenes\u548c\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMVLAD-AD\u5728\u89c4\u5212\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MVLAD-AD\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9ad8\u6548\u89c4\u5212\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21165", "abs": "https://arxiv.org/abs/2602.21165", "authors": ["Samah Fodeh", "Linhai Ma", "Yan Wang", "Srivani Talakokkul", "Ganesh Puthiaraju", "Afshan Khan", "Ashley Hagaman", "Sarah Lowe", "Aimee Roundtree"], "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data", "comment": null, "summary": "Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.", "AI": {"tldr": "PVminer\uff1a\u4e00\u79cd\u7528\u4e8e\u7ed3\u6784\u5316\u60a3\u8005\u58f0\u97f3\u7684NLP\u6846\u67b6\uff0c\u5728\u7406\u89e3\u60a3\u8005\u6c9f\u901a\u548c\u5065\u5eb7\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\u65b9\u9762\u8868\u73b0\u51fa\u8272", "motivation": "\u5206\u6790\u60a3\u8005\u751f\u6210\u6587\u672c\u4e2d\u7684\u60a3\u8005\u58f0\u97f3\uff08PV\uff09\uff0c\u53cd\u6620\u6c9f\u901a\u884c\u4e3a\u548c\u5065\u5eb7\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\uff08SDOH\uff09", "method": "\u63d0\u51faPVminer\uff0c\u4e00\u4e2a\u9488\u5bf9\u60a3\u8005\u58f0\u97f3\u7684NLP\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u5b89\u5168\u60a3\u8005-\u63d0\u4f9b\u8005\u6c9f\u901a\u4e2d\u7684\u60a3\u8005\u58f0\u97f3", "result": "PVminer\u5728\u5206\u5c42\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u9884\u8bad\u7ec3\u57fa\u7ebf\uff0cF1\u5206\u6570\u5206\u522b\u4e3a82.25%\uff08\u4ee3\u7801\uff09\u300180.14%\uff08\u5b50\u4ee3\u7801\uff09\u548c77.87%\uff08\u7ec4\u5408\uff09", "conclusion": "PVminer\u4e3a\u7ed3\u6784\u5316\u60a3\u8005\u58f0\u97f3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u60a3\u8005\u6c9f\u901a\u548c\u5065\u5eb7\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20"}}
{"id": "2602.20461", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20461", "abs": "https://arxiv.org/abs/2602.20461", "authors": ["Chen Zhang", "Jianghui Wang", "Bingyang Cheng", "Zhongtao Chen", "Wendong XU", "Cong Wang", "Marco Canini", "Francesco Orabona", "Yik Chung WU", "Ngai Wong"], "title": "Nonparametric Teaching of Attention Learners", "comment": "ICLR 2026 (36 pages, 6 figures)", "summary": "Attention learners, neural networks built on the attention mechanism, e.g., transformers, excel at learning the implicit relationships that relate sequences to their corresponding properties, e.g., mapping a given sequence of tokens to the probability of the next token. However, the learning process tends to be costly. To address this, we present a novel paradigm named Attention Neural Teaching (AtteNT) that reinterprets the learning process through a nonparametric teaching perspective. Specifically, the latter provides a theoretical framework for teaching mappings that are implicitly defined (i.e., nonparametric) via example selection. Such an implicit mapping is embodied through a dense set of sequence-property pairs, with the AtteNT teacher selecting a subset to accelerate convergence in attention learner training. By analytically investigating the role of attention on parameter-based gradient descent during training, and recasting the evolution of attention learners, shaped by parameter updates, through functional gradient descent in nonparametric teaching, we show for the first time that teaching attention learners is consistent with teaching importance-adaptive nonparametric learners. These new findings readily commit AtteNT to enhancing learning efficiency of attention learners. Specifically, we observe training time reductions of 13.01% for LLMs and 20.58% for ViTs, spanning both fine-tuning and training-from-scratch regimes. Crucially, these gains are achieved without compromising accuracy; in fact, performance is consistently preserved and often enhanced across a diverse set of downstream tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20463", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20463", "abs": "https://arxiv.org/abs/2602.20463", "authors": ["Zhiqi Li", "Bo Zhu"], "title": "A Long-Short Flow-Map Perspective for Drifting Models", "comment": "25 pages, 7 figures", "summary": "This paper provides a reinterpretation of the Drifting Model~\\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.", "AI": {"tldr": "\u91cd\u65b0\u89e3\u91ca\u6f02\u79fb\u6a21\u578b\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5e76\u89e3\u91ca\u7ed3\u679c", "motivation": "\u5bf9\u6f02\u79fb\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u89e3\u91ca", "method": "\u534a\u7fa4\u4e00\u81f4\u7684\u957f\u77ed\u6d41\u56fe\u5206\u89e3", "result": "\u63d0\u51fa\u65b0\u7684\u4f3c\u7136\u5b66\u4e60\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u9a8c\u8bc1", "conclusion": "\u4e3a\u7279\u5f81\u7a7a\u95f4\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411"}}
{"id": "2602.21154", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21154", "abs": "https://arxiv.org/abs/2602.21154", "authors": ["Ziwei Niu", "Hao Sun", "Shujun Bian", "Xihong Yang", "Lanfen Lin", "Yuxin Liu", "Yueming Jin"], "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning", "comment": "Accepted by ICASSP 2026", "summary": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.", "AI": {"tldr": "CG-DMER\u662f\u4e00\u79cd\u65b0\u7684ECG\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u51c6\u786e\u89e3\u91ca\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u5bf9\u4e8e\u8bca\u65ad\u5fc3\u8840\u7ba1\u75be\u75c5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCG-DMER\u7684\u5bf9\u6bd4\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u591a\u6a21\u6001ECG\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCG-DMER\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CG-DMER\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86ECG\u4fe1\u53f7\u5904\u7406\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.20932", "categories": ["cs.LG", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.20932", "abs": "https://arxiv.org/abs/2602.20932", "authors": ["Anupam Sharma", "Harish Katti", "Prajwal Singh", "Shanmuganathan Raman", "Krishna Miyapuram"], "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels", "comment": null, "summary": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.\n  We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5c42\u6b21\u611f\u77e5\u7684\u4e8b\u4ef6\u5206\u6790\uff0c\u63a2\u8ba8\u4e86EEG\u5728\u591a\u7ea7\u5c42\u6b21\u4e0a\u6355\u6349\u7269\u4f53\u8868\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5bf9\u62bd\u8c61\u654f\u611f\u3002", "motivation": "\u7814\u7a76EEG\u5728\u591a\u7ea7\u5c42\u6b21\u4e0a\u6355\u6349\u7269\u4f53\u8868\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4e8b\u4ef6\u5206\u6790\uff08episodic analysis\uff09\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528WordNet\u8fdb\u884c\u5c42\u6b21\u611f\u77e5\u7684\u4e8b\u4ef6\u91c7\u6837\uff0c\u6784\u5efa\u4e86\u6700\u5927\u7684EEG\u4e8b\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u4ecePEERS\u6570\u636e\u96c6\u4e2d\u7684EEG\u4fe1\u53f7\u4e2d\u68c0\u6d4b\u89c2\u5bdf\u5230\u7684\u6587\u672c\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u7684\u6027\u80fd\u5728\u5206\u7c7b\u7c7b\u522b\u6765\u81ea\u5c42\u6b21\u7ed3\u6784\u66f4\u9ad8\u5c42\u65f6\u6709\u6240\u63d0\u9ad8\uff0c\u8868\u660e\u5bf9\u62bd\u8c61\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u62bd\u8c61\u6df1\u5ea6\u4f5c\u4e3aEEG\u89e3\u7801\u7684\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6\uff0c\u5e76\u6fc0\u52b1\u4e86\u672a\u6765\u5728\u8fd9\u4e00\u65b9\u5411\u4e0a\u7684\u7814\u7a76\u3002"}}
{"id": "2602.20901", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20901", "abs": "https://arxiv.org/abs/2602.20901", "authors": ["Yuechen Xie", "Xiaoyan Zhang", "Yicheng Shan", "Hao Zhu", "Rui Tang", "Rong Wei", "Mingli Song", "Yuanyu Wan", "Jie Song"], "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models", "comment": "Accepted by CVPR 2026", "summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.", "code_url": "https://github.com/xieyc99/SpatiaLQA", "code_stars": 1, "code_last_update": "2026-02-22", "AI": {"tldr": "SpatiaLQA\u57fa\u51c6\u548c\u9012\u5f52\u573a\u666f\u56fe\u8f85\u52a9\u63a8\u7406\u65b9\u6cd5\u63d0\u9ad8\u4e86VLMs\u7684\u7a7a\u95f4\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1VLMs\u5728\u5e38\u89c1\u7684\u89c6\u89c9\u95ee\u7b54\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5408\u7406\u51b3\u7b56\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9012\u5f52\u573a\u666f\u56fe\u8f85\u52a9\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5c06\u590d\u6742\u573a\u666f\u9010\u6b65\u5206\u89e3\u4e3a\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u573a\u666f\u56fe\uff0c\u4ece\u800c\u589e\u5f3aVLMs\u7684\u7a7a\u95f4\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u9ad8\u7ea7\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u903b\u8f91\u63a8\u7406\u65b9\u9762\u4e5f\u9762\u4e34\u56f0\u96be\uff0c\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e4b\u524d\u7684\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "SpatiaLQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30VLMs\u7a7a\u95f4\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347VLMs\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.20974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20974", "abs": "https://arxiv.org/abs/2602.20974", "authors": ["Ahmed Mohamed Eisa Nasr", "Haris Moazam Sheikh"], "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting", "comment": "Submitted to International Conference on Machine Learning 2026", "summary": "In engineering design and scientific computing, computational cost and predictive accuracy are intrinsically coupled. High-fidelity simulations provide accurate predictions but at substantial computational costs, while lower-fidelity approximations offer efficiency at the expense of accuracy. Multi-fidelity surrogate modelling addresses this trade-off by combining abundant low-fidelity data with sparse high-fidelity observations. However, existing methods suffer from expensive training cost or rely on global correlation assumptions that often fail in practice to capture how fidelity relationships vary across the input space, leading to poor performance particularly under tight budget constraints. We introduce MAST, a method that blends corrected low-fidelity observations with high-fidelity predictions, trusting high-fidelity near observed samples and relying on corrected low-fidelity elsewhere. MAST achieves this through explicit discrepancy modelling and distance-based weighting with closed-form variance propagation, producing a single heteroscedastic Gaussian process. Across multi-fidelity synthetic benchmarks, MAST shows a marked improvement over the current state-of-the-art techniques. Crucially, MAST maintains robust performance across varying total budget and fidelity gaps, conditions under which competing methods exhibit significant degradation or unstable behaviour.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21020", "categories": ["cs.LG", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21020", "abs": "https://arxiv.org/abs/2602.21020", "authors": ["Antoine Bergerault", "Volkan Cevher", "Negar Mehr"], "title": "Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning", "comment": null, "summary": "Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $\u03b5_{\\text{BC}}$, this provides a Nash imitation gap of $\\mathcal{O}\\left(n\u03b5_{\\text{BC}}/(1-\u03b3)^2\\right)$ for a discount factor $\u03b3$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21043", "abs": "https://arxiv.org/abs/2602.21043", "authors": ["Dongik Park", "Hyunwoo Ryu", "Suahn Bae", "Keondo Park", "Hyung-Sin Kim"], "title": "T1: One-to-One Channel-Head Binding for Multivariate Time-Series Imputation", "comment": "Accepted at ICLR 2026", "summary": "Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other. We introduce T1 (Time series imputation with 1-to-1 channel-head binding), a CNN-Transformer hybrid architecture that achieves robust imputation through Channel-Head Binding--a mechanism creating one-to-one correspondence between CNN channels and attention heads. This design enables selective information transfer: when missingness corrupts certain temporal patterns, their corresponding attention pathways adaptively down-weight based on remaining observable patterns while preserving reliable cross-variable connections through unaffected channels. Experiments on 11 benchmark datasets demonstrate that T1 achieves state-of-the-art performance, reducing MSE by 46% on average compared to the second-best baseline, with particularly strong gains under extreme sparsity (70% missing ratio). The model generalizes to unseen missing patterns without retraining and uses a consistent hyperparameter configuration across all datasets. The code is available at https://github.com/Oppenheimerdinger/T1.", "code_url": "https://github.com/Oppenheimerdinger/T1", "AI": {"tldr": "T1\u662f\u4e00\u79cdCNN-Transformer\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u901a\u9053-\u5934\u90e8\u7ed1\u5b9a\u673a\u5236\u5b9e\u73b0\u9c81\u68d2\u586b\u5145\uff0c\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7f3a\u5931\u503c\u7684\u586b\u5145\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u6837\u5316\u7684\u7f3a\u5931\u6a21\u5f0f\u548c\u4e25\u91cd\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u635f\u574f\u7684\u65f6\u95f4\u7279\u5f81\u963b\u788d\u4e86\u6709\u6548\u7684\u8de8\u53d8\u91cf\u4fe1\u606f\u4f20\u9012\uff0c\u4ece\u800c\u653e\u5927\u4e86\u91cd\u5efa\u8bef\u5dee\u3002\u9c81\u68d2\u586b\u5145\u9700\u8981\u4ece\u6bcf\u4e2a\u53d8\u91cf\u7684\u7a00\u758f\u89c2\u6d4b\u4e2d\u63d0\u53d6\u65f6\u95f4\u6a21\u5f0f\uff0c\u5e76\u9009\u62e9\u6027\u5730\u8de8\u53d8\u91cf\u4f20\u9012\u4fe1\u606f\u2014\u2014\u7136\u800c\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u5176\u4e2d\u4e00\u4e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u5728\u53e6\u4e00\u65b9\u9762\u6709\u6240\u59a5\u534f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT1\u7684CNN-Transformer\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u901a\u9053-\u5934\u90e8\u7ed1\u5b9a\u673a\u5236\u5b9e\u73b0\u9c81\u68d2\u586b\u5145\u3002\u8be5\u8bbe\u8ba1\u5141\u8bb8\u9009\u62e9\u6027\u5730\u4f20\u9012\u4fe1\u606f\uff1a\u5f53\u7f3a\u5931\u6027\u635f\u574f\u67d0\u4e9b\u65f6\u95f4\u6a21\u5f0f\u65f6\uff0c\u76f8\u5e94\u7684\u6ce8\u610f\u529b\u8def\u5f84\u4f1a\u6839\u636e\u5269\u4f59\u53ef\u89c2\u5bdf\u6a21\u5f0f\u81ea\u9002\u5e94\u5730\u964d\u4f4e\u6743\u91cd\uff0c\u540c\u65f6\u901a\u8fc7\u672a\u53d7\u5f71\u54cd\u7684\u901a\u9053\u4fdd\u7559\u53ef\u9760\u7684\u8de8\u53d8\u91cf\u8fde\u63a5\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cT1\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u964d\u4f4e\u4e8646%\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4e0e\u7b2c\u4e8c\u597d\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u7a00\u758f\u6027\uff0870%\u7f3a\u5931\u7387\uff09\u4e0b\u6709\u7279\u522b\u5f3a\u7684\u6536\u76ca\u3002\u8be5\u6a21\u578b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7f3a\u5931\u6a21\u5f0f\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u4e00\u81f4\u7684\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "conclusion": "T1\u662f\u4e00\u79cd\u6709\u6548\u7684\u9c81\u68d2\u586b\u5145\u65b9\u6cd5\uff0c\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.20943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20943", "abs": "https://arxiv.org/abs/2602.20943", "authors": ["Kaiyuan Tan", "Yingying Shen", "Mingfei Tu", "Haohui Zhu", "Bing Wang", "Guang Chen", "Hangjun Ye", "Haiyang Sun"], "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling", "comment": null, "summary": "Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.", "AI": {"tldr": "UFO\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5faa\u73af\u8303\u5f0f\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u957f\u671f4D\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u548c\u95ed\u73af\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUFO\u7684\u65b0\u578b\u5faa\u73af\u8303\u5f0f\uff0c\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u548c\u524d\u9988\u65b9\u6cd5\u7684\u597d\u5904\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u671f4D\u91cd\u5efa\u3002", "result": "\u5728Waymo\u5f00\u653e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u4e2a\u5e8f\u5217\u957f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u573a\u666f\u4f18\u5316\u548c\u73b0\u6709\u524d\u9988\u65b9\u6cd5\u3002", "conclusion": "UFO\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u957f\u671f4D\u91cd\u5efa\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.21072", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21072", "abs": "https://arxiv.org/abs/2602.21072", "authors": ["Zhangjie Xia", "Yu Yang", "Pan Xu"], "title": "Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning", "comment": "33 pages, 9 figures, 11 tables", "summary": "Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.", "AI": {"tldr": "LoDADA\u901a\u8fc7\u5229\u7528\u5c40\u90e8\u52a8\u6001\u5dee\u5f02\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u6e90\u6570\u636e\u91cd\u7528\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u6e90\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoDADA\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5c40\u90e8\u52a8\u6001\u5dee\u5f02\u6765\u63d0\u9ad8\u6e90\u6570\u636e\u7684\u91cd\u7528\u7387\u3002", "method": "LoDADA\u901a\u8fc7\u805a\u7c7b\u6e90\u548c\u76ee\u6807\u6570\u636e\u96c6\u7684\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u5224\u522b\u4f30\u8ba1\u805a\u7c7b\u7ea7\u522b\u7684\u52a8\u6001\u5dee\u5f02\u3002\u4fdd\u7559\u6765\u81ea\u5dee\u5f02\u5c0f\u7684\u805a\u7c7b\u7684\u6e90\u8f6c\u6362\uff0c\u800c\u8fc7\u6ee4\u6389\u6765\u81ea\u5dee\u5f02\u5927\u7684\u805a\u7c7b\u7684\u8f6c\u6362\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLoDADA\u5728\u5229\u7528\u5c40\u90e8\u5206\u5e03\u5dee\u5f02\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5177\u6709\u4e0d\u540c\u5168\u5c40\u548c\u5c40\u90e8\u52a8\u6001\u53d8\u5316\u7684\u591a\u4e2a\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "LoDADA\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u6e90\u6570\u636e\u7684\u91cd\u7528\u7387\u3002"}}
{"id": "2602.20972", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20972", "abs": "https://arxiv.org/abs/2602.20972", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Zhiqiang Kou", "Zhongnian Li", "Gang Niu", "Masashi Sugiyama"], "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?", "comment": null, "summary": "Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\\% to 80\\% of human performance, while achieving over 90\\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\\% to 80\\% of the difference.", "AI": {"tldr": "TagLLM\u6846\u67b6\u6709\u6548\u7f29\u5c0f\u4e86MLLM\u751f\u6210\u7684\u6807\u6ce8\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u6807\u6ce8\u6548\u7387", "motivation": "\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u548c\u63d0\u9ad8\u6807\u6ce8\u6548\u7387", "method": "\u5206\u6790MLLM\u751f\u6210\u7684\u6807\u6ce8\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51faTagLLM\u6846\u67b6", "result": "TagLLM\u663e\u8457\u7f29\u5c0f\u4e86MLLM\u751f\u6210\u7684\u6807\u6ce8\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u4e0b\u6e38\u8bad\u7ec3\u6027\u80fd\u65b9\u9762\uff0c\u7f29\u5c0f\u4e86\u7ea660%\u523080%\u7684\u5dee\u8ddd", "conclusion": "TagLLM\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u6807\u6ce8\u6846\u67b6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c"}}
{"id": "2602.20980", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20980", "abs": "https://arxiv.org/abs/2602.20980", "authors": ["Yang Zhang", "Danyang Li", "Yuxuan Li", "Xin Zhang", "Tianyu Xie", "Mingming Cheng", "Xiang Li"], "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.", "AI": {"tldr": "CrystaL\u901a\u8fc7\u6539\u8fdb\u6f5c\u5728CoT\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6f5c\u5728\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u65b9\u6cd5\u5728\u4fdd\u7559\u4e2d\u95f4\u6f5c\u5728\u72b6\u6001\u4e2d\u7684\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u9650\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrystaL\u7684\u5355\u4e00\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u4e24\u6761\u8def\u5f84\u6765\u5904\u7406\u5b8c\u6574\u548c\u635f\u574f\u7684\u56fe\u50cf\u3002\u901a\u8fc7\u663e\u5f0f\u5730\u5bf9\u4e24\u6761\u8def\u5f84\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u9884\u6d4b\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\uff0cCrystaL\u5c06\u6f5c\u5728\u8868\u793a\u7ed3\u6676\u4e3a\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u8bed\u4e49\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u8f85\u52a9\u6ce8\u91ca\u6216\u5916\u90e8\u6a21\u5757\u3002", "result": "\u5728\u611f\u77e5\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8868\u660eCrystaL\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u4e00\u81f4\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "CrystaL\u901a\u8fc7\u6539\u8fdb\u6f5c\u5728CoT\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6574\u5408\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002"}}
{"id": "2602.20985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20985", "abs": "https://arxiv.org/abs/2602.20985", "authors": ["Munish Monga", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer", "comment": "Accepted at CVPR 2026", "summary": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.", "AI": {"tldr": "EW-DETR for evolving-world object detection outperforms others by 57.24% in FOGS evaluation.", "motivation": "Real-world object detection in evolving environments requires the ability to identify new classes and unseen objects without prior data access.", "method": "EWOD, EW-DETR framework with Incremental LoRA Adapters, Query-Norm Objectness Adapter, and Entropy-Aware Unknown Mixing.", "result": "EW-DETR improves FOGS by 57.24% on Pascal Series and Diverse Weather benchmarks.", "conclusion": "EW-DETR is effective in evolving-world settings and outperforms other methods."}}
{"id": "2602.20989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20989", "abs": "https://arxiv.org/abs/2602.20989", "authors": ["Zheng Gu", "Min Lu", "Zhida Sun", "Dani Lischinski", "Daniel Cohen-O", "Hui Huang"], "title": "Cycle-Consistent Tuning for Layered Image Decomposition", "comment": "Accepted to CVPR 2026. Project page: https://vcc.tech/research/2026/ImgDecom", "summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u4ece\u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u6807\u5fd7\u548c\u80cc\u666f\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u5c42\u89e3\u8026\u662f\u89c6\u89c9\u548c\u56fe\u5f62\u9886\u57df\u7684\u4e00\u4e2a\u6301\u7eed\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5c06\u6807\u5fd7\u4ece\u5176\u51fa\u73b0\u7684\u8868\u9762\u4e0a\u5206\u79bb\u51fa\u6765\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u6269\u6563\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5206\u5c42\u5206\u79bb\u7684\u4e0a\u4e0b\u6587\u56fe\u50cf\u5206\u89e3\u6846\u67b6\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u8c03\u6574\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u5faa\u73af\u4e00\u81f4\u8c03\u6574\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u8054\u5408\u8bad\u7ec3\u5206\u89e3\u548c\u7ec4\u5408\u6a21\u578b\uff0c\u5f3a\u5236\u6267\u884c\u5206\u89e3\u548c\u91cd\u7ec4\u56fe\u50cf\u4e4b\u95f4\u7684\u91cd\u5efa\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u81ea\u6211\u6539\u8fdb\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u8fed\u4ee3\u5730\u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\u751f\u6210\u793a\u4f8b\u6765\u589e\u5f3a\u8bad\u7ec3\u96c6\uff0c\u4ee5\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51c6\u786e\u548c\u8fde\u8d2f\u7684\u5206\u89e3\uff0c\u5e76\u6709\u6548\u5730\u63a8\u5e7f\u5230\u5176\u4ed6\u5206\u89e3\u7c7b\u578b\uff0c\u8868\u660e\u5b83\u4f5c\u4e3a\u5206\u5c42\u56fe\u50cf\u5206\u89e3\u7684\u7edf\u4e00\u6846\u67b6\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u4ece\u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u6807\u5fd7\u548c\u80cc\u666f\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.21010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21010", "abs": "https://arxiv.org/abs/2602.21010", "authors": ["Jiannan Huang", "Aditya Kane", "Fengzhe Zhou", "Yunchao Wei", "Humphrey Shi"], "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design", "comment": "CVPR Findings", "summary": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\\textbf{L}ow-cost and \\textbf{E}fficient \\textbf{DE}tection \\textbf{TR}ansformer), which achieves a new \\textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \\textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \\textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \\textbf{+0.6/-0.1 mAP} while achieving similar speed and \\textbf{+20\\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \\textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \\textbf{+0.4 mAP} with only \\textbf{0.4 ms} additional latency. Code and weights will be open-sourced.", "AI": {"tldr": "\u9ad8\u6548\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6027\u80fd\u5b9e\u65f6DETR\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u590d\u73b0\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u964d\u4f4e\u9884\u8bad\u7ec3\u6210\u672c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u9aa8\u5e72\u67b6\u6784\uff0c\u63d0\u51fa\u4e86EfficientNAT\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u6df7\u5408\u7f16\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u4e86Le-DETR\u6a21\u578b\u3002", "result": "Le-DETR\u5728COCO Val2017\u4e0a\u53d6\u5f97\u4e8652.9/54.3/55.1 mAP\uff0c\u901f\u5ea6\u4f18\u4e8eYOLOv12\u548cDEIM-D-FINE\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u5b9e\u65f6DETR\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u65e0\u9700\u590d\u6742\u9884\u8bad\u7ec3\u3002"}}
{"id": "2602.21168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21168", "abs": "https://arxiv.org/abs/2602.21168", "authors": ["Jingya Cheng", "Alaleh Azhir", "Jiazi Tian", "Hossein Estiri"], "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma", "comment": null, "summary": "Counterfactual inference enables clinicians to ask \"what if\" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -> AKI -> HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from \"what if this feature were different?\" to \"what if we had intervened earlier, and how would that propagate forward?\" --  yielding clinically actionable insights grounded in biological plausibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u987a\u5e8f\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4e34\u5e8a\u6570\u636e\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u60a3\u8005\u7ed3\u679c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u4e34\u5e8a\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u65b9\u6cd5\u5728\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u4e2d\u5047\u8bbe\u7279\u5f81\u72ec\u7acb\u6027\u548c\u540c\u65f6\u53ef\u4fee\u6539\u6027\u7684\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u987a\u5e8f\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u4e0d\u53ef\u53d8\u7279\u5f81\uff08\u6162\u6027\u8bca\u65ad\uff09\u548c\u53ef\u63a7\u5236\u7279\u5f81\uff08\u5b9e\u9a8c\u5ba4\u503c\uff09\uff0c\u5e76\u6a21\u62df\u5e72\u9884\u63aa\u65bd\u968f\u65f6\u95f4\u4f20\u64ad\u7684\u8fc7\u7a0b\uff0c\u6765\u5c0a\u91cd\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u57282,723\u540dCOVID-19\u60a3\u8005\uff08383\u4f8b\u957f\u671fCOVID\u5fc3\u810f\u8870\u7aed\u75c5\u4f8b\uff0c2,340\u4f8b\u5339\u914d\u5bf9\u7167\uff09\u4e2d\u5e94\u7528\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7b80\u5355\u65b9\u6cd5\u4e0b\uff0c38-67%\u7684\u6162\u6027\u75c5\u60a3\u8005\u7684\u53cd\u4e8b\u5b9e\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u6211\u4eec\u786e\u5b9a\u4e86\u4e00\u4e2a\u5fc3\u8840\u7ba1\u80be\u8fde\u9501\u53cd\u5e94\uff08CKD -> AKI -> HF\uff09\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u7684\u76f8\u5bf9\u98ce\u9669\u5206\u522b\u4e3a2.27\u548c1.19\uff0c\u8fd9\u8bf4\u660e\u4e86\u987a\u5e8f\u53cd\u4e8b\u5b9e\u53ef\u4ee5\u6355\u6349\u7684\u65f6\u95f4\u4f20\u64ad\u3002\u6846\u67b6\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4ece\u201c\u5982\u679c\u8fd9\u4e2a\u7279\u5f81\u4e0d\u540c\u4f1a\u600e\u6837\uff1f\u201d\u8f6c\u53d8\u4e3a\u201c\u5982\u679c\u6211\u4eec\u65e9\u70b9\u5e72\u9884\uff0c\u7ed3\u679c\u4f1a\u5982\u4f55\uff0c\u5e76\u5982\u4f55\u5411\u524d\u4f20\u64ad\uff1f\u201d\u2014\u2014\u4ece\u800c\u4ea7\u751f\u57fa\u4e8e\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u4e34\u5e8a\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "conclusion": "\u987a\u5e8f\u53cd\u4e8b\u5b9e\u6846\u67b6\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u60a3\u8005\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u4e34\u5e8a\u89c1\u89e3\u3002"}}
{"id": "2602.21015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21015", "abs": "https://arxiv.org/abs/2602.21015", "authors": ["Yuhao Wu", "Maojia Song", "Yihuai Lan", "Lei Wang", "Zhiqiang Hu", "Yao Xiao", "Heng Zhou", "Weihua Zheng", "Dylan Raharja", "Soujanya Poria", "Roy Ka-Wei Lee"], "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "comment": "Work in processing. Website: https://social-ai-studio.github.io/CHAIN/", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "code_url": "https://social-ai-studio.github.io/CHAIN/", "AI": {"tldr": "CHAIN\u57fa\u51c6\u8bc4\u4f30VLM\u5728\u7269\u7406\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u7406\u89e3\u7269\u7406\u7ed3\u6784\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684VLM\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63a8\u7406\u51e0\u4f55\u3001\u63a5\u89e6\u548c\u652f\u6491\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCHAIN\u57fa\u51c6\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u76843D\u7269\u7406\u9a71\u52a8\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u591f\u7406\u89e3\u3001\u89c4\u5212\u548c\u6267\u884c\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7406\u89e3\u7269\u7406\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5f80\u5f80\u65e0\u6cd5\u4ea7\u751f\u53ef\u9760\u7684\u957f\u671f\u8ba1\u5212\uff0c\u4e5f\u65e0\u6cd5\u5c06\u611f\u77e5\u5230\u7684\u7ed3\u6784\u8f6c\u6362\u4e3a\u6709\u6548\u7684\u52a8\u4f5c\u3002", "conclusion": "CHAIN\u57fa\u51c6\u4e3a\u8bc4\u4f30VLM\u548c\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u5728\u7406\u89e3\u7269\u7406\u7ed3\u6784\u548c\u56e0\u679c\u7ea6\u675f\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.21185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21185", "abs": "https://arxiv.org/abs/2602.21185", "authors": ["Justin Deschenaux", "Caglar Gulcehre", "Subham Sekhar Sahoo"], "title": "The Diffusion Duality, Chapter II: $\u03a8$-Samplers and Efficient Curriculum", "comment": null, "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PC\u91c7\u6837\u5668\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u63d0\u9ad8\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u6311\u6218\u4e86Masked\u6269\u6563\u6a21\u578b\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8d28\u91cf\uff0c\u5e76\u6311\u6218Masked\u6269\u6563\u6a21\u578b\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u7ec4\u9884\u6d4b-\u6821\u6b63\uff08PC\uff09\u91c7\u6837\u5668\uff0c\u5e76\u5c06\u5176\u4e0e\u5747\u5300\u72b6\u6001\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u8bfe\u7a0b\uff0c\u7528\u4e8e\u9ad8\u65af\u677e\u5f1b\u8bad\u7ec3\u9636\u6bb5\u3002", "result": "PC\u91c7\u6837\u5668\u5728\u8bed\u8a00\u548c\u56fe\u50cf\u5efa\u6a21\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u7956\u5148\u91c7\u6837\uff0c\u5e76\u4e14\u5728\u5339\u914d\u7684\u5355\u8bed\u71b5\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u751f\u6210\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u5728CIFAR10\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684FID/IS\u5206\u6570\u3002\u6b64\u5916\uff0c\u9ad8\u65af\u677e\u5f1b\u8bad\u7ec3\u9636\u6bb5\u7684\u8bfe\u7a0b\u51cf\u5c11\u4e8625%\u7684\u8bad\u7ec3\u65f6\u95f4\u548c33%\u7684\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "PC\u91c7\u6837\u5668\u5728\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u4f18\u4e8e\u7956\u5148\u91c7\u6837\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u65af\u677e\u5f1b\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u6311\u6218\u4e86Masked\u6269\u6563\u6a21\u578b\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5730\u4f4d\u3002"}}
{"id": "2602.21033", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21033", "abs": "https://arxiv.org/abs/2602.21033", "authors": ["Tianhao Fu", "Yucheng Chen"], "title": "MIP Candy: A Modular PyTorch Framework for Medical Image Processing", "comment": null, "summary": "Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.", "code_url": "https://github.com/ProjectNeura/MIPCandy", "code_stars": 87, "code_last_update": "2026-02-25", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21189", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21189", "abs": "https://arxiv.org/abs/2602.21189", "authors": ["Anas Barakat", "Souradip Chakraborty", "Khushbu Pahwa", "Amrit Singh Bedi"], "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training", "comment": null, "summary": "Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86Pass@k\u4f18\u5316\u53ef\u80fd\u964d\u4f4ePass@1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8d1f\u5e72\u6270\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "Pass@k\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\uff08\u5305\u62ec\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u7b80\u7b54\u9898\u63a8\u7406\uff09\u7684\u5e7f\u6cdb\u4f7f\u7528\u6027\u80fd\u6307\u6807\uff0c\u5b83\u5b9a\u4e49\u4e86\u6210\u529f\uff0c\u5982\u679ck\u4e2a\u72ec\u7acb\u91c7\u6837\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\u4efb\u4f55\u4e00\u4e2a\u901a\u8fc7\u9a8c\u8bc1\u5668\u3002\u8fd9\u79cd\u591a\u6837\u672c\u63a8\u7406\u6307\u6807\u63a8\u52a8\u4e86\u63a8\u7406\u611f\u77e5\u5fae\u8c03\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316pass@k\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u5de5\u4f5c\u62a5\u544a\u4e86\u4e00\u4e2a\u53cd\u590d\u51fa\u73b0\u7684\u6743\u8861\uff1a\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e0b\uff0cpass@k\u63d0\u9ad8\u7684\u540c\u65f6\uff0cpass@1\u4e0b\u964d\u3002\u8fd9\u79cd\u6743\u8861\u5728\u5b9e\u9645\u4e0a\u5f88\u91cd\u8981\uff0c\u56e0\u4e3apass@1\u901a\u5e38\u4ecd\u7136\u662f\u7531\u4e8e\u5ef6\u8fdf\u548c\u6210\u672c\u9884\u7b97\u3001\u9a8c\u8bc1\u5668\u8986\u76d6\u7387\u4e0d\u5b8c\u7f8e\u4ee5\u53ca\u9700\u8981\u53ef\u9760\u7684\u5355\u4e00\u51fb\u9000\u800c\u6210\u4e3a\u4e00\u4e2a\u96be\u4ee5\u64cd\u4f5c\u7684\u7ea6\u675f\u3002\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u79cd\u6743\u8861\u7684\u8d77\u6e90\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u8fc7\u7531\u63d0\u793a\u5e72\u6270\u5f15\u8d77\u7684\u68af\u5ea6\u51b2\u7a81\u6765\u51cf\u5c11pass@1\u7684pass@k\u7b56\u7565\u4f18\u5316\u7684\u7406\u8bba\u7279\u5f81\u3002\u6211\u4eec\u8868\u660e\uff0cpass@$k$\u7b56\u7565\u68af\u5ea6\u53ef\u80fd\u4e0epass@1\u68af\u5ea6\u51b2\u7a81\uff0c\u56e0\u4e3apass@$k$\u4f18\u5316\u9690\u5f0f\u5730\u91cd\u65b0\u52a0\u6743\u63d0\u793a\uff0c\u4f7f\u5176\u504f\u5411\u4f4e\u6210\u529f\u7387\u63d0\u793a\uff1b\u5f53\u8fd9\u4e9b\u63d0\u793a\u662f\u6211\u4eec\u6240\u8bf4\u7684\u8d1f\u5e72\u6270\u65f6\uff0c\u5b83\u4eec\u7684\u52a0\u91cd\u8981\u5c06pass@$k$\u66f4\u65b0\u65b9\u5411\u65cb\u8f6c\u79bb\u5f00pass@1\u65b9\u5411\u3002\u6211\u4eec\u4f7f\u7528\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u6765\u8bf4\u660e\u6211\u4eec\u7684\u7406\u8bba\u53d1\u73b0\u3002", "method": "\u7814\u7a76\u4e86pass@k\u4e0epass@1\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u8fc7\u68af\u5ea6\u51b2\u7a81\u5f15\u8d77\u7684\u7406\u8bba\u7279\u5f81\uff0c\u5f53pass@k\u7b56\u7565\u4f18\u5316\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u5e72\u6270\u51cf\u5c11pass@1\u65f6\u3002\u4f7f\u7528pass@$k$\u7b56\u7565\u68af\u5ea6\u4e0epass@1\u68af\u5ea6\u53ef\u80fd\u53d1\u751f\u51b2\u7a81\uff0c\u56e0\u4e3apass@$k$\u4f18\u5316\u9690\u5f0f\u5730\u91cd\u65b0\u52a0\u6743\u63d0\u793a\uff0c\u4f7f\u5176\u504f\u5411\u4f4e\u6210\u529f\u7387\u63d0\u793a\uff1b\u5f53\u8fd9\u4e9b\u63d0\u793a\u662f\u8d1f\u5e72\u6270\u65f6\uff0c\u5b83\u4eec\u7684\u52a0\u91cd\u8981\u5c06pass@$k$\u66f4\u65b0\u65b9\u5411\u65cb\u8f6c\u79bb\u5f00pass@1\u65b9\u5411\u3002\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0pass@k\u7b56\u7565\u68af\u5ea6\u53ef\u80fd\u4e0epass@1\u68af\u5ea6\u51b2\u7a81\uff0c\u56e0\u4e3apass@$k$\u4f18\u5316\u9690\u5f0f\u5730\u91cd\u65b0\u52a0\u6743\u63d0\u793a\uff0c\u4f7f\u5176\u504f\u5411\u4f4e\u6210\u529f\u7387\u63d0\u793a\uff1b\u5f53\u8fd9\u4e9b\u63d0\u793a\u662f\u8d1f\u5e72\u6270\u65f6\uff0c\u5b83\u4eec\u7684\u52a0\u91cd\u8981\u5c06pass@$k$\u66f4\u65b0\u65b9\u5411\u65cb\u8f6c\u79bb\u5f00pass@1\u65b9\u5411\u3002\u901a\u8fc7\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "Pass@k\u7b56\u7565\u4f18\u5316\u53ef\u80fd\u4f1a\u901a\u8fc7\u68af\u5ea6\u51b2\u7a81\u51cf\u5c11pass@1\uff0c\u5c24\u5176\u662f\u5728\u8d1f\u5e72\u6270\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cpass@k\u4f18\u5316\u53ef\u80fd\u4f1a\u5bf9pass@1\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2602.21042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21042", "abs": "https://arxiv.org/abs/2602.21042", "authors": ["Bonan Liu", "Zeyu Zhang", "Bingbing Meng", "Han Wang", "Hanshuo Zhang", "Chengping Wang", "Daji Ergu", "Ying Cai"], "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages", "comment": null, "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.", "code_url": "https://github.com/AIGeeksGroup/OmniOCR", "code_stars": 0, "code_last_update": "2026-02-24", "AI": {"tldr": "OmniOCR\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5c11\u6570\u6c11\u65cf\u6587\u5b57OCR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f4e\u79e9\u81ea\u9002\u5e94\u548c\u7a00\u758f\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53c2\u6570\u9ad8\u6548\u7684OCR\u4efb\u52a1\u3002", "motivation": "\u7531\u4e8e\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u7684\u4e66\u5199\u7cfb\u7edf\u590d\u6742\u3001\u6807\u6ce8\u7a00\u7f3a\u4ee5\u53ca\u5386\u53f2\u548c\u73b0\u4ee3\u5f62\u5f0f\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5728\u4f4e\u8d44\u6e90\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86OmniOCR\uff0c\u8fd9\u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5c11\u6570\u6c11\u65cf\u6587\u5b57\u7684\u901a\u7528\u6846\u67b6\u3002OmniOCR\u5f15\u5165\u4e86\u52a8\u6001\u4f4e\u79e9\u81ea\u9002\u5e94\uff08Dynamic LoRA\uff09\u6765\u5206\u914d\u6a21\u578b\u5bb9\u91cf\u8de8\u5c42\u548c\u811a\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u81ea\u9002\u5e94\u5e76\u4fdd\u7559\u77e5\u8bc6\u3002\u7a00\u758f\u6b63\u5219\u5316\u526a\u679d\u5197\u4f59\u66f4\u65b0\uff0c\u786e\u4fdd\u7d27\u51d1\u9ad8\u6548\u7684\u9002\u5e94\uff0c\u65e0\u9700\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728TibetanMNIST\u3001Shui\u3001\u53e4\u4ee3\u5f5d\u8bed\u548c\u4e1c\u5df4\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cOmniOCR\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7840\u6a21\u578b\u548c\u6807\u51c6\u540e\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u53c2\u6570\u6548\u7387\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e8639%-66%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "OmniOCR\u5728\u5c11\u6570\u6c11\u65cf\u6587\u5b57\u7684OCR\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.21053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21053", "abs": "https://arxiv.org/abs/2602.21053", "authors": ["Shimin Wen", "Zeyu Zhang", "Xingdou Bian", "Hongjie Zhu", "Lulu He", "Layi Shama", "Daji Ergu", "Ying Cai"], "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection", "comment": null, "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.", "code_url": "https://github.com/AIGeeksGroup/OCR-Agen", "AI": {"tldr": "\u901a\u8fc7\u80fd\u529b\u53cd\u601d\u548c\u8bb0\u5fc6\u53cd\u601d\uff0c\u6211\u4eec\u7684\u65b0\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86VLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684VLMs\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u6211\u6821\u6b63\u673a\u5236\uff0c\u96be\u4ee5\u72ec\u7acb\u7ea0\u6b63\u8ba4\u77e5\u504f\u5dee\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fed\u4ee3\u81ea\u6211\u6821\u6b63\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8d4b\u4e88\u6a21\u578b\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u80fd\u529b\u53cd\u601d\u548c\u8bb0\u5fc6\u53cd\u601d\u3002", "result": "\u5728OCRBench v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684OCR-Agent\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u5b50\u96c6\u4e0a\u5206\u522b\u6bd4\u5f53\u524d\u7684\u5f00\u6e90SOTA\u6a21\u578bInternVL3-8B\u9ad8+2.0\u548c+1.2\uff0c\u540c\u65f6\u5728\u89c6\u89c9\u7406\u89e3\uff0879.9\uff09\u548c\u63a8\u7406\uff0866.5\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u81ea\u6211\u610f\u8bc6\u7684\u53cd\u520d\u53ef\u4ee5\u663e\u8457\u589e\u5f3aVLMs\u7684\u63a8\u7406\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2602.21098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21098", "abs": "https://arxiv.org/abs/2602.21098", "authors": ["Hao Lu", "Richard J. Radke"], "title": "Optimizing Occupancy Sensor Placement in Smart Environments", "comment": null, "summary": "Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on careful sensor placement. To address this issue, we propose an automatic sensor placement method that determines optimal sensor layouts for a given number of sensors, and can predict the counting accuracy of such a layout. In particular, given the geometric constraints of an office environment, we simulate a large number of occupant trajectories. We then formulate the sensor placement problem as an integer linear programming (ILP) problem and solve it with the branch and bound method. We demonstrate the effectiveness of the proposed method based on simulations of several different office environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eILP\u7684\u81ea\u52a8\u4f20\u611f\u5668\u653e\u7f6e\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5360\u7528\u533a\u57df\u8ba1\u6570\u7cbe\u5ea6\u3002", "motivation": "\u5b9e\u73b0\u8282\u80fd\uff0c\u901a\u8fc7\u5728\u9700\u8981\u7684\u5730\u65b9\u63d0\u4f9b\u7167\u660e\u3001\u4f9b\u6696\u548c\u51b7\u5374\uff0c\u4e86\u89e3\u5546\u4e1a\u5efa\u7b51\u73af\u5883\u4e2d\u5c45\u6c11\u7684\u4f4d\u7f6e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u4f20\u611f\u5668\u653e\u7f6e\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u786e\u5b9a\u7ed9\u5b9a\u6570\u91cf\u4f20\u611f\u5668\u7684\u6700\u4f73\u5e03\u5c40\uff0c\u5e76\u53ef\u4ee5\u9884\u6d4b\u6b64\u7c7b\u5e03\u5c40\u7684\u8ba1\u6570\u7cbe\u5ea6\u3002\u5c06\u4f20\u611f\u5668\u653e\u7f6e\u95ee\u9898\u5efa\u6a21\u4e3a\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5206\u652f\u5b9a\u754c\u6cd5\u89e3\u51b3\u3002", "result": "\u57fa\u4e8e\u5bf9\u591a\u4e2a\u4e0d\u540c\u529e\u516c\u73af\u5883\u7684\u6a21\u62df\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u65f6\u8bc6\u522b\u533a\u57df\u5360\u7528\u60c5\u51b5\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5c45\u6c11\u7684\u65e5\u5e38\u6d3b\u52a8\u6216\u4fb5\u72af\u9690\u79c1\u3002"}}
{"id": "2602.21141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21141", "abs": "https://arxiv.org/abs/2602.21141", "authors": ["Jose Moises Araya-Martinez", "Thushar Tom", "Adri\u00e1n Sanchis Reig", "Pablo Rey Valiente", "Jens Lambrecht", "J\u00f6rg Kr\u00fcger"], "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception", "comment": null, "summary": "Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. Furthermore, we benchmark recent Reality-to-Simulation techniques for 3D asset creation from 2D images of real parts. Combined with Domain Randomization, these synthetic assets provide low-overhead, transferable data even for parts lacking 3D files. We also introduce IRIS, the Industrial Real-Sim Imagery Set, containing 32 categories with diverse textures, intra-class variation, strong inter-class similarities and about 20,000 labels. Ablations on multiple benchmarks outline guidelines for efficient data generation with SynthRender. Our method surpasses existing approaches, achieving 99.1% mAP@50 on a public robotics dataset, 98.3% mAP@50 on an automotive benchmark, and 95.3% mAP@50 on IRIS.", "AI": {"tldr": "SynthRender\u548cIRIS\u5728\u7269\u4f53\u611f\u77e5\u6a21\u578b\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u4ee3\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u578b\u5728\u534a\u53d7\u63a7\u6761\u4ef6\u4e0b\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u6846\u67b6SynthRender\uff0c\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86IRIS\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6SynthRender\uff0c\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86IRIS\u6570\u636e\u96c6\uff0c\u5305\u542b32\u4e2a\u7c7b\u522b\uff0c\u7ea620000\u4e2a\u6807\u7b7e\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86SynthRender\u7684\u6709\u6548\u6027\u3002", "result": "SynthRender\u5728\u516c\u5171\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8699.1%\u7684mAP@50\uff0c\u5728\u6c7d\u8f66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e8698.3%\u7684mAP@50\uff0c\u5728IRIS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8695.3%\u7684mAP@50\u3002", "conclusion": "SynthRender\u6846\u67b6\u548cIRIS\u6570\u636e\u96c6\u5728\u63d0\u9ad8\u7269\u4f53\u611f\u77e5\u6a21\u578b\u7684\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2602.21175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21175", "abs": "https://arxiv.org/abs/2602.21175", "authors": ["Jianglin Lu", "Simon Jenni", "Kushal Kafle", "Jing Shi", "Handong Zhao", "Yun Fu"], "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models", "comment": null, "summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.", "code_url": "https://github.com/Jianglin954/QCQC", "code_stars": 9, "code_last_update": "2026-02-16", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8d28\u91cf\u53ef\u63a7\u7684\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u6765\u4e30\u5bcc\u77ed\u67e5\u8be2\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u7d22\u7ed3\u679c\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u9488\u5bf9\u77ed\u548c\u6a21\u7cca\u7684\u7528\u6237\u67e5\u8be2\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u65b9\u6cd5\uff0c\u4ee5\u4e30\u5bcc\u77ed\u67e5\u8be2\u5e76\u5f15\u5165\u56fe\u50cf\u8d28\u91cf\u7684\u6982\u5ff5\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u67e5\u8be2\u8865\u5168\u51fd\u6570\uff0c\u5c06\u4e0d\u660e\u786e\u7684\u67e5\u8be2\u6269\u5c55\u4e3a\u63cf\u8ff0\u6027\u5f62\u5f0f\uff0c\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u5982\u59ff\u6001\u3001\u573a\u666f\u548c\u7f8e\u5b66\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u68c0\u7d22\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5f25\u5408\u4e86\u73b0\u4ee3VLM\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u77ed\u7528\u6237\u67e5\u8be2\u7684\u4e0d\u660e\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.21179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21179", "abs": "https://arxiv.org/abs/2602.21179", "authors": ["Nicol\u00e1s Gaggion", "Maria J. Ledesma-Carbayo", "Stergios Christodoulidis", "Maria Vakalopoulou", "Enzo Ferrante"], "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision", "comment": null, "summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21186", "abs": "https://arxiv.org/abs/2602.21186", "authors": ["Haoyi Jiang", "Liu Liu", "Xinjie Wang", "Yonghao He", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang"], "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning", "comment": null, "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.", "code_url": "https://github.com/hustvl/Spa3R", "code_stars": 2, "code_last_update": "2026-02-24", "AI": {"tldr": "Spa3R\u901a\u8fc7PSFM\u57283D VQA\u4e0a\u53d6\u5f97\u7a81\u7834\u3002", "motivation": "\u7a7a\u95f4\u667a\u80fd\u5e94\u8be5\u4ece2D\u89c6\u89c9\u4e2d\u5185\u5728\u5730\u4ea7\u751f\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u6307\u4ee4\u8c03\u6574\u3002", "method": "Spa3R\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u672a\u6446\u653e\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u5b66\u4e60\u7edf\u4e00\u7684\u3001\u89c6\u89d2\u4e0d\u53d8\u7684\u7a7a\u95f4\u8868\u793a\u3002", "result": "Spa3-VLM\u57283D VQA\u4e0a\u5b9e\u73b0\u4e8658.6%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "PSFM\u662f\u63d0\u9ad8\u7a7a\u95f4\u667a\u80fd\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2602.21195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21195", "abs": "https://arxiv.org/abs/2602.21195", "authors": ["Xingyi Cheng", "Julien Maufront", "Aur\u00e9lie Di Cicco", "Dani\u00ebl M. Pelt", "Manuela Dezi", "Daniel L\u00e9vy"], "title": "Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography", "comment": null, "summary": "Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}

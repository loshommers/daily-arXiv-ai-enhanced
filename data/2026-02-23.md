<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 35]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.AI](#cs.AI) [Total: 6]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频字幕模型，通过结合运动学和语言分析，提高了对运动细节的描述能力，并减少了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视频字幕模型在描述精细运动细节和避免幻觉问题上存在局限。

Method: 提出了一种自动标注流程，结合运动学和语言分析，构建了一个新的开源数据集KPM-Bench，并提出了一种MoPE算法来提取运动属性。

Result: KPM-Bench包含了详细的视频字幕对、问题和答案对以及评估集，MoPE算法有效地减少了幻觉问题，提高了运动视频字幕模型的可靠性。

Conclusion: 本文提出的方法和工具有助于提高视频字幕模型对运动细节的描述能力，并减少幻觉问题。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的手部动画系统，结合了新的数据集和模型，显著提高了自然手部动作建模的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决自然手部动作建模的不足，现有的方法依赖于有限动作和情境的摄影棚捕获数据集，这使得它们难以扩展到“野外”环境。此外，当代模型及其训练方案在捕捉动画保真度与文本运动对齐方面存在困难。

Method: （1）引入“3D Hands in the Wild”（3D-HIW），一个包含32K 3D手部运动序列和对齐文本的数据集；（2）提出基于LLM的手部动画系统CLUTCH，具有两个关键创新：（a）SHIFT，一种新的VQ-VAE架构，用于标记手部运动；（b）几何细化阶段，用于微调LLM；（3）构建3D-HIW的数据标注流程，结合视觉语言模型（VLMs）和最先进的3D手部跟踪器；（4）采用SHIFT，一种部分模态分解的VQ-VAE，以改善泛化能力和重建保真度；（5）引入几何细化阶段，CLUTCH与直接应用于解码手部运动参数的重建损失进行共监督。

Result: 实验在文本到运动和运动到文本任务上取得了最先进的性能，建立了可扩展的野外手部运动建模的第一个基准。

Conclusion: 该研究通过引入新的数据集和模型，提高了自然手部动作建模的准确性和可扩展性，为手部动画领域做出了贡献。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: Achieved state-of-the-art zero-shot segmentation with VLMs and SAM, best results on 19 remote sensing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery.

Method: Proposed approach integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. CLIP is used as mask selector for SAM's grid-based proposals, while GPT-5 generates click prompts for SAM in zero-shot setting and a LoRA-tuned Qwen-VL model.

Result: Achieved state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. Best results on 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks.

Conclusion: The proposed approach demonstrates strong capabilities in zero-shot text-guided segmentation of remote sensing imagery.

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [4] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT: A simple, efficient video segmentation model without tracking modules.


<details>
  <summary>Details</summary>
Motivation: Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules.

Method: Proposes VidEoMT, a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. Introduces a lightweight query propagation mechanism and a query fusion strategy.

Result: VidEoMT achieves competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone.

Conclusion: VidEoMT is a simple and efficient video segmentation model that eliminates the need for dedicated tracking modules, achieving competitive accuracy and high speed.

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [5] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出VQPP基准，探索视频查询性能预测，并展示其在LLM训练中的应用。


<details>
  <summary>Details</summary>
Motivation: Query performance prediction (QPP)在信息检索任务中非常重要，但在基于内容的视频检索 (CBVR) 方面研究不足。

Method: 提出第一个视频查询性能预测基准（VQPP），包含两个文本到视频检索数据集和两个CBVR系统。

Result: 结果表明，预检索预测器可以获得有竞争力的性能，并且VQPP可以用于训练大型语言模型（LLM）进行查询重写任务。

Conclusion: VQPP为视频领域的QPP研究提供了一个有价值的基准。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [6] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 分析揭示评估协议问题，强调数据划分的重要性。


<details>
  <summary>Details</summary>
Motivation: 对Liu和Szirányi提出的姿态识别方法进行方法学分析，特别是对其评估协议的有效性进行评估。

Method: 通过分析混淆矩阵、学习曲线和数据集构建，揭示了评估中存在的数据泄露问题。

Result: 指出评估结果无法衡量对未见个体的泛化能力，强调在基于视觉的姿态识别研究中，特别是在需要可靠识别未见人手势的应用（如无人机与人类交互）中，独立于主体的数据划分的重要性。

Conclusion: 验证了Liu和Szirányi的评估协议存在问题，并强调了独立于主体的数据划分的重要性。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [7] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出了一种新的长视频理解框架，有效处理信息提取和压缩问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 分析长视频序列的挑战，特别是内存限制和信息提取方面。

Method: 提出一种基于信息密度自适应的视频采样器（AVS）和基于自动编码器的时空视频压缩器（SVC），并结合多模态大型语言模型（MLLM）。

Result: 系统在长视频理解任务和标准视频理解基准测试中表现出色，展示了其有效性和多功能性。

Conclusion: 该框架在处理长时间视频序列的复杂性方面表现出色，具有广泛的应用前景。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [8] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [9] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 提出了一种利用稀疏多模态距离测量的方法，提高了单图像新视角合成的质量和一致性


<details>
  <summary>Details</summary>
Motivation: 提高单图像新视角合成的质量和一致性

Method: 结合稀疏的多模态距离测量，提出了一种多模态深度重建框架，使用局部高斯过程公式在角度域中对深度进行建模，并使用重建的深度和不确定性作为现有扩散渲染管道中单目深度估计器的替代

Result: 实验表明，使用基于稀疏距离的重建替代仅视觉的深度，显著提高了单图像新视角视频生成中的几何一致性和视觉质量

Conclusion: 可靠几何先验对于基于扩散的视角合成至关重要，并且即使在极端稀疏的水平上，多模态传感也具有实际益处

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [10] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑的Vision Transformer，在医学成像和边缘部署的临床系统中表现出色，同时保持较小的参数量和快速的推理时间。


<details>
  <summary>Details</summary>
Motivation: 在自然图像中，Vision Transformers的有效性依赖于位置嵌入和类别标记，这些先验可能会在空间布局弱或不一致的情况下阻碍泛化，这在医学成像和边缘部署的临床系统中很常见。

Method: 引入ZACH-ViT（Zero-token Adaptive Compact Hierarchical Vision Transformer），这是一种紧凑的Vision Transformer，它移除了位置嵌入和[CLS]标记，通过全局平均池化在补丁表示上实现排列不变性。

Result: 在七个MedMNIST数据集上进行了评估，涵盖了二元和多类任务，在严格的少样本协议下（每类50个样本，固定超参数，五个随机种子）。经验分析表明，ZACH-ViT（0.25M参数，从头开始训练）在BloodMNIST上取得了最强的优势，在PathMNIST上与TransMIL保持竞争力，而在具有强解剖先验的数据集（OCTMNIST、OrganAMNIST）上相对优势降低，与架构假设一致。

Conclusion: 这些发现支持这样一种观点：将架构的归纳偏差与数据结构对齐可能比追求通用基准优势更重要。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [11] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET通过多层表示对齐，提升了VLA模型的3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: VLA模型在2D数据上预训练，缺乏3D空间理解。

Method: ROCKET，一种残差导向的多层表示对齐框架。

Result: 在LIBERO上达到98.5%的成功率，在LIBERO-Plus和RoboTwin上表现优异。

Conclusion: ROCKET在VLA模型中实现了有效的3D空间理解。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [12] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: MQAF通过记忆机制实现图像质量评估，有效解决了参考图像质量限制问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于现有全参考图像质量评估（FR-IQA）方法受限于参考图像质量，限制了实际应用。受人类视觉系统长期记忆能力启发，提出了一种基于记忆的图像质量感知框架（MQAF）。

Method: MQAF通过建立记忆库存储扭曲模式，动态切换双模式质量评估策略，以减少对高质量参考图像的依赖。当有参考图像时，通过自适应加权参考信息并与记忆库中存储的扭曲模式进行比较来获取参考引导的质量评分。当没有参考图像时，框架依赖于记忆库中的扭曲模式来推断图像质量，实现无参考质量评估（NR-IQA）。

Result: 实验结果表明，该方法在多个数据集上优于最先进的方法，同时适应无参考和全参考任务。

Conclusion: MQAF在图像质量评估方面具有优越性能，有效解决了参考图像质量限制问题，适用于实际应用场景。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [13] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: MUOT_3M和MUTrack为水下目标跟踪提供了新的基准和跟踪器，显著提高了性能，并建立了实际应用的新的基础。


<details>
  <summary>Details</summary>
Motivation: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets.

Method: We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model.

Result: Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

Conclusion: MUOT_3M and MUTrack provide a new benchmark and tracker for underwater object tracking, significantly improving performance and establishing a new foundation for practical applications.

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [14] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的情感视觉定制方法，通过高效精确的情感操纵实现了情感内容的定制。


<details>
  <summary>Details</summary>
Motivation: 针对现有视觉定制研究主要依赖于客观的信号对齐，忽略了主观情感内容，缺乏通用基础模型的问题，提出了一种基于LLM的情感视觉定制任务。

Method: 提出了一种高效且精确的情感操纵方法，包括高效的跨情感转换模块和精确的外情感保留模块。

Result: 在构建的L-AVC数据集上进行了综合实验评估，证明了所提出的方法在情感视觉定制任务中的优越性。

Conclusion: 强调了情感信息在L-AVC中的重要性，以及EPEM在高效精确操纵此类信息中的有效性。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [15] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度安全视频理解（DeepSVU）任务和UPRM方法，用于更全面地理解和评估视频中的威胁。


<details>
  <summary>Details</summary>
Motivation: 由于现有文献中关于安全视频理解（SVU）的研究主要集中在检测和定位视频中的威胁（例如：枪击、抢劫），而缺乏有效地生成和评估威胁原因的能力，本文旨在填补这一空白。

Method: 本文提出了一个名为深度安全视频理解（DeepSVU）的新聊天范式SVU任务，旨在不仅识别和定位威胁，还归因和评估威胁段的原因。针对任务中存在的两个关键挑战：1）如何有效地建模粗到细的物理世界信息（例如：人类行为、物体交互和背景上下文）以提高DeepSVU任务；2）如何自适应地权衡这些因素。为了解决这些挑战，本文提出了一种新的统一物理世界正则化MoE（UPRM）方法。具体来说，UPRM结合了两个关键组件：统一物理世界增强MoE（UPE）块和物理世界权衡正则化器（PTR），分别解决上述两个挑战。

Result: 在DeepSVU指令数据集（即UCF-C指令和CUVA指令）上进行的广泛实验表明，UPRM优于多种先进的视频LLMs以及非VLM方法。这证实了粗到细的物理世界信息在DeepSVU任务中的重要性，并证明了我们的UPRM在捕捉此类信息方面的有效性。

Conclusion: 本文提出的UPRM方法有效地解决了DeepSVU任务中的关键挑战，并在实际应用中取得了良好的效果。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [16] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 我们提出了一种名为 UAOR 的方法，它通过在 VLA 模型中注入观察信息来减少不确定性，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLA 方法需要额外的观察线索或模块，增加了数据收集和训练成本。

Method: 我们提出了一种名为 Uncertainty-aware Observation Reinjection (UAOR) 的模块，该模块在 VLA 模型中注入关键观察信息以减少不确定性。

Result: UAOR 在模拟和真实世界任务中提高了 VLA 模型的性能，无需额外的观察线索或模块。

Conclusion: 我们的方法 UAOR 显著提高了 VLA 模型的性能，并在模拟和真实世界任务中表现出色。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [17] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出DCAG，显著提高图像编辑模型的编辑保真度。


<details>
  <summary>Details</summary>
Motivation: 控制编辑强度是扩散图像编辑模型的关键需求。

Method: 提出了一种名为双通道注意力引导（DCAG）的无监督框架，同时操作键通道和价值通道。

Result: DCAG在所有保真度指标上优于仅键通道的引导，尤其在局部编辑任务中表现显著。

Conclusion: DCAG框架在图像编辑任务中提高了编辑保真度。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [18] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST是一种用于Few-Shot Action Recognition的创新框架，通过时空知识学习，显著提高了识别性能。


<details>
  <summary>Details</summary>
Motivation: 识别新动作类别需要使用少量标记视频，但现有的方法在提供足够的背景知识方面存在局限性。

Method: 提出了一种名为DiST的创新分解-融合框架，利用大型语言模型提供的解耦时空知识来学习表达式的多粒度原型。在分解阶段，将动作名称分解为多种时空属性描述。在融合阶段，提出空间/时间知识补偿器（SKC/TKC）来发现具有区分性的对象级和帧级原型。

Result: 在五个标准FSAR数据集上实现了最先进的成果。

Conclusion: DiST通过利用解耦的时空知识，在Few-Shot Action Recognition任务上取得了显著的性能提升。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [19] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard: A topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance.


<details>
  <summary>Details</summary>
Motivation: City-scale person re-identification across distributed cameras requires handling appearance changes and complying with data protection rules.

Method: CityGuard, a topology-aware transformer, integrates a dispersion-adaptive metric learner, spatially conditioned attention, and differentially private embedding maps.

Result: Experiments show gains in retrieval precision and query throughput, confirming the framework's practicality for privacy-critical urban identity matching.

Conclusion: CityGuard effectively addresses the challenges of person re-identification across distributed cameras while ensuring privacy protection.

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [20] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M通过引入时间一致性感知的方法，提高了文本到动作生成的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有两阶段框架在文本到动作生成中忽略跨序列时间一致性的问题，导致语义错位和物理上不可能的动作。

Method: 提出TCA-T2M框架，包括时间一致性感知的空间VQ-VAE（TCaS-VQ-VAE）和掩码动作转换器，以及动力学约束块。

Result: 在HumanML3D和KIT-ML基准测试中，TCA-T2M实现了最先进的性能，突出了时间一致性在稳健和连贯的文本到动作生成中的重要性。

Conclusion: 时间一致性对于稳健和连贯的文本到动作生成至关重要。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [21] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 通过去除全监督和减少训练数据，该方法在nuScenes上显著提高了BEV分割的性能和质量。


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头方法依赖于昂贵且不一致标注的BEV地面实况数据。

Method: 两阶段训练策略，预训练阶段去除全监督，微调阶段数据量减半。使用BEVFormer预测，对Mask2Former生成的多视角语义伪标签进行训练。时间损失鼓励帧间一致性。微调阶段仅需50%的数据集。

Result: 在nuScenes上，方法将性能和BEV分割质量提升了+2.5pp mIoU，同时将标注数据的使用量减少了一半，总训练时间减少了三分之二。

Conclusion: 可微投影加相机视角伪标签产生了可迁移的BEV特征和降低标签的自动感知的可扩展途径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [22] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: 为了解决德文尼手写文本在基准数据集中代表性不足的问题，研究人员引入了DohaScript，这是一个大规模、多作者的数据集，用于支持手写识别、作者识别、风格分析和生成建模等任务。


<details>
  <summary>Details</summary>
Motivation: 手写德文尼文字在公开可用的基准数据集中严重代表性不足，现有资源规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和作者级别多样性，这限制了它们在现代数据驱动手写分析中的实用性。

Method: 引入了DohaScript，这是一个大规模、多作者的手写印地语文本数据集，从531个独特的贡献者那里收集而来。数据集设计为一个并行风格语料库，其中所有作者转录相同的一组六个传统的印地语道哈（对句）。这种受控设计使得可以系统地分析作者特定的变异，独立于语言内容，并支持手写识别、作者识别、风格分析和生成建模等任务。

Result: 数据集附带不可识别的的人口统计数据、基于客观清晰度和分辨率标准的严格质量控制，以及页面级布局难度注释，这有助于分层基准测试。基线实验表明，数据集具有明显的质量分离和强大的泛化能力，对未见过的作者。

Conclusion: DohaScript旨在作为标准化和可重复的基准，以推动在低资源脚本设置中对连续手写德文尼文字的研究。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [23] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT通过预测特征来加速DiT，在保持生成质量的同时，实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 为了解决Diffusion Transformers (DiT)在图像和视频生成中的高计算成本问题

Method: 提出PrediT，一种无需训练的加速框架，将特征预测作为线性多步问题，并采用经典线性多步方法进行预测，同时结合校正器以防止误差累积，并使用动态步长调制机制来调整预测范围

Result: 在多种基于DiT的图像和视频生成模型上，该方法实现了高达5.54倍的延迟降低，同时质量损失可忽略不计

Conclusion: PrediT在保持生成质量的同时，显著提高了DiT的生成效率

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [24] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: 提出OODBench，评估VLMs处理异常分布数据的能力，发现当前模型在此方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，很难保证所有数据都满足独立同分布的假设，而处理异常分布数据的能力对某些应用至关重要。

Method: 提出OODBench，一个以自动化为主、人工验证为辅的方法，用于构建新的基准和评估VLM处理异常分布数据的能力。

Result: 在OODBench上，当前VLMs在处理异常分布数据时仍存在明显的性能下降。

Conclusion: OODBench为评估VLMs处理异常分布数据的能力提供了一个有效的基准，并为未来研究提供了重要的见解。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [25] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard：一种基于融合思维链推理和强化学习的短视频广告内容审核框架。


<details>
  <summary>Details</summary>
Motivation: 当前短视频平台上的广告需要更精细的监管，以应对欺骗性的视觉、语音和字幕。

Method: 提出BLM-Guard，一个融合思维链推理、基于规则的政策原则和批评引导的奖励的内容审核框架。使用规则驱动的数据合成管道生成结构化场景描述、推理链和标签，以降低标注成本。然后使用强化学习通过平衡因果一致性和政策遵守来细化模型。多任务架构模拟模态内操作（例如，夸张的图像）和跨模态不匹配（例如，字幕-语音偏差），提高鲁棒性。

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化方面优于强基线。

Conclusion: BLM-Guard是一个有效的短视频广告内容审核框架，能够提高审核的准确性和鲁棒性。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [26] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [27] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 这项研究探讨了离散图像分词器对对抗攻击的脆弱性，并提出了一种防御方法，以增强其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中越来越受欢迎，但其对对抗攻击的脆弱性尚未得到研究。

Method: 首先，提出旨在扰动离散分词器提取的特征的攻击，从而改变提取的标记。其次，通过无监督对抗训练微调流行的分词器，以防御这种脆弱性。

Result: 该研究提高了分词器对无监督和端到端监督攻击的鲁棒性，并将其推广到未见过的任务和数据。

Conclusion: 该研究强调了分词器鲁棒性在下游任务中的关键作用，并提出了安全多模态基础模型开发的重要一步。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [28] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一种新的多实例生成框架，通过实例详情提取器和细节融合模块提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法在细粒度语义理解方面的局限性，特别是在处理复杂文本描述时。

Method: 提出了一种名为DEIG的新型框架，该框架集成了实例详情提取器（IDE）和细节融合模块（DFM）。IDE将文本编码嵌入转换为紧凑的、实例感知的表示，DFM则应用基于实例的掩码注意力来防止实例间的属性泄漏。

Result: 实验表明，DEIG在空间一致性、语义准确性和组合泛化方面始终优于现有方法。

Conclusion: DEIG是一种用于细粒度和可控多实例生成的框架，在多个基准测试中表现出色，并且易于集成到标准扩散型管道中。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [29] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: 提出LOTS框架，通过结合全局和局部草图指导，改进了时尚图像生成。


<details>
  <summary>Details</summary>
Motivation: 有效结合文本和视觉模式需要遵循草图视觉结构，同时利用文本的局部属性指导。

Method: 提出了一种名为LOTS的框架，结合全局草图指导和多个局部草图-文本对，通过多级条件阶段和扩散对指导阶段进行图像生成。

Result: 实验表明，该方法提高了全局结构遵循，同时利用了更丰富的局部语义指导，优于现有技术。

Conclusion: 该方法在全局结构遵循和局部语义指导方面都取得了改进，优于现有技术。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [30] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [31] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++：一种基于3DGS的鲁棒NVS框架，提高现实环境中的3D NVS性能。


<details>
  <summary>Details</summary>
Motivation: 高质量图像采集在现实环境中具有挑战性，因为复杂的光照变化和相机成像管道的固有局限性。这些问题在多视图捕获中更加严重，其中光照、传感器响应和图像信号处理器（ISP）配置的差异引入了光度学和色度不一致性，违反了现代3D新视图合成（NVS）方法（包括神经辐射场（NeRF）和3D高斯分层（3DGS））的光度一致性假设，导致重建和渲染质量下降。

Method: 提出了一种基于3DGS的框架Luminance-GS++，用于在各种光照条件下进行鲁棒的NVS。该方法结合了全局视图自适应亮度调整和局部像素级残差细化，以实现精确的色彩校正。进一步设计了无监督目标，共同强制执行亮度校正和多视图几何和光度一致性。

Result: 在包括低光、过曝和复杂的亮度和色度变化在内的挑战性场景中，Luminance-GS++在性能上达到了最先进的水平。与修改底层表示的先前方法不同，该方法保留了显式的3DGS公式，在提高重建保真度的同时保持了实时渲染效率。

Conclusion: Luminance-GS++是一种有效的框架，可以解决现实环境中高质量图像采集的挑战，提高3D NVS的性能。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [32] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出了一种基于G-LoG双滤波器的方法，用于从医学图像中提取特征，在MedMNIST数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析（TDA）领域，构建实用滤波器以检测拓扑和几何特征是一项重要任务。

Method: 利用高斯拉普拉斯算子的能力来增强医学图像的边界，定义G-LoG（高斯-高斯拉普拉斯）双滤波器以生成更适合多参数持久性模块的特征。将体积图像建模为有界函数，然后我们证明了从我们的有界函数上的双滤波器获得的持久性模块上的交错距离在有界函数的最大范数方面是稳定的。

Result: 在MedMNIST数据集上进行了实验，将我们的双滤波器与单参数滤波器和已建立的深度学习基线（包括Google AutoML Vision、ResNet、AutoKeras和auto-sklearn）进行了比较。实验结果表明，我们的双滤波器显著优于单参数滤波器。值得注意的是，一个简单的多层感知器（MLP）在由我们的双滤波器生成的拓扑特征上训练，其性能与在原始数据集上训练的复杂深度学习模型相当。

Conclusion: 本文提出了一种基于G-LoG双滤波器的方法，用于从医学图像中提取更适合多参数持久性模块的特征，并在MedMNIST数据集上取得了优于单参数滤波器和深度学习基线的性能。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [33] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出了一种基于退化流形的自感知目标检测框架，有效提高了目标检测器在各种不利条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在模糊、噪声、压缩、恶劣天气或分辨率变化等不利条件下提高目标检测器的性能，提出了自感知目标检测方法。

Method: 基于退化流形的自感知框架，通过多层对比学习训练轻量级嵌入头，对检测器的特征空间进行结构化处理。

Result: 在合成损坏基准、跨数据集零样本迁移和自然天气引起的分布偏移的广泛实验中，该方法表现出强大的原始-退化分离能力，在不同检测器架构中表现出一致的行为，以及在语义偏移下的鲁棒泛化能力。

Conclusion: 退化感知表示几何为实际且与检测器无关的基础提供了可能。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [34] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: Developed a human-centric video world model for XR, improving interaction and control.


<details>
  <summary>Details</summary>
Motivation: Extended reality (XR) needs generative models that can respond to users' tracked real-world motion.

Method: Proposing a human-centric video world model, evaluating diffusion transformer conditioning strategies, and training a bidirectional video diffusion model.

Result: Improved task performance and higher perceived control over actions.

Conclusion: The proposed model enhances embodied interaction in XR environments.

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [35] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream方法通过自适应选择策略和无训练检索混合专家模型，在视频理解任务中显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理密集视频流时，特征编码导致查询帧相似度得分随时间增加，偏向于检索后期帧。

Method: 引入自适应选择策略以减少token冗余，同时保留局部时空信息；提出无训练检索混合专家模型，利用外部模型更好地识别相关帧。

Result: MemStream方法在CG-Bench、LVBench和VideoMME（长）测试中分别比ReKV和Qwen2.5-VL-7B提高了8.0%、8.5%和2.4%的准确率。

Conclusion: MemStream方法在CG-Bench、LVBench和VideoMME（长）测试中分别比ReKV和Qwen2.5-VL-7B提高了8.0%、8.5%和2.4%的准确率。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [37] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [38] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 研究发现引导向量在处理非线性潜在行为表示时不可靠，需要更鲁棒的引导方法。


<details>
  <summary>Details</summary>
Motivation: 探究为什么引导的可靠性在不同行为之间有所不同，以及它如何受到引导向量训练数据的影响。

Method: 首先，发现训练激活差异之间的余弦相似度越高，预测的引导越可靠。其次，观察到行为数据集中，正负激活在引导方向上更好地分离，其引导更可靠。最后，在引导向量训练上，不同提示变化具有方向上的差异，但表现相似，并表现出数据集间的相关有效性。

Result: 发现当潜在的目标行为表示不能被线性引导方向有效地近似时，引导向量是不可靠的。这些发现为引导不可靠性提供了实用的诊断，并激励了更鲁棒的引导方法的发展，这些方法明确考虑了非线性潜在行为表示。

Conclusion: 引导向量在处理非线性潜在行为表示时不可靠，需要更鲁棒的方法。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [39] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出了一种新的主题建模方法，该方法使用语言模型和上下文信息来提高主题质量和检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型通过重建文档的词袋（BoW）表示来优化，忽略了上下文信息，并且在处理数据稀疏性方面存在困难。

Method: 提出了一种使用语言模型（LM）构建语义地基于的软标签目标的新方法，通过将条件在专用提示下的下一个标记概率投影到预定义词汇表上，以获得上下文丰富的监督信号。通过训练主题模型使用LM隐藏状态重建软标签，该方法产生了更高质量的、与语料库潜在主题结构更紧密对齐的主题。

Result: 在三个数据集上的实验表明，该方法在主题一致性、纯度方面相对于现有基线有显著提高。此外，还引入了一种基于检索的度量标准，该度量标准显示，在识别语义相似文档方面，该方法显著优于现有方法，突出了其在检索应用中的有效性。

Conclusion: 提出的方法在主题建模方面取得了显著成果，提高了主题质量和检索性能。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [40] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 该研究提出了CondMedQA基准和CGR框架，以提高生物医学问答中条件推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学问答系统往往假设医学知识适用于所有情况，然而现实中的临床推理本质上是条件性的，几乎每个决定都取决于患者特定的因素，如合并症和禁忌症。现有的基准评估方法没有评估这种条件推理，基于检索增强或图的方法缺乏确保检索到的知识适用于给定上下文的明确机制。

Method: 提出了一种名为CondMedQA的条件生物医学问答基准，其中包含多跳问题，其答案随患者条件而变化。此外，还提出了一种名为条件门控推理（CGR）的新框架，该框架构建条件感知知识图，并基于查询条件有选择地激活或修剪推理路径。

Result: 结果表明，CGR在选择适用于条件的答案方面更可靠，在生物医学问答基准测试中与现有技术相当甚至超过，突出了对条件性进行明确建模对于稳健医学推理的重要性。

Conclusion: 该研究强调了条件性建模在生物医学问答中的重要性，并提出了CGR框架以解决现有方法的不足。

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [41] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [42] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate通过结合图检索和LLM推理，为临床NLP管道提供了可适应不同表型和分析要求的概念集构建框架。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有工具在构建临床概念集方面的不足，特别是对于直接操作UMLS CUIs的NLP管道，提出了CUICurate框架。

Method: 构建UMLS知识图谱并进行语义检索，从图谱中检索候选CUIs，然后通过大型语言模型（LLM）进行过滤和分类。

Result: CUICurate在五个临床概念上产生了比人工基准更大的、更完整的概念集，同时保持了与人类精确度相匹配。

Conclusion: CUICurate提供了一种可扩展且可重复的方法，以支持UMLS概念集的构建，显著降低了人工工作量。

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [43] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 本研究评估了在金融问答中检索长法规文件时，不同检索策略的效果，并引入了领域微调的页面评分器，以提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在金融问答中检索长法规文件时，检索精确上下文以证明答案的可靠性问题。

Method: 评估检索在多个粒度级别（文档、页面和块）上的性能，并引入基于Oracle的分析来提供检索和生成性能的经验性上限。在FinanceBench的150个问题子集上，比较了包括密集型、稀疏型、混合型和分层方法在内的各种检索策略，以及重新排序和查询重写。

Result: 结果显示，在文档发现方面取得的改进往往转化为更强的页面召回率，但Oracle性能仍然表明在页面和块级别检索方面仍有改进空间。通过引入领域微调的页面评分器，我们提高了页面召回率和块检索。

Conclusion: 研究证明了在金融问答中，通过改进检索策略，可以显著提高页面和块检索的性能。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [44] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 本文探讨了人工智能时代评估的重要性，指出传统评估方法的局限性，并提出了新的评估方法和框架。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统从静态模型发展到复合型、使用工具的智能体，评估已成为核心控制功能。

Method: 本文分析了现有的评估实践，并探讨了评估管道如何引入沉默的失败模式，以及高基准分数如何误导团队，以及智能体系统如何改变性能测量的意义。

Result: 本文主张，我们应该明确评估在人工智能时代的角色，特别是对于智能体：不是作为性能的舞台，而是一种衡量信任、迭代和治理非确定性系统的学科。

Conclusion: 传统的评估方法已不再适用于人工智能时代，需要新的评估方法和框架。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [45] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 对话式AI的说服力受政治偏见感知的影响。


<details>
  <summary>Details</summary>
Motivation: 为了测试对话式AI在纠正公众误解和传播错误信息方面的有效性，并了解其政治中立性对效果的影响。

Method: 通过在美国进行的一项预先注册的问卷调查实验，让参与者与ChatGPT进行关于个人持有的经济政策误区的三轮对话。

Result: 结果表明，表明LLM（大型语言模型）对受访者所在党派有偏见的简短信息使得说服力降低了28%。转录分析表明，警告改变了互动：受访者更倾向于反驳，并且不太愿意接受。

Conclusion: 对话式AI的说服力受政治因素影响，受限于对党派一致性的认知。

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [46] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 提出对抗性问题生成框架，提高模型特定领域适应性


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在特定领域适应性不足的问题，以及高质量数据稀缺的问题

Method: 提出一种对抗性问题生成框架，通过比较模型输出和专家模型输出，生成语义挑战性问题

Result: 在LegalBench语料库的特定子集上，该方法以更少的合成样本实现了更高的准确率

Conclusion: 该方法有效提高了模型在特定领域的适应性，并提高了样本效率

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [47] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 本研究提出了一种基于高频注意力特征的超轻量级幻觉检测器，有效提高了LLMs的可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了确保大型语言模型（LLMs）在基于上下文生成中的可靠性，幻觉检测至关重要。

Method: 受信号处理启发，我们通过分析生成过程中的注意力变化，引入了一种频率感知的注意力视角。我们将注意力分布建模为离散信号，并提取反映注意力快速局部变化的高频分量。

Result: 实验表明，与基于验证、基于内部表示和基于注意力的方法相比，该方法在模型和任务上取得了性能提升。

Conclusion: 该研究提出了一种基于高频注意力特征的超轻量级幻觉检测器，有效地提高了LLMs在基于上下文生成中的可靠性。

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [48] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [49] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [50] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: Hybrid approach for clickbait detection outperforms traditional methods and enhances interpretability.


<details>
  <summary>Details</summary>
Motivation: Clickbait headlines degrade the quality of online information and undermine user trust.

Method: Hybrid approach combining transformer-based text embeddings with linguistically motivated informativeness features. Evaluation using natural language processing techniques on classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers.

Result: Best-performing model: XGBoost over embeddings augmented with 15 explicit features, achieving an F1-score of 91%. Outperforms TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. Proposed feature set enhances interpretability.

Conclusion: The hybrid approach effectively detects clickbait and improves interpretability.

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [51] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 提出新的解码框架Info-Gain Sampler，显著提升MDMs生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了提高Masked Diffusion Models（MDMs）的生成质量，研究提出了新的解码框架。

Method: 提出Info-Gain Sampler，通过平衡即时不确定性与未来掩码标记的信息增益来解码。

Result: 在多种架构和任务上，Info-Gain Sampler优于现有采样器。例如，推理任务上平均准确率提高3.6%，创意写作胜率63.1%。

Conclusion: Info-Gain Sampler在MDMs的解码中表现出色，能够显著提升生成质量。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [52] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 提出了一种新的衡量实时句子理解工作记忆负荷的方法。


<details>
  <summary>Details</summary>
Motivation: 研究实时句子理解对工作记忆的负荷，以及如何衡量这种负荷。

Method: 提出基于信息论形式化的处理存储成本度量方法，并从预训练的神经语言模型中估计。

Result: 该方法在英语中的三个分析中得到了验证，包括恢复已知的处理不对称性、与基于语法的存储成本相关联以及预测阅读时间变化。

Conclusion: 该方法为衡量实时句子理解的工作记忆负荷提供了一种新的视角。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [53] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 提出了一种通过选择性干预低置信度标记来提高大型语言模型推理可靠性的方法


<details>
  <summary>Details</summary>
Motivation: 推理不确定性高度局部化，低置信度标记在推理错误和输出扩展中占比不均

Method: 信心驱动的对比解码，检测低置信度标记并进行选择性干预

Result: 在数学推理基准测试中显著提高准确率，同时大幅减少输出长度，最小化KV缓存开销

Conclusion: 提出了一种基于信心驱动的对比解码方法，通过针对低置信度标记的干预提高推理可靠性

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [54] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 研究创建了首个罗马尼亚心理健康语料库，用于分析情感和心理健康。


<details>
  <summary>Details</summary>
Motivation: 分析人类心理学、情感和心理健康，以及解决罗马尼亚心理健康语料库的缺失问题。

Method: 通过开放式问题和标准化PHQ-9和GAD-7筛选问卷收集数据，并使用统计分析和文本分析、情绪检测和主题建模方法。

Result: 创建了第一个罗马尼亚抑郁和焦虑语料库PsihoRo，包含205位受访者的文本。

Conclusion: PsihoRo为理解和分析罗马尼亚人群心理健康文本提供了重要资源。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [55] [Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning](https://arxiv.org/abs/2602.18326)
*Tao Wu,Adam Kapelner*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.

</details>


### [56] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara框架在印度司法系统中预测和解释上诉判决，效果显著，GPT-4o mini模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在印度等司法管辖区，法院面临大量案件积压，人工智能在法律判决预测方面具有变革性潜力。

Method: 我们提出了Vichara，这是一个针对印度司法系统量身定制的框架，它预测并解释上诉判决。Vichara处理英文上诉案件审理文件，并将它们分解为决策点。决策点是包含法律问题、决定权、结果、理由和时间背景的离散法律判断。结构化表示将核心判断及其背景分离出来，从而实现准确的预测和可解释的解释。Vichara的解释采用受IRAC（问题-规则-应用-结论）框架启发的结构化格式，并针对印度法律推理进行了调整。这提高了可解释性，使法律专业人士能够有效地评估预测的合理性。

Result: 在PredEx和印度法律文件语料库（ILDC_expert）的专家注释子集两个数据集上，Vichara超过了现有的判决预测基准，GPT-4o mini在两个数据集上均取得了最高性能（PredEx上的F1值为81.5，ILDC_expert上的F1值为80.3），其次是Llama-3.1-8B。对生成的解释在清晰度、链接和实用性三个指标上进行的人评显示，GPT-4o mini的可解释性优于其他模型。

Conclusion: Vichara在预测和解释上诉判决方面表现优异，具有很高的准确性和可解释性。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [57] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 本研究提出了一种新的验证方法，用于评估政治立场预测的准确性，并构建了一个大规模知识库，用于支持政治领域的推理和生成。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中知识表示需要捕捉主观、连续的属性，如政治立场，这些属性与成对验证冲突，而成对验证是广泛接受的人类评估的黄金标准。

Method: 提出了一种双尺度验证框架，用于应用于论辩话语中的政治立场预测，结合了点wise和成对的人类标注。使用22个语言模型，构建了一个包含来自30场辩论的23,228个论点的政治立场预测大规模知识库，这些辩论出现在英国政治电视节目《Question Time》上。

Result: 点wise评估显示人机模型有中等程度的一致性（Krippendorff的α=0.578），反映了固有的主观性，而成对验证揭示了人类和模型衍生排名之间有显著的一致性（最佳模型的α=0.86）。

Conclusion: 本研究贡献了：（i）一种平衡可扩展性和可靠性的主观连续知识的实用验证方法；（ii）一个经过验证的结构化论辩知识库，它能够支持基于图的推理和检索增强生成在政治领域的应用；（iii）从本质上主观的现实世界话语中提取序数结构，从而提高了传统符号或分类方法不足的领域的知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [58] [RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering](https://arxiv.org/abs/2602.18425)
*Deniz Qian,Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: RVR是一种多轮检索框架，旨在提高答案覆盖率。


<details>
  <summary>Details</summary>
Motivation: 全面检索各种文档以解决有广泛有效答案的查询。

Method: RVR多轮检索框架，包括检索器、验证器和后续的检索器。

Result: 在QAMPARI数据集上，RVR实现了至少10%的相对和3%的绝对召回率提升；在领域外数据集上，也实现了稳定的收益。

Conclusion: 该方法在多个答案检索数据集上表现优于基线，实现了至少10%的相对和3%的绝对召回率提升。在两个领域外数据集上，也实现了稳定的收益。

Abstract: Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.

</details>


### [59] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: VIRAASAT和SCoM框架可提高LLM在印度文化推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有文化基准在测试LLM印度文化推理能力方面的不足。

Method: 提出VIRAASAT数据集，利用知识图谱和Symbolic Chain-of-Manipulation（SCoM）框架。

Result: VIRAASAT包含3200多个涉及多跳推理的问答，SCoM框架在监督微调实验中提高了20%的性能。

Conclusion: SCoM框架能够显著提高LLM在涉及印度文化的推理任务中的性能，并有助于构建文化意识推理模型。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 使用本体增强语言模型，在数学领域取得初步成效，但存在检索质量和不相关上下文的挑战。


<details>
  <summary>Details</summary>
Motivation: 语言模型在高风险专业领域中存在基本局限性，如幻觉、脆弱性和缺乏形式化基础，这些问题特别突出。

Method: 通过检索增强生成，使用形式化领域本体来提高语言模型的可靠性。以数学为证明概念，实现一个神经符号管道，利用OpenMath本体，结合混合检索和交叉编码重排序，将相关定义注入模型提示中。

Result: 在MATH基准测试中，使用三种开源模型进行评估，发现当检索质量高时，本体引导的上下文可以改善性能；但当上下文不相关时，会主动降低性能，突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体可以提高语言模型的可靠性，但在检索质量不高的情况下，本体引导的上下文可能会降低性能。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [61] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: 通过设计Token Games (TTG) 评估框架，模型通过创建自己的谜题相互挑战，评估了10个前沿模型，发现创建好谜题对当前模型来说仍然是一个极具挑战性的任务。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力越来越具有挑战性，特别是使用博士级别领域知识挑战最强大模型的最新基准测试。

Method: 设计Token Games (TTG) 评估框架，模型通过创建自己的谜题相互挑战。利用编程谜题格式，找到使布尔函数返回True的输入，灵活地表示问题并验证解决方案。使用成对对决的结果计算Elo等级，比较模型之间的相对能力。

Result: 在TTG上评估了10个前沿模型，与现有基准测试（如Humanity's Last Exam）的排名非常接近，无需人工创建谜题。发现创建好谜题对当前模型来说仍然是一个极具挑战性的任务，这是以前基准测试没有衡量的。

Conclusion: 提出了新的评估推理的范式，这些范式无法通过设计饱和，并允许测试模型的其他技能，如创造力和任务创建，以及问题解决能力。

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [62] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: WorkflowPerturb是一个用于研究工作流程评估指标的受控基准，支持对工作流程评估分数的严重程度感知解释。


<details>
  <summary>Details</summary>
Motivation: LLM-based systems increasingly generate structured workflows for complex tasks, but automatic evaluation of these workflows is difficult.

Method: We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It applies realistic, controlled perturbations to golden workflows.

Result: Benchmarking multiple metric families and analyzing their sensitivity and calibration, results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores.

Conclusion: WorkflowPerturb provides a controlled benchmark for studying workflow evaluation metrics and supports severity-aware interpretation of workflow evaluation scores.

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [63] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: Combining offline RL and cross-embodiment learning enhances robot policy pre-training, but challenges remain in managing conflicts between different robot types.


<details>
  <summary>Details</summary>
Motivation: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform.

Method: Uniting offline reinforcement learning (offline RL) with cross-embodiment learning.

Result: Combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. Embodiment-based grouping strategy reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

Conclusion: Offline RL and cross-embodiment learning improve robot policy pre-training, but require careful handling of conflicts between different robot types.

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [64] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [65] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD：一种新的在线多智能体强化学习框架，通过使用扩散策略和优化联合分布值函数，显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 增强策略的表达能力对于在在线多智能体强化学习（MARL）中实现卓越性能至关重要。扩散型生成模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示，但其在在线MARL中的潜力尚未得到充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出了一种新的在线离策略多智能体强化学习（OMAD）框架，使用扩散策略来协调智能体。关键创新是放宽策略目标，以最大化缩放联合熵，从而在不依赖可处理似然的情况下进行有效探索。在集中式训练和分布式执行（CTDE）范式下，使用联合分布值函数来优化分布式扩散策略。它利用可处理的熵增强目标来引导扩散策略的同时更新，从而确保稳定的协调。

Result: 在MPE和MAMuJoCo上的大量评估表明，该方法在10个不同的任务中达到了新的最先进水平，在样本效率方面表现出显著的2.5倍到5倍改进。

Conclusion: OMAD框架在在线多智能体强化学习中实现了卓越的性能，为未来研究提供了新的方向。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [66] [Topological Exploration of High-Dimensional Empirical Risk Landscapes: general approach, and applications to phase retrieval](https://arxiv.org/abs/2602.17779)
*Antoine Maillard,Tony Bonnaire,Giulio Biroli*

Main category: stat.ML

TL;DR: 分析了高维Gaussian单指数模型的实证风险最小化景观，提出简化框架，并应用于实际相位恢复问题。


<details>
  <summary>Details</summary>
Motivation: 研究高维高斯单指数模型（广义线性模型）的实证风险最小化景观。

Method: 使用Kac-Rice公式分析不同类型临界点的复杂性，包括局部最小值。

Result: 提出了一种简化变分公式的框架，并应用于实际相位恢复问题，对损失景观进行了完全的拓扑相图分析。

Conclusion: 为高维统计模型中的损失景观和拓扑平凡化现象的渐近研究开辟了新的途径。

Abstract: We consider the landscape of empirical risk minimization for high-dimensional Gaussian single-index models (generalized linear models). The objective is to recover an unknown signal $\boldsymbolθ^\star \in \mathbb{R}^d$ (where $d \gg 1$) from a loss function $\hat{R}(\boldsymbolθ)$ that depends on pairs of labels $(\mathbf{x}_i \cdot \boldsymbolθ, \mathbf{x}_i \cdot \boldsymbolθ^\star)_{i=1}^n$, with $\mathbf{x}_i \sim \mathcal{N}(0, I_d)$, in the proportional asymptotic regime $n \asymp d$. Using the Kac-Rice formula, we analyze different complexities of the landscape -- defined as the expected number of critical points -- corresponding to various types of critical points, including local minima. We first show that some variational formulas previously established in the literature for these complexities can be drastically simplified, reducing to explicit variational problems over a finite number of scalar parameters that we can efficiently solve numerically. Our framework also provides detailed predictions for properties of the critical points, including the spectral properties of the Hessian and the joint distribution of labels. We apply our analysis to the real phase retrieval problem for which we derive complete topological phase diagrams of the loss landscape, characterizing notably BBP-type transitions where the Hessian at local minima (as predicted by the Kac-Rice formula) becomes unstable in the direction of the signal. We test the predictive power of our analysis to characterize gradient flow dynamics, finding excellent agreement with finite-size simulations of local optimization algorithms, and capturing fine-grained details such as the empirical distribution of labels. Overall, our results open new avenues for the asymptotic study of loss landscapes and topological trivialization phenomena in high-dimensional statistical models.

</details>


### [67] [Drift Estimation for Stochastic Differential Equations with Denoising Diffusion Models](https://arxiv.org/abs/2602.17830)
*Marcos Tapia Costa,Nikolas Kantas,George Deligiannidis*

Main category: stat.ML

TL;DR: 提出了一种基于条件扩散模型的漂移函数估计方法，该方法在低维和高维情况下均表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究在已知扩散系数的多变量随机微分方程中，对时间齐次漂移函数的估计。

Method: 将漂移估计作为条件去噪问题，并提出了一个条件扩散模型训练的漂移函数估计器。

Result: 在低维情况下，该估计器与经典方法相匹配，在高维情况下保持竞争力，且优势不能仅归因于架构设计选择。

Conclusion: 提出了一种有效的漂移函数估计方法，在多变量随机微分方程中具有较高的精度和竞争力。

Abstract: We study the estimation of time-homogeneous drift functions in multivariate stochastic differential equations with known diffusion coefficient, from multiple trajectories observed at high frequency over a fixed time horizon. We formulate drift estimation as a denoising problem conditional on previous observations, and propose an estimator of the drift function which is a by-product of training a conditional diffusion model capable of simulating new trajectories dynamically. Across different drift classes, the proposed estimator was found to match classical methods in low dimensions and remained consistently competitive in higher dimensions, with gains that cannot be attributed to architectural design choices alone.

</details>


### [68] [Interactive Learning of Single-Index Models via Stochastic Gradient Descent](https://arxiv.org/abs/2602.17876)
*Nived Rajaraman,Yanjun Han*

Main category: stat.ML

TL;DR: 研究了SGD在单指数模型学习中的应用，发现其存在预热阶段，通过合适的学习率调度，在两个阶段都达到了近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 研究单指数模型的序列学习问题，特别是SGD作为解决方案，但其学习动态尚不明确。

Method: 分析SGD在单指数模型中的应用，特别是其学习动态。

Result: 证明了SGD在单指数模型学习过程中存在一个明显的“预热”阶段，并且通过合适的学习率调度，在两个阶段都达到了近最优的样本复杂度和后悔保证。

Conclusion: SGD在自适应数据下的单指数模型学习中仍然具有很强的竞争力。

Abstract: Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.

</details>


### [69] [Learning from Biased and Costly Data Sources: Minimax-optimal Data Collection under a Budget](https://arxiv.org/abs/2602.17894)
*Michael O. Harding,Vikas Singh,Kirthevasan Kandasamy*

Main category: stat.ML

TL;DR: 本研究提出了一种在固定预算下进行多源数据收集的方法，该方法在降低风险方面取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 研究在固定预算下多源数据收集，关注人口均值和组条件均值的估计。

Method: 开发了一种采样计划，该计划最大化有效样本量，并与经典的后分层估计器配对，以降低其风险。

Result: 证明了该方法在预算内达到最小化最大风险，并将其技术扩展到最小化预测问题的额外风险。

Conclusion: 该方法为具有昂贵和异构数据源的多源学习提供了一种原则性的方法。

Abstract: Data collection is a critical component of modern statistical and machine learning pipelines, particularly when data must be gathered from multiple heterogeneous sources to study a target population of interest. In many use cases, such as medical studies or political polling, different sources incur different sampling costs. Observations often have associated group identities (for example, health markers, demographics, or political affiliations) and the relative composition of these groups may differ substantially, both among the source populations and between sources and target population.
  In this work, we study multi-source data collection under a fixed budget, focusing on the estimation of population means and group-conditional means. We show that naive data collection strategies (e.g. attempting to "match" the target distribution) or relying on standard estimators (e.g. sample mean) can be highly suboptimal. Instead, we develop a sampling plan which maximizes the effective sample size: the total sample size divided by $D_{χ^2}(q\mid\mid\overline{p}) + 1$, where $q$ is the target distribution, $\overline{p}$ is the aggregated source distribution, and $D_{χ^2}$ is the $χ^2$-divergence. We pair this sampling plan with a classical post-stratification estimator and upper bound its risk. We provide matching lower bounds, establishing that our approach achieves the budgeted minimax optimal risk. Our techniques also extend to prediction problems when minimizing the excess risk, providing a principled approach to multi-source learning with costly and heterogeneous data sources.

</details>


### [70] [On the Generalization and Robustness in Conditional Value-at-Risk](https://arxiv.org/abs/2602.18053)
*Dinesh Karthik Mulumudi,Piyushi Manupriya,Gholamali Aminian,Anant Raj*

Main category: stat.ML

TL;DR: 本研究分析了CVaR在重尾数据下的学习理论，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 分析CVaR在重尾数据下的统计行为，以及其在学习理论中的应用。

Method: 建立CVaR在重尾和污染数据下的学习理论分析，推导一般化和剩余风险界限，并提出剪枝中位数CVaR估计器。

Result: 建立了CVaR学习在何种情况下能泛化和稳健，以及在尾部稀缺时不可避免的不稳定性。

Conclusion: 为CVaR学习提供了原则性的描述，包括其泛化、稳健性和不稳定性。

Abstract: Conditional Value-at-Risk (CVaR) is a widely used risk-sensitive objective for learning under rare but high-impact losses, yet its statistical behavior under heavy-tailed data remains poorly understood. Unlike expectation-based risk, CVaR depends on an endogenous, data-dependent quantile, which couples tail averaging with threshold estimation and fundamentally alters both generalization and robustness properties. In this work, we develop a learning-theoretic analysis of CVaR-based empirical risk minimization under heavy-tailed and contaminated data. We establish sharp, high-probability generalization and excess risk bounds under minimal moment assumptions, covering fixed hypotheses, finite and infinite classes, and extending to $β$-mixing dependent data; we further show that these rates are minimax optimal. To capture the intrinsic quantile sensitivity of CVaR, we derive a uniform Bahadur-Kiefer type expansion that isolates a threshold-driven error term absent in mean-risk ERM and essential in heavy-tailed regimes. We complement these results with robustness guarantees by proposing a truncated median-of-means CVaR estimator that achieves optimal rates under adversarial contamination. Finally, we show that CVaR decisions themselves can be intrinsically unstable under heavy tails, establishing a fundamental limitation on decision robustness even when the population optimum is well separated. Together, our results provide a principled characterization of when CVaR learning generalizes and is robust, and when instability is unavoidable due to tail scarcity.

</details>


### [71] [Box Thirding: Anytime Best Arm Identification under Insufficient Sampling](https://arxiv.org/abs/2602.18186)
*Seohwa Hwang,Junyong Park*

Main category: stat.ML

TL;DR: B3是一种高效且灵活的最佳臂识别算法，在有限预算约束下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对固定预算约束下的最佳臂识别（BAI）问题，提出了一种灵活高效的算法Box Thirding（B3）。

Method: 使用迭代的三元比较方法，每次比较三个臂，探索表现最好的臂，延迟中位数臂的比较，丢弃表现最差的臂。

Result: 即使没有T的先验知识，B3也能实现与Successive Halving（SH）相当的epsilon最佳臂误识别概率。在New Yorker卡通标题比赛数据集上，B3在有限预算约束下，在简单后悔方面优于现有方法。

Conclusion: B3是一种有效的最佳臂识别算法，适用于固定预算约束下的任何时间BAI和大规模N的情景。

Abstract: We introduce Box Thirding (B3), a flexible and efficient algorithm for Best Arm Identification (BAI) under fixed-budget constraints. It is designed for both anytime BAI and scenarios with large N, where the number of arms is too large for exhaustive evaluation within a limited budget T. The algorithm employs an iterative ternary comparison: in each iteration, three arms are compared--the best-performing arm is explored further, the median is deferred for future comparisons, and the weakest is discarded. Even without prior knowledge of T, B3 achieves an epsilon-best arm misidentification probability comparable to Successive Halving (SH), which requires T as a predefined parameter, applied to a randomly selected subset of c0 arms that fit within the budget. Empirical results show that B3 outperforms existing methods under limited-budget constraints in terms of simple regret, as demonstrated on the New Yorker Cartoon Caption Contest dataset.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [72] [Exploiting Liquidity Exhaustion Attacks in Intent-Based Cross-Chain Bridges](https://arxiv.org/abs/2602.17805)
*André Augusto,Christof Ferreira Torres,André Vasconcelos,Miguel Correia*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Intent-based cross-chain bridges have emerged as an alternative to traditional interoperability protocols by allowing off-chain entities (\emph{solvers}) to immediately fulfill users' orders by fronting their own liquidity. While improving user experience, this approach introduces new systemic risks, such as solver liquidity concentration and delayed settlement. In this paper, we propose a new class of attacks called \emph{liquidity exhaustion attacks} and a replay-based parameterized attack simulation framework. We analyze 3.5 million cross-chain intents that moved \$9.24B worth of tokens between June and November 2025 across three major protocols (Mayan Swift, Across, and deBridge), spanning nine blockchains.
  For rational attackers, our results show that protocols with higher solver profitability, such as deBridge, are vulnerable under current parameters: 210 historical attack instances yield a mean net profit of \$286.14, with 80.5\% of attacks profitable. In contrast, Across remains robust in all tested configurations due to low solver margins and very high liquidity, while Mayan Swift is generally secure but becomes vulnerable under stress-test conditions. Under byzantine attacks, we show that it is possible to suppress availability across all protocols, causing dozens of failed intents and solver profit losses of up to \$978 roughly every 16 minutes. Finally, we propose an optimized attack strategy that exploits patterns in the data to reduce attack costs by up to 90.5\% compared to the baseline, lowering the barrier to liquidity exhaustion attacks.

</details>


### [73] [Symfrog-512: High-Capacity Sponge-Based AEAD Cipher (1024-bit State)](https://arxiv.org/abs/2602.17900)
*Victor Duarte Melo*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This submission includes a complete reference implementation together with deterministic test vectors and a reproducible benchmark suite. All source code, build instructions, and regression artifacts are publicly available in the project repository, enabling independent verification and reimplementation of the scheme. The AEAD construction is fully specified, including domain separation, rate and capacity choices, tag generation, and the exact file format used by the reference CLI. Reported performance numbers are produced by the built in benchmark tool under documented hardware and compiler settings. All security claims are made strictly within the ideal permutation model following standard sponge and duplex bounds, and no stronger guarantees are asserted for the concrete permutation beyond the documented analysis and empirical behavior. The implementation aims for constant time behavior with respect to secret dependent operations, although no formal side channel proof is provided. The project is released under the MIT license, and external cryptanalysis, feedback, and reproducibility checks are explicitly encouraged.

</details>


### [74] [PenTiDef: Enhancing Privacy and Robustness in Decentralized Federated Intrusion Detection Systems against Poisoning Attacks](https://arxiv.org/abs/2602.17973)
*Phan The Duy,Nghi Hoang Khoa,Nguyen Tran Anh Quan,Luong Ha Tien,Ngo Duc Hoang Son,Van-Hau Pham*

Main category: cs.CR

TL;DR: 提出了针对DFL-IDS的防御框架PenTiDef，在隐私保护和抗毒攻击方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对传统的集中式FL-IDS的局限性，以及分布式FL-IDS（DFL-IDS）的独特挑战，提出了一种新的防御框架。

Method: 提出了一种名为PenTiDef的隐私保护和鲁棒防御框架，包括分布式差分隐私（DDP）和潜在空间表示（LSR）。

Result: 在CIC-IDS2018和Edge-IIoTSet上的实验结果表明，PenTiDef在各种攻击场景和数据分布中优于现有防御方法。

Conclusion: PenTiDef作为一种可扩展且安全的框架，有望在对抗环境中部署基于DFL的IDS，提供针对现实世界攻击的实用安全解决方案。

Abstract: The increasing deployment of Federated Learning (FL) in Intrusion Detection Systems (IDS) introduces new challenges related to data privacy, centralized coordination, and susceptibility to poisoning attacks. While significant research has focused on protecting traditional FL-IDS with centralized aggregation servers, there remains a notable gap in addressing the unique challenges of decentralized FL-IDS (DFL-IDS). This study aims to address the limitations of traditional centralized FL-IDS by proposing a novel defense framework tailored for the decentralized FL-IDS architecture, with a focus on privacy preservation and robustness against poisoning attacks. We propose PenTiDef, a privacy-preserving and robust defense framework for DFL-IDS, which incorporates Distributed Differential Privacy (DDP) to protect data confidentiality and utilizes latent space representations (LSR) derived from neural networks to detect malicious updates in the decentralized model aggregation context. To eliminate single points of failure and enhance trust without a centralized aggregation server, PenTiDef employs a blockchain-based decentralized coordination mechanism that manages model aggregation, tracks update history, and supports trust enforcement through smart contracts. Experimental results on CIC-IDS2018 and Edge-IIoTSet demonstrate that PenTiDef consistently outperforms existing defenses (e.g., FLARE, FedCC) across various attack scenarios and data distributions. These findings highlight the potential of PenTiDef as a scalable and secure framework for deploying DFL-based IDS in adversarial environments. By leveraging privacy protection, malicious behavior detection in hidden data, and working without a central server, it provides a useful security solution against real-world attacks from untrust participants.

</details>


### [75] [AndroWasm: an Empirical Study on Android Malware Obfuscation through WebAssembly](https://arxiv.org/abs/2602.18082)
*Diego Soi,Silvia Lucia Sanna,Lorenzo Pisu,Leonardo Regano,Giorgio Giacinto*

Main category: cs.CR

TL;DR: WebAssembly可用于隐藏Android恶意软件，绕过传统检测机制。


<details>
  <summary>Details</summary>
Motivation: 近年来，隐蔽的Android恶意软件采用复杂技术绕过自动检测机制和强化人工分析。

Method: 本研究调查了WebAssembly（Wasm）作为隐藏恶意负载和规避传统静态分析和签名匹配机制的新技术。

Result: 提供了关于Android可能采用的机制以将其包含在执行管道中的深入分析，并提供了概念验证以证明攻击者可以嵌入并执行恶意程序，有效地绕过工业级工具的IoC检测。

Conclusion: WebAssembly可以作为隐藏恶意负载和规避传统安全检测的新技术，对Android安全构成威胁。

Abstract: In recent years, stealthy Android malware has increasingly adopted sophisticated techniques to bypass automatic detection mechanisms and harden manual analysis. Adversaries typically rely on obfuscation, anti-repacking, steganography, poisoning, and evasion techniques to AI-based tools, and in-memory execution to conceal malicious functionality.
  In this paper, we investigate WebAssembly (Wasm) as a novel technique for hiding malicious payloads and evading traditional static analysis and signature-matching mechanisms. While Wasm is typically employed to render specific gaming activities and interact with the native components in web browsers, we provide an in-depth analysis on the mechanisms Android may employ to include Wasm modules in its execution pipeline. Additionally, we provide Proofs-of-Concept to demonstrate a threat model in which an attacker embeds and executes malicious routines, effectively bypassing IoC detection by industrial state-of-the-art tools, like VirusTotal and MobSF.

</details>


### [76] [Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security](https://arxiv.org/abs/2602.18270)
*Kevin Hermann,Sven Peldszus,Thorsten Berger*

Main category: cs.CR

TL;DR: 本文综述了静态安全分析，发现其存在局限性，需改进。


<details>
  <summary>Details</summary>
Motivation: 为了全面了解静态安全分析在软件漏洞检测中的应用，

Method: 进行文献综述，分析了246个静态安全分析器

Result: 发现大多数分析器专注于有限的弱点，检测到的漏洞很少可利用，且评估使用的基准测试太小，无法进行稳健评估。

Conclusion: 需要改进静态安全分析器的针对性和评估方法。

Abstract: Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.

</details>


### [77] [Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning](https://arxiv.org/abs/2602.18285)
*Said Varlioglu,Nelly Elsayed,Murat Ozer,Zag ElSayed,John M. Emmert*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.

</details>


### [78] [FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators](https://arxiv.org/abs/2602.18304)
*Darsh Asher,Farshad Dizani,Joshua Kalyanapu,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: 本文提出了一种硬件级后端检索数据窃取攻击，发现AI加速器中的零跳过可能导致数据泄露，并提出了一种基于填充的防御措施。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域如产品推荐、医疗和金融中，后端富集被广泛应用。模型在保密数据上训练，并检索影响推理行为的私有特征，同时从API调用者隐藏。

Method: 提出了一种名为FEATUREBLEED的硬件级后端检索数据窃取攻击，该攻击利用AI加速器中的零跳过，仅通过端到端时间推断私有后端检索特征，而不依赖于功耗分析、DVFS操作或共享缓存侧信道。

Result: 在医学和非医学领域的数据集上评估了FEATUREBLEED，并在三种硬件后端和三种模型架构上进行了评估，结果表明泄漏在CPU和GPU加速器、数据模态和应用领域之间普遍存在，具有高达98.87个百分点的对抗优势。

Conclusion: 确定了泄漏的根本原因是现代硬件中的稀疏驱动零跳过，并提出了一种基于填充的防御措施，通过将响应均等化到最坏情况执行时间来掩盖时间泄漏，实现了仅7.24%的平均性能开销和没有额外的功耗成本的保护。

Abstract: Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.
  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.
  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.

</details>


### [79] [Trojans in Artificial Intelligence (TrojAI) Final Report](https://arxiv.org/abs/2602.07152)
*Kristopher W. Reese,Taylor Kulp-McDowall,Michael Majurski,Tim Blattner,Derek Juba,Peter Bajcsy,Antonio Cardone,Philippe Dessauw,Alden Dima,Anthony J. Kearsley,Melinda Kleczynski,Joel Vasanth,Walid Keyrouz,Chace Ashcraft,Neil Fendley,Ted Staley,Trevor Stout,Josh Carney,Greg Canal,Will Redman,Aurora Schmidt,Cameron Hickert,William Paul,Jared Markowitz,Nathan Drenkow,David Shriver,Marissa Connor,Keltin Grimes,Marco Christiani,Hayden Moore,Jordan Widjaja,Kasimir Gabert,Uma Balakrishnan,Satyanadh Gundimada,John Jacobellis,Sandya Lakkur,Vitus Leung,Jon Roose,Casey Battaglino,Farinaz Koushanfar,Greg Fields,Xihe Gu,Yaman Jandali,Xinqiao Zhang,Akash Vartak,Tim Oates,Ben Erichson,Michael Mahoney,Rauf Izmailov,Xiangyu Zhang,Guangyu Shen,Siyuan Cheng,Shiqing Ma,XiaoFeng Wang,Haixu Tang,Di Tang,Xiaoyi Chen,Zihao Wang,Rui Zhu,Susmit Jha,Xiao Lin,Manoj Acharya,Wenchao Li,Chao Chen*

Main category: cs.CR

TL;DR: IARPA的TrojAI项目旨在应对AI木马威胁，揭示了威胁的复杂性质，并提出了检测和减轻木马风险的方法。


<details>
  <summary>Details</summary>
Motivation: 应对现代人工智能中新兴的漏洞：AI木马威胁

Method: 通过权重分析和触发反转进行检测，以及减轻已部署模型中木马风险的方法

Result: 揭示了威胁的复杂性质，开创了基础检测方法，并确定了需要人工智能安全领域持续关注的未解决挑战

Conclusion: 总结了项目的关键发现，包括检测方法，以及提高人工智能安全研究的建议

Abstract: The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of "natural" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.

</details>


### [80] [Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol](https://arxiv.org/abs/2602.18370)
*Benjamin Dowling,Prosanta Gope,Mehr U Nisa,Bhagya Wimalasiri*

Main category: cs.CR

TL;DR: 首次对LINEv2消息协议进行证明安全性分析，发现并解决了安全问题。


<details>
  <summary>Details</summary>
Motivation: 分析LINE消息协议的安全性，尤其是其密码学保障。

Method: 采用改进的多阶段密钥交换（MSKE）模型，对LINEv2消息协议进行证明安全性分析。

Result: 发现LINEv2缺乏前向机密性和后泄露安全性，并提出了改进的LINE协议。

Conclusion: 对LINEv2的密码学保障进行了深入分析，并提出了增强安全性建议。

Abstract: LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [81] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 提出了一种新的MCQA基准，显著提高了VLM在驾驶任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 提高视觉语言模型（VLM）在驾驶任务中的性能测量标准。

Method: 提出了一种新的MCQA基准，通过减少可利用的文本捷径来提高VLM的性能。

Result: 该方法将盲准确率从+66.9%降低到+2.9%，使模型更依赖于视觉理解。

Conclusion: 该方法能够提高VLM的性能，使其更准确地反映视觉理解。

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [82] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: POGPN-JPSS通过结合专家知识和结构化概率模型，在优化过程中实现了显著的性能提升和时间节约。


<details>
  <summary>Details</summary>
Motivation: 提高高维多阶段系统中BO的性能。

Method: 将POGPN与JPSS模型相结合，利用中间提取的信息。

Result: POGPN-JPSS在多阶段生物乙醇生产过程的模拟中表现出色，达到预期性能阈值所需时间减半，可靠性更高。

Conclusion: POGPN-JPSS显著优于现有方法，在快速优化过程中节省了时间和资源。

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [83] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge是一种结合PLM和LLM优点的蛋白质理解领域自适应持续预训练框架，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了结合蛋白质语言模型（PLM）和通用大型语言模型（LLM）的优点，同时解决它们在适应性和泛化方面的不足。

Method: 提出了一种名为BioBridge的领域自适应持续预训练框架，该框架使用领域增量持续预训练（DICP）将蛋白质领域知识和通用推理语料库注入LLM，并通过PLM-投影器-LLM管道实现跨模态对齐。

Result: 在多个蛋白质基准测试中，如EC和BindingDB，BioBridge的表现与主流PLM相当。在MMLU和RACE等通用理解任务上，它也达到了与LLM相当的结果。

Conclusion: BioBridge展示了将特定领域适应性结合到通用语言能力中的创新优势。

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [84] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: 提出了一种新的方法，用于提高LLMs后训练量化的鲁棒性，通过在MX量化下进行理论分析和实验验证，该方法显著提高了量化后的模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了降低大型语言模型的内存和计算成本，后训练量化（PTQ）被广泛应用。

Method: 提出了一种名为LATMiX的方法，该方法将异常值减少推广到可学习的可逆仿射变换，并使用标准的深度学习工具进行优化。

Result: 在多个模型大小和广泛的零样本基准测试中，实验表明，在MX低比特量化方面，平均准确率在强基线之上有显著提高。

Conclusion: 该方法在MX量化下提供了理论分析和实验证明，表明其在降低量化误差和提高模型准确率方面是有效的。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [85] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 提出了一种晴空条件下田间NDVI概率预测的新方法，该方法在多个评估指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了在精准农业中实现数据驱动的决策支持，需要准确预测植被动态。然而，由于云覆盖导致的稀疏和不规则采样，以及作物生长的异质气候条件，从卫星观测中预测归一化植被指数（NDVI）仍然具有挑战性。

Method: 提出了一种专门为在晴空获取约束下进行田间NDVI预测的概率预测框架。该方法利用基于transformer的架构，明确地将历史植被动态建模与未来外生信息建模分离，结合历史NDVI观测值以及历史和未来的气象协变量。为了解决不规则重访模式和预测范围相关的不确定性，引入了时间距离加权分位数损失，使训练目标与有效预测范围相一致。此外，还结合累积和极端天气特征工程，更好地捕捉与植被响应相关的延迟气象效应。

Result: 在欧盟卫星数据上的大量实验表明，所提出的方法在点预测和概率评估指标上均优于各种统计、深度学习和最近的时间序列基线。消融研究进一步强调了目标历史的核心作用，同时表明当联合利用时，气象协变量提供了互补的收益。

Conclusion: 该方法在晴空条件下对田间NDVI的预测方面表现出色，为精准农业的数据驱动决策支持提供了有效的工具。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [86] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler模型通过语法感知和有效性保持技术，有效提升了代码生成和推理的强化学习可扩展性


<details>
  <summary>Details</summary>
Motivation: 提升代码大型语言模型的可扩展性

Method: 提出CodeScaler模型，基于语法感知的代码提取和有效性保持的奖励塑造

Result: 在五个编码基准上平均提升Qwen3-8B-Base 11.72分，性能优于基于二进制执行的强化学习，并使无测试用例的合成数据集上的强化学习可扩展

Conclusion: CodeScaler模型在代码生成和推理方面表现优异，有效提高了强化学习的可扩展性

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [87] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 提出了一种新的ADR规划框架，实验表明掩码PPO在任务效率和计算性能方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 应对低地球轨道上多目标主动碎片清除（ADR）的挑战

Method: 提出了一种统一的共椭圆机动框架，结合霍曼转移、安全椭圆邻近操作和明确的加注逻辑；在具有随机碎片场、禁飞区和推力限制的逼真轨道模拟环境中，对三种不同的规划算法（贪婪启发式、蒙特卡洛树搜索（MCTS）和深度强化学习（RL）使用掩码近端策略优化（PPO））进行了基准测试。

Result: 实验结果表明，掩码PPO在任务效率和计算性能方面优于贪婪启发式和MCTS，能够访问多达两倍的碎片，并在运行时间上显著优于MCTS。

Conclusion: 这些发现强调了现代强化学习方法在可扩展、安全和资源高效的空间任务规划方面的潜力，为未来ADR自主性的进步铺平了道路。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [88] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree框架通过结构锚定扩散，提高了代码生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法未能尊重编程语言的严格结构，导致生成的程序经常无法执行的问题。

Method: 引入了AnchorTree框架，该框架使用结构化、层次化的先验知识来锚定扩散过程，具体使用抽象语法树来优先解决语法和语义上显著的标记，如关键词和标识符，从而建立结构支架以指导剩余的生成过程。

Result: 通过AnCoder模型系列验证了该框架，表明结构锚定的扩散是高效生成高质量代码的途径。

Conclusion: AnchorTree框架能够有效地提高代码生成的质量，为编程语言生成提供了一种新的思路。

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [89] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 提出了Robust-MMR，一种增强医学视觉-语言模型鲁棒性的预训练方法，显著提高了其在多个基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型在医学图像和临床文本的联合推理方面显示出巨大潜力，但它们的性能往往在受成像设备、采集协议和报告风格变化导致的领域迁移下下降。现有的多模态预训练方法主要忽视了鲁棒性，将其视为下游适应问题。

Method: 提出了鲁棒多模态掩码重建（Robust-MMR），这是一种将鲁棒性目标明确纳入掩码视觉-语言学习的自监督预训练框架。Robust-MMR集成了非对称扰动感知掩码、领域一致性正则化和模态鲁棒性约束，以鼓励领域不变表示。

Result: 在多个医学视觉-语言基准上评估了Robust-MMR，包括医学视觉问答（VQA-RAD、SLAKE、VQA-2019）、跨领域图像-文本分类（MELINDA）和鲁棒图像-文本检索（ROCO）。在VQA-RAD上，Robust-MMR实现了78.9%的跨领域准确率，比最强的基线高出3.8个百分点，在SLAKE和VQA-2019上分别达到74.6%和77.0%的准确率。在受扰动的评估下，Robust-MMR将VQA-RAD的准确率从69.1%提高到75.6%。对于图像-文本分类，跨领域的MELINDA准确率从70.3%提高到75.2%，而检索实验表明，在扰动下，平均排名下降从超过16降至4.1。定性的结果进一步证明了在疾病检测和结构异常评估方面的临床推理改进。

Conclusion: 在预训练期间明确建模鲁棒性导致更可靠和可迁移的医学视觉-语言表示，适用于实际部署。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [90] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 提出了一种分析分布偏移下风险的统一框架，并通过显式不等式和可识别性条件提高了模型的可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究在分布偏移的情况下预测器的风险和可验证性。

Method: 提出一个统一的框架来分析分布偏移下的风险，并使用可验证的规则性和复杂性约束。

Result: 得到一个关于分布偏移下风险的上界，该上界由可计算的偏移度量和模型参数确定。

Conclusion: 通过显式不等式验证风险，确保学习模型的可信度，并通过可识别性条件强制执行可解释性。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [91] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 通过引入分布鲁棒元学习框架，分析了对抗性条件下ICL的极限，表明模型容量对分布鲁棒性至关重要。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前对大语言模型适应新任务能力的理论解释，考虑到了对抗性分布偏移对实际可靠性的威胁。

Method: 提出了一种基于Wasserstein分布偏移的分布鲁棒元学习框架，推导了关于对抗性扰动强度、模型容量和上下文示例数量的非渐近界限。

Result: 分析表明，模型鲁棒性与其容量的平方根成正比，而对抗性设置对样本复杂度产生了与扰动幅度平方成正比的惩罚。实验验证了这些比例定律。

Conclusion: 这些发现推进了对对抗条件下ICL极限的理论理解，并表明模型容量是分布鲁棒性的基本资源。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [92] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 提出了一种新的联合参数和记忆路径的未学习框架SBU，有效解决了现有方法的不足，并在医学QA基准上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法仅针对参数进行删除，忽略了参数和持久记忆之间的反馈，以及缺乏统一策略覆盖参数和记忆路径的问题。

Method: 提出同步回溯未学习（SBU）框架，通过参数和记忆路径联合未学习。记忆路径采用基于依赖闭包的未学习，剪枝孤立实体并逻辑上无效化共享工件。参数路径采用随机参考对齐，引导模型输出向高熵先验发展。这些路径通过同步双更新协议集成，形成一个闭环机制，其中记忆未学习和参数抑制相互强化，以防止跨路径重新污染。

Result: 在医学QA基准上的实验表明，SBU在保留数据上有限退化的同时，降低了跨路径的目标私人信息痕迹。

Conclusion: SBU框架有效解决了参数和记忆路径的联合未学习问题，在医学QA基准上取得了良好的效果。

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [93] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 评估了PTQ算法在Ascend NPU上的效果，发现平台敏感性显著，8位量化更稳定，INT8部署优化内核降低延迟，但动态量化开销限制加速。


<details>
  <summary>Details</summary>
Motivation: 为了高效部署模型，PTQ至关重要，但其对Ascend NPU的有效性尚不明确。

Method: 对DeepSeek-R1-Distill-Qwen系列（1.5B/7B/14B）和QwQ-32B等推理模型应用代表性的PTQ基线，评估AWQ、GPTQ、SmoothQuant和FlatQuant四种算法。

Result: 发现平台敏感性显著，4位权重压缩对大型模型可行，但4位权重-激活方案在NPU上不稳定，导致逻辑崩溃。8位量化保持数值稳定性。INT8部署显示优化内核降低延迟，但动态量化开销限制端到端加速。

Conclusion: 为在Ascend NPU上部署量化推理模型提供实用参考。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [94] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 本文针对大规模数据问题，在机器学习领域开展了三个创新性项目研究，取得显著成果。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习在处理大规模数据方面的应用，本文研究了三个基于数学理论的项目。

Method: 第一项研究监督学习和流形学习；第二项研究迁移学习；第三项研究机器学习中的分类任务，特别是在主动学习范式下。

Result: 第一项研究提出了一种旨在弥补当前监督学习范式理论缺陷的方法；第二项研究探讨了在数据仅部分可知的域中函数提升的问题；第三项研究提出了将信号分离技术与分类相结合的理论和方法，并设计了一种新的算法。

Conclusion: 本文在机器学习领域取得了创新性的研究成果，为解决大规模数据问题提供了新的思路和方法。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [95] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT：一种基于可解释属性的解码时间个性化方法，显著提高个性化生成质量。


<details>
  <summary>Details</summary>
Motivation: 对现有个性化对齐方法的不足进行分析，提出新的解码时间个性化方法EXACT。

Method: 使用预定义的可解释属性集，通过最大化偏好响应的可能性来识别用户特定的属性子集，并在在线推理中检索最相关的属性以引导生成。

Result: 理论保证算法的近似性，并通过实验证明EXACT在偏好建模准确性和个性化生成质量方面优于强基线。

Conclusion: EXACT是一种有效的解码时间个性化方法，可以显著提高个性化生成质量。

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [96] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [97] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: LLMs安全区域识别技术存在局限性，无法稳定识别安全区域。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的安全区域，即参数子集的修改直接影响安全行为。

Method: 系统评估了四种不同参数粒度的安全区域识别方法，从单个权重到整个Transformer层，涵盖四种不同大小的LLM主干。

Result: 发现识别出的安全区域重叠程度低到中等，使用IoU衡量。当使用效用数据集（即无害查询）进一步细化安全区域时，重叠程度显著下降。

Conclusion: 当前技术无法可靠地识别稳定、数据集无关的安全区域。

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [98] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [99] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [100] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [101] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [102] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: Grassmannian MoE（GrMoE）是一种新的路由框架，通过控制稀疏性和利用率之间的权衡，提高了Mixture-of-Experts模型的表现。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts模型依赖于学习到的路由器将标记分配给专家，但标准的softmax门控没有提供控制稀疏性和利用率之间权衡的原理机制。

Method: 提出Grassmannian MoE（GrMoE），一个在子空间Grassmann流形上操作的路由框架，其中门控权重来自矩阵Bingham分布的集中参数。

Result: GrMoE在合成路由任务上实现了0%的路由崩溃，与15-30%的负载平衡改进，以及浓度和有效稀疏性之间的平滑单调关系，使得可以在不重新训练的情况下进行后处理稀疏性调整。

Conclusion: GrMoE建立了集中控制的稀疏性的第一个形式化理论，并通过专家学习异构的集中值，这些值与语言专业化相关，从而提供可解释的路由行为。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [103] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: MantisV2 and Mantis+ improve zero-shot feature extraction for time series classification


<details>
  <summary>Details</summary>
Motivation: Developing foundation models for time series classification is of high practical relevance

Method: Introducing Mantis+, refining architecture, proposing enhanced test-time methodology, self-ensembling and cross-model embedding fusion

Result: MantisV2 and Mantis+ achieve state-of-the-art zero-shot performance on UCR, UEA, HAR benchmarks, and EEG datasets

Conclusion: MantisV2 and Mantis+ significantly strengthen zero-shot feature extraction for time series

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [104] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: SBA is a Bayesian PEFT framework that improves the performance and reliability of language models by providing principled uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Parameter-efficient fine-tuning methods like LoRA enable practical adaptation of large language models but lack principled uncertainty estimates, causing poor calibration and unreliable behavior under domain shift.

Method: Introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction.

Result: SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34% over deterministic baselines, improving selective prediction AUROC by 12 to 25% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost.

Conclusion: The placement of uncertainty on the right geometric structure matters more than simply adding Bayesian treatment to adapters.

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [105] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: ruleXplain：利用LLMs从模拟驱动动力学系统中提取可解释的因果规则


<details>
  <summary>Details</summary>
Motivation: 在具有延迟效应的时间序列数据中推断因果关系是一个基本挑战，特别是当底层系统表现出无法通过简单函数映射捕捉的复杂动力学时。

Method: 提出ruleXplain框架，利用大型语言模型（LLMs）提取模拟驱动动力学系统中的输入-输出关系的正式解释。该方法引入了一种受约束的符号规则语言，具有时间运算符和延迟语义，使LLMs能够通过结构化提示生成可验证的因果规则。

Result: 使用PySIRTEM流行病模拟器和EnergyPlus建筑能源模拟器验证了该框架。通过输入重建、消融研究和跨未见输出趋势的规则泛化测试来评估规则集的有效性。

Conclusion: ruleXplain框架能够有效地从模拟驱动动力学系统中提取可解释的因果规则，为复杂时间序列数据的因果关系推断提供了一种新的方法。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [106] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 研究评估了机器学习模型在预测慢性鼻炎手术受益方面的潜力，结果表明该模型具有高准确率，可以改善临床决策。


<details>
  <summary>Details</summary>
Motivation: 探讨利用机器学习预测慢性鼻炎手术受益的可能性，以降低成本并改善患者预后。

Method: 使用监督机器学习模型，基于前瞻性收集的观察性干预试验标准化数据集，评估手术受益的预测。

Result: 最佳模型在多个算法中达到约85%的分类准确率，在30个难度不一的案例中达到80%的准确率，超过专家临床医生的平均预测准确率（75.6%）。

Conclusion: 机器学习模型在预测慢性鼻炎手术受益方面具有潜力，可以增强临床决策并支持个性化护理。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [107] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: MePoly是一种新的策略参数化方法，用于解决多模态决策问题，在理论和实证研究中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统策略在表示解的多模态方面的不足和扩散策略缺乏显式概率密度的问题。

Method: 提出了一种基于多项式能量模型的策略参数化方法MePoly，利用经典矩问题，并证明其可逼近任意分布。

Result: MePoly在捕捉复杂非凸流形方面表现良好，在多个基准测试中优于基线模型。

Conclusion: MePoly为解决多模态决策问题提供了一种新的方法，在理论和实证方面均取得了较好的效果。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [108] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [109] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个针对差异隐私机器学习的库，提供模块化组件和最新研究。


<details>
  <summary>Details</summary>
Motivation: 简化差异隐私机器学习的部署

Method: 设计原则包括可用性、灵活性和效率

Result: 提供了验证的、模块化的基本组件，包括批量选择、梯度裁剪、噪声添加、会计和审计，并汇集了关于差异隐私机器学习的最新研究

Conclusion: JAX-Privacy是一个旨在简化差异隐私机器学习部署的库，适用于需要深度定制的研究人员和寻求现成解决方案的从业者。

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [110] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种新的LLM特征可视化方法，在多个层和潜在类型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习到的方向所编码的特征需要识别出能够强烈激活这些方向的数据输入。特征可视化通过优化输入以最大程度地激活目标方向，为代价高昂的数据集搜索方法提供了一种替代方案，但由于文本的离散性，对于LLM来说，这一方法仍处于探索阶段。此外，现有的提示优化技术不适合这个领域，因为它很容易陷入局部最优解。

Method: 我们引入了ADAPT，这是一种结合了波束搜索初始化和自适应梯度引导变异的混合方法，旨在解决这些局限性。在Gemma 2 2B的稀疏自编码器潜在空间上进行了评估，提出了基于数据集激活统计的指标，以实现严格的比较。

Result: ADAPT在各个层和潜在类型上均优于先前的方法。结果表明，LLM的特征可视化是可行的，但需要针对领域设计假设。

Conclusion: LLM的特征可视化是可行的，但需要针对领域设计假设。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [111] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出使用GR提高RLHF中的奖励模型准确性，实验证明其效果优于KL惩罚。


<details>
  <summary>Details</summary>
Motivation: 解决奖励黑客问题，即策略可能利用奖励的不准确性学习到非预期行为。

Method: 通过理论推导和实验验证，提出使用梯度正则化（GR）来使策略更新偏向于奖励更准确的位置区域。

Result: GR在RLHF中表现优于KL惩罚，在规则性数学奖励和LLM-as-a-Judge数学任务中避免了对格式或评委的过度关注，提高了GPT-judged win-rate。

Conclusion: 梯度正则化（GR）在RLHF中能够有效提高奖励模型准确性，防止策略利用奖励不准确性学习到非预期行为。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [112] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法，在解决实际决策问题时具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 强化学习和深度学习对决策模式的革命性影响，但实际应用中存在挑战，尤其是高收益应用中的决策策略学习。

Method: 使用工具变量（IVs）识别因果关系，结合双/偏差机器学习，提出了一种高效的算法解决条件矩限制（CMR）问题，并改进了现有的算法。

Result: 该方法在强化学习基准测试和合成/半合成数据集上证明了其在实际决策中的有效性。

Conclusion: 本文提出的方法在解决实际决策问题中具有显著优势，为强化学习在实际应用中的推广提供了新的思路。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [113] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 提出了一种基于信息论分析的CoT监控系统，通过针对性的训练目标提高了监控精度，并提出了两种互补方法。


<details>
  <summary>Details</summary>
Motivation: 为了检测代码生成中的测试黑客行为等属性，使用基于LLM的Chain-of-thought（CoT）监控系统分析推理轨迹。

Method: 利用信息论分析，识别了可能导致CoT监控系统性能下降的两个近似误差来源：信息差距和启发误差。通过针对性的训练目标系统地提高CoT的可监控性。提出两种互补方法：（a）基于oracle的方法，直接奖励监控模型产生最大化监控精度的CoT；（b）一种更实用的、无标签的方法，最大化输出和CoT之间的条件互信息。

Result: 在多个不同环境中，两种方法都显著提高了监控精度，防止了CoT退化，甚至在对抗监控时也能减轻奖励黑客行为。

Conclusion: CoT监控性可以通过针对性的训练目标系统地提高，并提出两种互补方法来提高监控精度。

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [114] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力回归模型ECA，通过理论分析和改进，实现了PCC的显著提升。


<details>
  <summary>Details</summary>
Motivation: 针对注意力回归模型训练过程中PCC（皮尔逊相关系数）提升停滞的问题，本文提出了一种新的方法来提高PCC优化并突破凸包限制。

Method: 通过理论分析PCC提升停滞现象，揭示了优化动态和模型容量的基本限制，并提出了Extrapolative Correlation Attention（ECA）模型。

Result: ECA模型在多个基准测试中，包括具有挑战性的同质数据设置中，均能打破PCC提升停滞，实现PCC的显著提升而不影响MSE性能。

Conclusion: ECA模型通过引入新的理论动机机制，有效提高了PCC优化，突破了凸包限制，实现了PCC的显著提升。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [115] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 研究LLMs中原型标记的编码和表现，为非自回归seq2seq系统提供支持。


<details>
  <summary>Details</summary>
Motivation: 研究自回归大语言模型（LLMs）中原型标记的编码信息和行为，以及它们在重建和受控约束下的表现。

Method: 进行了一系列实验，旨在分离两个原型标记中的语义和句法内容，分析e-token的稳定性属性，并可视化重建过程中对e-token的注意力模式。测试了两种正则化方案，包括基于锚点的损失和关系蒸馏目标，以在e-token上“施加”语义结构。

Result: 结果表明，m-token在标准优化下比e-token更能捕捉语义信息；基于锚点的约束与重建精度之间存在尖锐的权衡；关系蒸馏可以在不牺牲重建质量的情况下将批级语义关系转移到原型标记空间中，支持未来非自回归seq2seq系统预测原型标记作为中间表示的可行性。

Conclusion: 该研究为非自回归seq2seq系统预测原型标记提供了理论支持和实验依据。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [116] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 提出了一种新的算法AbstainBoost，在无分布的弃权学习中实现了子线性错误保证，并揭示了错误分类误差与错误弃权次数之间的多项式权衡。


<details>
  <summary>Details</summary>
Motivation: 研究在先验分布未知的情况下，如何在没有惩罚的半对抗性环境中进行学习。

Method: 提出了一种基于弱学习者的提升过程的算法AbstainBoost，用于在无分布的弃权学习中保证子线性错误。

Result: 该算法对于无知的对手保证了子线性错误，并且对于自适应对手和结构化函数类（包括线性分类器）也有类似的保证。

Conclusion: 该研究在无分布的弃权学习中提供了新的算法和理论保证，并揭示了错误分类误差与错误弃权次数之间的有趣的多项式权衡。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [117] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST是一种用于结构化时间动态的Port-Hamiltonian架构，在长期预测和物理参数恢复方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 在科学机器学习中，从部分观察中预测耗散物理系统的动力学是一个核心挑战。

Method: 提出了一种名为PHAST（Port-Hamiltonian Architecture for Structured Temporal dynamics）的方法，该方法将哈密顿量分解为势能、质量和阻尼，并在三个知识范围内使用低秩PSD/SPD参数化。

Result: 在机械、电气、分子、热、重力和生态系统等十三个仅位置（q-only）基准测试中，PHAST在长期预测方面优于竞争基准，并能够在提供足够锚点的状态下实现物理参数的恢复。

Conclusion: PHAST在长期预测和物理参数恢复方面表现出色，但在没有锚点的情况下，识别问题是基本不定的。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [118] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformers struggle with state tracking, needing more data and less effective weight sharing than RNNs.


<details>
  <summary>Details</summary>
Motivation: Recent work has raised concerns about the ability of transformer-based language models to perform state tracking, particularly in out-of-distribution generalization.

Method: Large-scale experimental study of data efficiency of transformers and RNNs across multiple supervision regimes.

Result: Transformers require more training data than RNNs, exhibit negligible or detrimental weight sharing across lengths, while RNNs exhibit effective amortized learning by sharing weights across lengths.

Conclusion: State tracking remains a fundamental challenge for transformers even when training and evaluation distributions match.

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [119] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 提出了一种新的序列模型框架，用于分析U(d)闭子群上的隐藏状态，并取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 提出一个直接框架，用于对U(d)的闭子群上的具有隐藏状态的序列模型进行分析。

Method: 使用最小化的公理化设置，从共享骨架中推导出循环和转换模板，其中子群选择作为状态空间、切向投影和更新映射的替代品。

Result: 针对O(d)进行专门化，在Tiny Shakespeare和Penn Treebank上对正交状态RNN和转换器模型进行评估。还报告了一种在切向空间中的通用线性混合扩展，它适用于所有子群选择，并提高了当前O(d)实验中的有限预算性能。

Conclusion: 该方法有效提高了序列模型在U(d)闭子群上的性能。

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [120] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 研究了GP乐队问题的下界，解决了维度相关的对数因子之间的差距，并提供了最大信息增益的改进上界。


<details>
  <summary>Details</summary>
Motivation: 研究高斯过程（GP）乐队问题在频率主义设置中的算法无关、最坏情况下的下界，其中奖励函数是固定的并且具有有界范数在已知的再生核希尔伯特空间（RKHS）中。

Method: 关注平方指数（SE）核，这是GP乐队中最广泛使用的核函数之一。通过在超球面输入域下部分解决开放问题，研究了维度相关的对数因子之间的差距。

Result: 证明了任何算法都受到Ω(√T( lnT)^{d}( lnlnT)^{-d})累积遗憾的影响。还提供了SE核的最大信息增益的改进上界O((lnT)^{d+1}(lnlnT)^{-d})。

Conclusion: 结果保证了在超球面输入域下，现有最佳算法的优化性，直到维度无关的对数因子。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [121] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 提出了一种新的HPO方法，通过降低方差来提高超梯度估计的准确性，并在多个任务上证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对现有梯度超参数优化（HPO）方法在降低估计与真实值之间的差距（即偏差）方面存在不足，同时忽略了数据分布误差（即方差），导致性能下降。

Method: 进行超梯度估计误差的偏差-方差分解，提供对先前工作中忽略的方差项的详细分析，并对超梯度估计的误差界限进行综合分析。提出了一种集成超梯度策略来有效降低HPO算法中的方差。

Result: 实验结果表明，所提出的方差减少策略提高了超梯度估计的准确性，并解释了性能提升的原因。

Conclusion: 提出了一种新的HPO方法，通过降低方差来提高超梯度估计的准确性，并在多个任务上证明了其有效性。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [122] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: This paper resolves the paradox in noise-agnostic generative models, showing stability and identifying key issues in noise-prediction.


<details>
  <summary>Details</summary>
Motivation: Autonomous (noise-agnostic) generative models challenge the standard paradigm by learning a single, time-invariant vector field without explicit noise-level conditioning.

Method: Formalizing Marginal Energy, proving that generation using autonomous models is a specific form of Riemannian gradient flow on this Marginal Energy, and establishing structural stability conditions for sampling.

Result: Resolving the paradox of noise-agnostic networks, demonstrating stability near the data manifold, and identifying the 'Jensen Gap' in noise-prediction parameterizations.

Conclusion: The study provides a theoretical foundation for understanding and improving autonomous generative models, particularly in handling noise and ensuring stability.

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [123] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [124] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [125] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [126] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [127] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 本文提出了一种计算模型来分析学校拥挤问题，发现地理邻近性和学位容量是主要限制因素。


<details>
  <summary>Details</summary>
Motivation: 分析学校拥挤问题及其对教育公平的影响，以及菲律宾教育服务合同计划在缓解学校拥挤方面的不足。

Method: 引入计算框架来模拟学生流动模式和模拟政策情景，使用随机重力模型和负二项回归分析来估计行为弹性。

Result: 发现地理邻近性对学校选择的影响比学费高四倍，并且学位容量是限制因素，而非补贴金额。

Conclusion: 补贴计划不能单独解决系统性的拥挤问题，计算模型可以帮助教育政策制定者做出公平、数据驱动的决策。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [128] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO通过增强探索和平衡探索与利用，提高了离线到在线RL的样本效率，并在多个任务中实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习（RL）中，生成模型最近在各个领域取得了显著的成功，促使它们被采用为表达策略。

Method: 提出了一种名为FINO的新方法，该方法利用基于流匹配的策略来提高离线到在线RL的样本效率。FINO通过在策略训练中注入噪声，促进对更广泛动作的有效探索。此外，我们结合了一种熵引导的采样机制，以平衡探索和利用，使策略能够在在线微调过程中调整其行为。

Result: 在多个具有挑战性的任务上的实验表明，FINO在有限的在线预算下，始终实现了优异的性能。

Conclusion: FINO为离线到在线RL提供了一种有效的方法，通过增强探索和平衡探索与利用，提高了样本效率并实现了更好的性能。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [129] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: Develops optimization methods to generate adversarial input points for AC power flow neural network models, contributing to rigorous verification and robust training methods.


<details>
  <summary>Details</summary>
Motivation: This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

Method: formulates and solves optimization problems to generate input points that yield high errors

Result: Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. The constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus.

Conclusion: This work contributes to the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [130] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出了一种新的框架来测量AI倾向性，并证明将倾向性和能力相结合可以提高预测能力。


<details>
  <summary>Details</summary>
Motivation: 评估人工智能模型的能力和倾向性对于确定性能和安全结果至关重要。然而，传统的IRT方法不适用于描述倾向性，因此需要新的方法。

Method: 提出了一种新的框架来测量AI倾向性，该框架使用双逻辑公式来衡量模型的成功，并使用任务无关的评估标准来估计“理想带”的极限。

Result: 通过将倾向性和能力相结合，可以获得比单独使用能力评价更强的预测能力。

Conclusion: 该框架展示了如何进行严格的倾向性测量，并表明仅使用能力评价来预测AI行为的方法是有局限性的。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [131] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 通过FlyGM模型，将果蝇大脑连接图转化为有效的运动控制策略，实现不同运动任务中的稳定控制。


<details>
  <summary>Details</summary>
Motivation: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored.

Method: Develop Fly-connectomic Graph Model (FlyGM), representing the static connectome as a directed message-passing graph, and integrating with a biomechanical fruit fly model.

Result: Achieves stable control across diverse locomotion tasks without task-specific architectural tuning. FlyGM yields higher sample efficiency and superior performance compared to other models.

Conclusion: Static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [132] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: 提出LERD，一种基于贝叶斯神经动力学系统的阿尔茨海默病脑电图诊断新方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法依赖黑盒分类器且未显式建模生成观察信号的基本动态的局限性。

Method: 提出LERD，一个端到端的贝叶斯电生理神经动力学系统，直接从多通道脑电图推断潜在神经事件及其关系结构，无需事件或交互注释。结合连续时间事件推断模块和随机事件生成过程，以捕获灵活的时间模式，同时引入电生理启发的动力学先验，以有原则地引导学习。

Result: 在合成基准和两个真实世界的AD脑电图队列上的广泛实验表明，LERD在性能上优于强大的基线，并产生与生理学一致的潜在摘要，有助于表征群体级别的动态差异。

Conclusion: LERD是一个有效的工具，可以提高基于脑电图诊断阿尔茨海默病的准确性。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [133] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [134] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 本研究提出了一种处理重尾梯度噪声的异步优化新方案，提高了Transformer模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中常见的重尾随机梯度噪声会导致优化过程不稳定。

Method: 提出基于延迟感知学习率调度和延迟补偿的算法修改，以处理具有重尾梯度噪声的异步更新中的落后者。

Result: 在重尾噪声下的收敛保证与同步对应物的速率相匹配，并且与现有的异步方法相比，提高了延迟容忍度。在图像和语言任务中，我们的方法在准确度/运行时间权衡方面优于先前的同步和异步方法，并且对超参数更加鲁棒。

Conclusion: 本研究通过提出新的异步优化方案，有效解决了重尾梯度噪声对Transformer模型优化过程的不稳定影响，提高了模型的性能和鲁棒性。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [135] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文提出了一种针对Dual-to-Dual MLLMs的持续学习框架和MAGE方法，显著提高了其持续学习能力。


<details>
  <summary>Details</summary>
Motivation: Dual-to-Dual MLLMs在终身进化方面存在缺陷，显著影响了持续适应动态真实场景的能力。

Method: 提出Continual-NExT，一个针对Dual-to-Dual MLLMs的持续学习框架，并引入MAGE方法。

Result: MAGE方法优于其他持续学习方法，并实现了最先进的性能。

Conclusion: Continual-NExT框架和MAGE方法能够有效提高Dual-to-Dual MLLMs的持续学习能力。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [136] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [137] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [138] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [139] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet是一种基于Transformer的强化学习调度程序，提高了实时任务的截止日期满足率和优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 实时调度程序需要在严格的计算预算下对紧密的截止日期进行推理。

Method: 提出TempoNet，一个结合了排列不变的Transformer和深度Q近似的强化学习调度程序。使用Urgency Tokenizer将时间松弛离散化为可学习的嵌入。采用延迟感知的稀疏注意力堆栈，具有分块top-k选择和局部敏感块划分，以实现全局推理，具有近线性扩展和亚毫秒推理。多核映射层通过掩码贪婪选择或可微分匹配将上下文化的Q分数转换为处理器分配。

Result: 在工业混合关键性跟踪和大型多处理器设置上进行了大量评估，结果表明，与解析调度程序和神经基线相比，TempoNet在满足截止日期方面表现出一致的改进，并且优化稳定性得到提高。

Conclusion: TempoNet为基于Transformer的高吞吐量实时调度中的决策提供了实用的框架。

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [140] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [141] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: FedZMG通过结构化正则化优化空间，有效应对客户端漂移，提高了联邦学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决非独立同分布数据导致的客户端漂移问题，提高联邦学习模型的收敛速度和性能。

Method: 提出了一种名为FedZMG的客户端优化算法，通过结构化正则化优化空间来应对客户端漂移。

Result: FedZMG在EMNIST、CIFAR100和Shakespeare数据集上实现了比FedAvg和FedAdam更好的收敛速度和最终验证精度。

Conclusion: FedZMG是一种有效的联邦学习优化算法，能够提高模型在非独立同分布数据集上的性能。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [142] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 提出了一种新的图拉普拉斯算子，提高了GNN在长距离信息传播中的性能。


<details>
  <summary>Details</summary>
Motivation: GNNs在传播信息时由于过度平滑和压缩而难以跨越长距离。

Method: 提出了一种Bakry-Emery图拉普拉斯算子，通过可学习的节点势整合扩散和推进，在不改变拓扑结构的情况下诱导任务相关的传播动态。

Result: 开发了mu-ChebNet，这是一种联合学习势和切比雪夫滤波器的谱架构，在合成长距离推理任务和现实世界基准测试中表现出色。

Conclusion: Bakry-Emery拉普拉斯算子为自适应谱图学习提供了一个原则性和高效的框架。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [143] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出了一种统一框架来分析机器学习模型预测的解释，揭示了全局和局部解释性设置之间的差异，并提出了一系列新的计算方法。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习模型预测的解释性，特别是充分原因和对比原因的计算复杂性。

Method: 引入统一框架分析这些解释，并证明它们可以通过最小化统一概率值函数来表征。

Result: 发现了一些关于这些属性在解释设置中的反直觉结果，并提出了一系列在全局可解释性设置下计算各种解释的新的多项式时间结果。

Conclusion: 证明了计算这些解释的复杂性受价值函数的三个关键属性（单调性、次可加性和超可加性）的影响，并揭示了全局和局部解释性设置之间的差异。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [144] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+通过密集预训练和灵活的推理时间切换，在保持高精度的同时提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高推理时的效率，研究者提出了结构化扩张注意力机制。然而，这种机制在稀疏化预训练注意力模型时会导致精度下降。

Method: 研究者提出了RAT+，一种密集预训练架构，通过添加全序列递归和主动递归学习来增强注意力。RAT+模型在推理时可以灵活地切换到扩张注意力或混合层/头部组合，只需进行短暂的1B标记分辨率调整，而不是重新训练稀疏模型。

Result: RAT+在16B参数和100B标记的训练下，在常识推理和LongBench任务上与密集模型精度相近，在64B参数和200B标记的训练下，精度略有下降。RAT+在稀疏化到top-k块注意力时优于注意力机制。

Conclusion: RAT+通过密集预训练和灵活的推理时间切换，在保持高精度的同时提高了推理效率。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [145] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出了一种新的计算单元——可变分布神经元，用于处理序列生成中的不确定性和因果关系。


<details>
  <summary>Details</summary>
Motivation: 针对序列生成中因果关系的组织方式以及不确定性在计算单元中的表示问题，提出了一种新的计算单元——可变分布神经元。

Method: 提出了一种基于变分自编码器（VAE）砖块的计算单元，该单元具有先验、归一化后验和局部ELBO，并通过KL项进行正则化。

Result: 该计算单元能够表示分布，并通过局部约束进行测试和监控。

Conclusion: 该研究提出了一个新颖的计算单元，能够更好地处理序列生成中的不确定性和因果关系。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [146] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 本文展示了迁移学习和跨任务解码在MEG-based语音模型中的应用，提高了解码性能，并允许跨任务解码。


<details>
  <summary>Details</summary>
Motivation: 数据高效的神经解码是语音脑机接口的核心挑战。

Method: 使用迁移学习和跨任务解码，在MEG-based语音模型中进行预训练和微调。

Result: 迁移学习提高了任务内和跨任务的准确性，并且模型能够在感知和产生任务之间进行可靠的跨任务解码。

Conclusion: 预训练不仅提高了每个任务的性能，还允许感知和产生任务之间的可靠跨任务解码。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [147] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 本研究通过审查现有方法，应用可解释性技术，为提高无监督学习自动化的决策透明度提供了实用基础。


<details>
  <summary>Details</summary>
Motivation: 提高无监督学习自动化的决策透明度

Method: 对现有方法进行审查，构建元特征结构分类，应用全局可解释性技术和局部可解释性工具

Result: 揭示了元特征的相关性模式，识别了当前元学习策略中的结构弱点，为更可解释的AutoML设计提供了指导

Conclusion: 为无监督学习自动化提供了提高决策透明度的实用基础

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [148] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出了一种结合物理和数据的混合模型，用于优化船舶性能和燃油效率


<details>
  <summary>Details</summary>
Motivation: 优化船舶性能、提高燃油效率和符合排放法规

Method: 结合物理知识和数据驱动残差学习，建立混合模型框架

Result: 混合模型在稀疏数据区域优于纯数据驱动模型，在密集数据区域表现相似

Conclusion: 混合模型是船舶性能监控的实用和计算效率工具

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [149] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE通过评估每个点的置信度来提高聚类质量。


<details>
  <summary>Details</summary>
Motivation: 聚类在无监督结构发现中被广泛使用，但其对每个个体分配的可靠性了解有限。

Method: 引入CAKE（通过K分区集成进行分配置信度评估），该框架通过聚类集成计算两个互补统计数据：分配稳定性和局部几何拟合一致性。

Result: CAKE在噪声下仍然有效，并能区分稳定和不稳定的点。在合成和真实世界数据集上的实验表明，CAKE有效地突出了模糊点和稳定的中心成员，提供了一个置信度排名，可以引导过滤或优先处理以提高聚类质量。

Conclusion: CAKE是一个有效的框架，可以评估聚类中每个点的置信度，从而提高聚类的质量和鲁棒性。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>
